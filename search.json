[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the site on “Workshop on reproducible research”, at the JdS 2025, Marseille."
  },
  {
    "objectID": "docs/ioannidis2005.html",
    "href": "docs/ioannidis2005.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Author: John P. A. Ioannidis\nSource: PLOS Medicine, August 2005, Vol. 2, Issue 8, e124\nOriginal Article (PDF)\n\n\n\nThere is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when: - Studies are smaller - Effect sizes are smaller - There are more and less preselected tested relationships - There is greater flexibility in designs, definitions, outcomes, and analytical modes - There is greater financial and other interest and prejudice - More teams are involved in a scientific field in chase of statistical significance\nSimulations show that for most study designs and settings, it is more likely for a research claim to be false than true. For many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. The essay discusses the implications of these problems for the conduct and interpretation of research.\n\n\n\n\nPublished research findings are sometimes refuted by subsequent evidence, with ensuing confusion and disappointment. Refutation and controversy is seen across the range of research designs, from clinical trials and traditional epidemiological studies to the most modern molecular research. There is increasing concern that in modern research, false findings may be the majority or even the vast majority of published research claims. However, this should not be surprising. It can be proven that most claimed research findings are false. Here I will examine the key factors that influence this problem and some corollaries thereof.\n\n\nSeveral methodologists have pointed out that the high rate of nonreplication of research discoveries is a consequence of the convenient, yet ill-founded strategy of claiming conclusive research findings solely on the basis of a single study assessed by formal statistical significance, typically for a p-value less than 0.05. Research is not most appropriately represented and summarized by p-values, but, unfortunately, there is a widespread notion that medical research articles should be interpreted based only on p-values. Research findings are defined here as any relationship reaching formal statistical significance, e.g., effective interventions, informative predictors, risk factors, or associations. “Negative” research is also very useful. “Negative” is actually a misnomer, and the misinterpretation is widespread. However, here we will target relationships that investigators claim exist, rather than null findings. As has been shown previously, the probability that a research finding is indeed true depends on the prior probability of it being true (before doing the study), the statistical power of the study, and the level of statistical significance.\n…existing code…\n\n\n\nFirst, let us define bias as the combination of various design, data, analysis, and presentation factors that tend to produce research findings when they should not be produced. Bias should not be confused with chance variability that causes some findings to be false by chance even though the study design, data, analysis, and presentation are perfect. Bias can entail manipulation in the analysis or reporting of findings. Selective or distorted reporting is a typical form of such bias. We may assume that bias does not depend on whether a true relationship exists or not. In the presence of bias, the chances that a research finding is true diminish considerably.\n…existing code…\n\n\n\nSeveral independent teams may be addressing the same sets of research questions. As research efforts are globalized, it is practically the rule that several research teams, often dozens of them, may probe the same or similar questions. Unfortunately, in some areas, the prevailing mentality until now has been to focus on isolated discoveries by single teams and interpret research experiments in isolation. An increasing number of questions have at least one study claiming a research finding, and this receives unilateral attention. The probability that at least one study, among several done on the same question, claims a statistically significant research finding is easy to estimate. With increasing number of independent studies, the probability that a research finding is true tends to decrease.\n…existing code…\n\n\n\n\nThe smaller the studies conducted in a scientific field, the less likely the research findings are to be true.\nThe smaller the effect sizes in a scientific field, the less likely the research findings are to be true.\nThe greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true.\nThe greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.\nThe greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true.\nThe hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.\n\n…existing code…\n\n\n\nIn the described framework, a positive predictive value (PPV) exceeding 50% is quite difficult to get. Simulations and practical examples show that even well-conducted studies may have a low probability of being true, especially in fields with low pre-study odds or high bias.\n…existing code…\n\n\n\nThe majority of modern biomedical research is operating in areas with very low pre- and poststudy probability for true findings. In such fields, claimed effect sizes may simply be measures of the prevailing bias. Large and highly significant effects may actually be more likely to be signs of large bias in most fields of modern research.\n…existing code…\n\n\n\nIs it unavoidable that most research findings are false, or can we improve the situation? Approaches to improve the post-study probability include better powered evidence, large studies, low-bias meta-analyses, and enhanced research standards. Registration of studies and adherence to protocols can also help. However, it is unavoidable that one should make approximate assumptions on how many relationships are expected to be true among those probed across the relevant research fields and research designs. The wider field may yield some guidance for estimating this probability for the isolated research project.\n…existing code…\n\n\n\n\nA full list of references is available in the original article.\n\nLicense: © 2005 John P. A. Ioannidis. This is an open-access article distributed under the terms of the Creative Commons Attribution License."
  },
  {
    "objectID": "docs/ioannidis2005.html#summary",
    "href": "docs/ioannidis2005.html#summary",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when: - Studies are smaller - Effect sizes are smaller - There are more and less preselected tested relationships - There is greater flexibility in designs, definitions, outcomes, and analytical modes - There is greater financial and other interest and prejudice - More teams are involved in a scientific field in chase of statistical significance\nSimulations show that for most study designs and settings, it is more likely for a research claim to be false than true. For many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. The essay discusses the implications of these problems for the conduct and interpretation of research."
  },
  {
    "objectID": "docs/ioannidis2005.html#full-text",
    "href": "docs/ioannidis2005.html#full-text",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Published research findings are sometimes refuted by subsequent evidence, with ensuing confusion and disappointment. Refutation and controversy is seen across the range of research designs, from clinical trials and traditional epidemiological studies to the most modern molecular research. There is increasing concern that in modern research, false findings may be the majority or even the vast majority of published research claims. However, this should not be surprising. It can be proven that most claimed research findings are false. Here I will examine the key factors that influence this problem and some corollaries thereof.\n\n\nSeveral methodologists have pointed out that the high rate of nonreplication of research discoveries is a consequence of the convenient, yet ill-founded strategy of claiming conclusive research findings solely on the basis of a single study assessed by formal statistical significance, typically for a p-value less than 0.05. Research is not most appropriately represented and summarized by p-values, but, unfortunately, there is a widespread notion that medical research articles should be interpreted based only on p-values. Research findings are defined here as any relationship reaching formal statistical significance, e.g., effective interventions, informative predictors, risk factors, or associations. “Negative” research is also very useful. “Negative” is actually a misnomer, and the misinterpretation is widespread. However, here we will target relationships that investigators claim exist, rather than null findings. As has been shown previously, the probability that a research finding is indeed true depends on the prior probability of it being true (before doing the study), the statistical power of the study, and the level of statistical significance.\n…existing code…\n\n\n\nFirst, let us define bias as the combination of various design, data, analysis, and presentation factors that tend to produce research findings when they should not be produced. Bias should not be confused with chance variability that causes some findings to be false by chance even though the study design, data, analysis, and presentation are perfect. Bias can entail manipulation in the analysis or reporting of findings. Selective or distorted reporting is a typical form of such bias. We may assume that bias does not depend on whether a true relationship exists or not. In the presence of bias, the chances that a research finding is true diminish considerably.\n…existing code…\n\n\n\nSeveral independent teams may be addressing the same sets of research questions. As research efforts are globalized, it is practically the rule that several research teams, often dozens of them, may probe the same or similar questions. Unfortunately, in some areas, the prevailing mentality until now has been to focus on isolated discoveries by single teams and interpret research experiments in isolation. An increasing number of questions have at least one study claiming a research finding, and this receives unilateral attention. The probability that at least one study, among several done on the same question, claims a statistically significant research finding is easy to estimate. With increasing number of independent studies, the probability that a research finding is true tends to decrease.\n…existing code…\n\n\n\n\nThe smaller the studies conducted in a scientific field, the less likely the research findings are to be true.\nThe smaller the effect sizes in a scientific field, the less likely the research findings are to be true.\nThe greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true.\nThe greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.\nThe greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true.\nThe hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.\n\n…existing code…\n\n\n\nIn the described framework, a positive predictive value (PPV) exceeding 50% is quite difficult to get. Simulations and practical examples show that even well-conducted studies may have a low probability of being true, especially in fields with low pre-study odds or high bias.\n…existing code…\n\n\n\nThe majority of modern biomedical research is operating in areas with very low pre- and poststudy probability for true findings. In such fields, claimed effect sizes may simply be measures of the prevailing bias. Large and highly significant effects may actually be more likely to be signs of large bias in most fields of modern research.\n…existing code…\n\n\n\nIs it unavoidable that most research findings are false, or can we improve the situation? Approaches to improve the post-study probability include better powered evidence, large studies, low-bias meta-analyses, and enhanced research standards. Registration of studies and adherence to protocols can also help. However, it is unavoidable that one should make approximate assumptions on how many relationships are expected to be true among those probed across the relevant research fields and research designs. The wider field may yield some guidance for estimating this probability for the isolated research project.\n…existing code…"
  },
  {
    "objectID": "docs/ioannidis2005.html#references",
    "href": "docs/ioannidis2005.html#references",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "A full list of references is available in the original article.\n\nLicense: © 2005 John P. A. Ioannidis. This is an open-access article distributed under the terms of the Creative Commons Attribution License."
  },
  {
    "objectID": "docs/quarto-docs.html",
    "href": "docs/quarto-docs.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Quarto logo.\n\n\n\nOverview\nGet Started\nGuide\nExtensions\nReference\nGallery\nBlog\nHelp\n\nReport a Bug\nAsk a Question\nFAQ\n\n\n\nGuide\n\n\nGuide\n\nAuthoring\n\nMarkdown Basics\nFigures\nTables\nDiagrams\nShortcodes\n\nPlaceholder Image\nLorem Lipsum Text\nRearrange Contents\n\nVideos\nEmbeds\nCallout Blocks\nCode Annotation\nBrand\nArticle Layout\nScholarly Writing\n\nFront Matter\nTitle Blocks\nCitations\nCross-References\n\nBasics\nOptions\nDiv Syntax\nCustom Floats\n\nCreating Citeable Articles\nAppendices\n\n\nComputations\n\nUsing Python\nUsing R\nUsing Julia\nUsing Observable\nInline Code\nRendering Script Files\nExecution Options\nParameters\n\nTools\n\nJupyterLab\n\nJupyterLab Basics\nJupyterLab Extension\n\nRStudio IDE\n\nRStudio Basics\nVisual Editor\n\nEditor Basics\nTechnical Writing\nContent Editing\nShortcuts & Options\nMarkdown Output\n\n\nVS Code\n\nVS Code Basics\nVisual Editor\nNotebook Editor\n\nNeovim\nText Editors\n\nDocuments\n\nHTML\n\nHTML Basics\nHTML Code Blocks\nHTML Theming\nIncluding Other Formats\nLightbox Figures\nPublishing HTML\n\nPDF\n\nPDF Basics\nPDF Engines\n\nMS Word\n\nWord Basics\nWord Templates\n\nTypst\n\nTypst Basics\nCustom Formats\n\nMarkdown\n\nGitHub (GFM)\nHugo\nDocusaurus\n\nAll Formats\n\nPresentations\n\nOverview\nRevealjs\n\nReveal Basics\nPresenting Slides\nAdvanced Reveal\nReveal Themes\n\nPowerPoint\nBeamer\n\nDashboards\n\nOverview\nUsing Dashboards\n\nLayout\nData Display\nInputs\nTheming\nParameters\nDeployment\n\nInteractivity\n\nOverview\nShiny for Python\n\nGetting Started\nRunning Dashboards\nExecution Contexts\n\nShiny for R\n\nGetting Started\nRunning Documents\nExecution Contexts\n\nObservable JS\n\nExamples\n\nWebsites\n\nCreating a Website\nWebsite Navigation\nCreating a Blog\nWebsite Drafts\nWebsite Search\nWebsite Tools\nAbout Pages\nListing Pages\n\nDocument Listings\nCustom Listings\n\n\nBooks\n\nCreating a Book\nBook Structure\nBook Crossrefs\nCustomizing Output\n\nManuscripts\n\nGetting Started\n\nAuthoring Manuscripts\n\nJupyter Lab\nVS Code\nRStudio\n\nPublishing Manuscripts\nNext Steps\n\nUsing Manuscripts\n\nInteractivity\n\nOverview\nObservable JS\n\nIntroduction\nLibraries\nData Sources\nOJS Cells\nShiny Reactives\nCode Reuse\nExamples\n\nPenguins\nSunburst\nArquero\nPopulation\nNOAA CO2\nGitHub API\nLayout\nShiny\n\nK-Means\nBinning\nData Binding\nCovid Map\n\n\n\nShiny\n\nIntroduction\nRunning Documents\nExecution Contexts\nExternal Resources\nExamples\n\nOld Faithful\nK-Means\nDiamonds\n\n\nWidgets\n\nJupyter Widgets\nhtmlwidgets for R\n\nComponent Layout\n\nPublishing\n\nPublishing Basics\nQuarto Pub\nGitHub Pages\nPosit Connect\nPosit Cloud\nNetlify\nConfluence\nHugging Face Spaces\nOther Services\nPublishing with CI\n\nProjects\n\nProject Basics\nManaging Execution\nProject Profiles\nEnvironment Variables\nProject Scripts\nVirtual Environments\nUsing Binder With Quarto\n\nAdvanced\n\nIncludes\nVariables\nPage Layout\nDocument Language\nConditional Content\nNotebook Filters\nJupyter\n\nJupyter Kernel Execution\n\n\n\n\n\nGuide\nComprehensive guide to using Quarto. If you are just starting out, you may want to explore the tutorials to learn the basics.\n\nAuthoring\n\nCreate content with markdown\n\nMarkdown Basics\nFigures\nTables\nDiagrams\nCitations\nCross References\nArticle Layout\nShortcodes\n\n\n\n\nComputations\n\nExecute code and display its output\n\nUsing Python\nUsing R\nUsing Julia\nUsing Observable\nExecution Options\nParameters\n\n\n\n\nTools\n\nUse your favorite tools with Quarto\n\nJupyterLab\nRStudio IDE\nVS Code\nNeovim\nText Editors\nVisual Editor\n\n\n\n\nDocuments\n\nGenerate output in many formats\n\nHTML\nPDF\nMS Word\nTypst\nMarkdown\nAll Formats\n\n\n\n\nPresentations\n\nPresent code and technical content\n\nPresentation Basics\nRevealjs (HTML)\nPowerPoint (Office)\nBeamer (PDF)\n\n\n\n\nDashboards\n\nPublish data with dashboards\n\nDashboard Basics\nLayout\nData Display\nInteractivity\nDeployment\n\n\n\n\nWebsites\n\nCreate websites and blogs\n\nCreating a Website\nWebsite Navigation\nCreating a Blog\nWebsite Search\nWebsite Listings\n\n\n\n\nBooks\n\nCreate books and manuscripts\n\nCreating a Book\nBook Structure\nBook Crossrefs\nCustomizing Output\n\n\n\n\nManuscripts\n\nWrite and publish notebook-first scholarly articles\n\nGetting Started\nAuthoring Manuscripts\nPublishing Manuscripts\nUsing Manuscripts\n\n\n\n\nInteractivity\n\nEngage readers with interactivity\n\nOverview\nObservable JS\nShiny\nWidgets\nComponent Layout\n\n\n\n\nPublishing\n\nPublishing documents and sites\n\nPublishing Basics\nQuarto Pub\nGitHub Pages\nPosit Connect\nPosit Cloud\nNetlify\nConfluence\nOther Services\n\n\n\n\nProjects\n\nScale up your work with projects\n\nProject Basics\nManaging Execution\nProject Profiles\nEnvironment Variables\nProject Scripts\nVirtual Environments\n\n\n\n\nAdvanced\n\nRefine documents with advanced tools\n\nIncludes\nVariables\nPage Layout\nDocument Language\nConditional Content\nNotebook Filters\n\nNo matching items\nMarkdown Basics\nProudly supported by \n\nAbout\nFAQ\nLicense\nTrademark\nEdit this page\nReport an issue"
  },
  {
    "objectID": "docs/computo-journal.html",
    "href": "docs/computo-journal.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Computo was created in response to the reproducibility crisis in science, aiming to promote computational/algorithmic contributions in statistics and machine learning (ML) that provide insight into which models or methods are most appropriate for specific scientific questions.\nThe journal welcomes: - New methods with original stats/ML developments, or numerical studies illustrating theoretical results - Case studies or surveys on stats/ML methods for specific data analysis questions, including neutral comparison studies - Software/tutorial papers presenting implementations of stats/ML algorithms, with concrete use cases and benchmarking\nProspective authors can contact the editor at computo@sfds.asso.fr for pre-submission enquiries.\n\n\n\nComputo is free for readers and authors. All content is open access, in line with the Budapest Open Access Initiative. Reproducibility of numerical results is required for publication. Submissions must include all necessary data and code, and the quality of code is assessed during review. Reviews are open and visible after acceptance.\n\n\n\nEnquiries: computo@sfds.asso.fr\n\n\n\nComputo relies on Jekyll, BibTeX, the aI-folio Jekyll theme, Rmarkdown, Jupyter-book, and draws inspiration from Rescience-C and distill.pub.\n\n\n\nLogo designed by Loïc Schwaller."
  },
  {
    "objectID": "docs/computo-journal.html#aims-and-scope",
    "href": "docs/computo-journal.html#aims-and-scope",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Computo was created in response to the reproducibility crisis in science, aiming to promote computational/algorithmic contributions in statistics and machine learning (ML) that provide insight into which models or methods are most appropriate for specific scientific questions.\nThe journal welcomes: - New methods with original stats/ML developments, or numerical studies illustrating theoretical results - Case studies or surveys on stats/ML methods for specific data analysis questions, including neutral comparison studies - Software/tutorial papers presenting implementations of stats/ML algorithms, with concrete use cases and benchmarking\nProspective authors can contact the editor at computo@sfds.asso.fr for pre-submission enquiries."
  },
  {
    "objectID": "docs/computo-journal.html#open-access-and-reproducibility",
    "href": "docs/computo-journal.html#open-access-and-reproducibility",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Computo is free for readers and authors. All content is open access, in line with the Budapest Open Access Initiative. Reproducibility of numerical results is required for publication. Submissions must include all necessary data and code, and the quality of code is assessed during review. Reviews are open and visible after acceptance."
  },
  {
    "objectID": "docs/computo-journal.html#contact",
    "href": "docs/computo-journal.html#contact",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Enquiries: computo@sfds.asso.fr"
  },
  {
    "objectID": "docs/computo-journal.html#thanks",
    "href": "docs/computo-journal.html#thanks",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Computo relies on Jekyll, BibTeX, the aI-folio Jekyll theme, Rmarkdown, Jupyter-book, and draws inspiration from Rescience-C and distill.pub."
  },
  {
    "objectID": "docs/computo-journal.html#about-the-logo",
    "href": "docs/computo-journal.html#about-the-logo",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Logo designed by Loïc Schwaller."
  },
  {
    "objectID": "docs/computo-journal.html#recent-publications",
    "href": "docs/computo-journal.html#recent-publications",
    "title": "Workshop on Reproducible Research",
    "section": "Recent Publications",
    "text": "Recent Publications\n\n2025:\n\nEfficient simulation of individual-based population models (Giorgi, Kaakai, Lemaire)\nSpectral Bridges: Scalable Spectral Clustering Based on Vector Quantization (Laplante, Ambroise)\n\n2024:\n\nBayesian Spatiotemporal Modelling of Wildfire Occurrences and Sizes (Legrand et al.)\nGeometric-Based Pruning Rules for Change Point Detection (Pishchagina et al.)\nAdaptiveConformal: An R Package for Adaptive Conformal Inference (Susmann et al.)\nPeerannot: classification for crowdsourced image datasets with Python (Lefort et al.)\nOptimal projection for parametric importance sampling in high dimensions (El Masri et al.)\nPoint Process Discrimination According to Repulsion (Adrat, Decreusefond)\nA hierarchical model to evaluate pest treatments (Favrot, Makoswki)\n\n2023:\n\nLocal tree methods for classification: a review and some dead ends (Cleynen et al.)\nComputing an empirical Fisher information matrix estimate in latent variable models (Delattre, Kuhn)\nInference of Multiscale Gaussian Graphical Model (Sanou et al.)\nMacrolitter Video Counting on Riverbanks (Chagneux et al.)\nA Python Package for Sampling from Copulae: clayton (Boulin)\n\n2022:\n\nTrade-off between deep learning for species identification and inference about predator-prey co-occurrence (Gimenez et al.)\n\n\nSee the website for the full list and details."
  },
  {
    "objectID": "docs/computo-journal.html#submission-overview",
    "href": "docs/computo-journal.html#submission-overview",
    "title": "Workshop on Reproducible Research",
    "section": "Submission Overview",
    "text": "Submission Overview\nSubmissions require both scientific content (equations, code, figures, data) and proof of reproducibility. This is achieved by: - Using a notebook system (e.g., Quarto, Rmarkdown, Jupyter) - Providing a virtual environment for dependencies - Setting up continuous integration (CI) for automated builds - Storing data files on platforms like Zenodo or OSF if needed\nA submission is a git repository containing: - The notebook source file (Markdown with YAML metadata) - Auxiliary files (BibTeX, figures, data tables) - Environment configuration files - CI configuration files\nThe compiled notebook (HTML and PDF) is generated via CI and published to a web page. The PDF and repository URL are then submitted via OpenReview."
  },
  {
    "objectID": "docs/computo-journal.html#templates",
    "href": "docs/computo-journal.html#templates",
    "title": "Workshop on Reproducible Research",
    "section": "Templates",
    "text": "Templates\nTemplates are available for R, Python, and Julia. Authors are encouraged to use these to ensure reproducibility."
  },
  {
    "objectID": "docs/computo-journal.html#reviewing-and-publication",
    "href": "docs/computo-journal.html#reviewing-and-publication",
    "title": "Workshop on Reproducible Research",
    "section": "Reviewing and Publication",
    "text": "Reviewing and Publication\nPapers are reviewed by external reviewers. Authors should suggest four potential referees. Accepted papers are published under CC BY 4.0."
  },
  {
    "objectID": "docs/computo-journal.html#code-of-ethics-for-authors",
    "href": "docs/computo-journal.html#code-of-ethics-for-authors",
    "title": "Workshop on Reproducible Research",
    "section": "Code of Ethics for Authors",
    "text": "Code of Ethics for Authors\n\nOriginality, conflicts of interest, impartiality, funding, and copyright compliance are required."
  },
  {
    "objectID": "docs/computo-journal.html#review-process",
    "href": "docs/computo-journal.html#review-process",
    "title": "Workshop on Reproducible Research",
    "section": "Review Process",
    "text": "Review Process\nComputo uses Open Review for peer review. Reviews and discussion are made available on the website after acceptance. Reviewers can remain anonymous."
  },
  {
    "objectID": "docs/computo-journal.html#guidelines-for-evaluation",
    "href": "docs/computo-journal.html#guidelines-for-evaluation",
    "title": "Workshop on Reproducible Research",
    "section": "Guidelines for Evaluation",
    "text": "Guidelines for Evaluation\n\nIs the paper within Computo’s scope?\nIs the paper clearly written?\nIs the paper correct?\nIs the paper adequately evaluated?\nIs the paper reproducible?"
  },
  {
    "objectID": "docs/computo-journal.html#plagiarism-policy",
    "href": "docs/computo-journal.html#plagiarism-policy",
    "title": "Workshop on Reproducible Research",
    "section": "Plagiarism Policy",
    "text": "Plagiarism Policy\nComputo follows COPE guidelines on plagiarism."
  },
  {
    "objectID": "docs/computo-journal.html#editors",
    "href": "docs/computo-journal.html#editors",
    "title": "Workshop on Reproducible Research",
    "section": "Editors",
    "text": "Editors\n\nJulien Chiquet (Chief editor) – Université Paris-Saclay, AgroParisTech, INRAE, Paris, France\nChloé-Agathe Azencott (Associate editor) – Mines ParisTech, Institut Curie and INSERM, Paris, France\nMathurin Massias (Associate editor) – INRIA, OCKHAM, Lyon, France\nPierre Neuvial (Associate editor) – CNRS, Institut de Mathématiques de Toulouse, Toulouse, France\nNelle Varoquaux (Associate editor) – CNRS, Université Grenoble Alpes, Grenoble, France\nMarie-Pierre Etienne (Associate editor) – Agrocampus Ouest, Rennes, France"
  },
  {
    "objectID": "docs/computo-journal.html#technical-support",
    "href": "docs/computo-journal.html#technical-support",
    "title": "Workshop on Reproducible Research",
    "section": "Technical Support",
    "text": "Technical Support\n\nFrançois-David Collin – Institut Montpelliérain Alexander Grothendieck, CNRS, France\nGhislain Durif – ENS Lyon, LBMC, CNRS, France"
  },
  {
    "objectID": "docs/computo-journal.html#additional-details-and-support-for-authors",
    "href": "docs/computo-journal.html#additional-details-and-support-for-authors",
    "title": "Workshop on Reproducible Research",
    "section": "Additional Details and Support for Authors",
    "text": "Additional Details and Support for Authors\n\nHow to automatically publish the HTML of my contribution to a website?\nWhat is expected exactly in terms of reproducibility?\nMy data analysis takes several hours/days/weeks… How to address the issue of reproducibility?\nI have large or sensitive data. How should I proceed?\nI use a different language than Python, R or Julia: would Computo accept my contributions?\n\nFor more, visit the FAQ section."
  },
  {
    "objectID": "docs/steen2011.html",
    "href": "docs/steen2011.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Author: R. Grant Steen Journal: Journal of Medical Ethics, 2011, Vol 37, Issue 4, pp. 249-253 DOI: 10.1136/jme.2010.040923\n\n\nScientific papers are retracted for many reasons including fraud (data fabrication or falsification) or error (plagiarism, scientific mistake, ethical problems). Growing attention to fraud in the lay press suggests that the incidence of fraud is increasing.\n\nRetractions in the scientific literature: is the incidence of research fraud increasing? R Grant Steen\nBackground Scientific papers are retracted for many reasons including fraud (data fabrication or falsification) or error (plagiarism, scientific mistake, ethical problems). Growing attention to fraud in the lay press suggests that the incidence of fraud is increasing. Methods The reasons for retracting 742 English language research papers retracted from the PubMed database between 2000 and 2010 were evaluated. Reasons for retraction were initially dichotomised as fraud or error and then analysed to determine specific reasons for retraction. Results Error was more common than fraud (73.5% of papers were retracted for error (or an undisclosed reason) vs 26.6% retracted for fraud). Eight reasons for retraction were identified; the most common reason was scientific mistake in 234 papers (31.5%), but 134 papers (18.1%) were retracted for ambiguous reasons. Fabrication (including data plagiarism) was more common than text plagiarism. Total papers retracted per year have increased sharply over the decade (r=0.96; p&lt;0.001), as have retractions specifically for fraud (r=0.89; p&lt;0.001). Journals now reach farther back in time to retract, both for fraud (r=0.87; p&lt;0.001) and for scientific mistakes (r=0.95; p&lt;0.001). Journals often fail to alert the naïve reader; 31.8% of retracted papers were not noted as retracted in any way. Conclusions Levels of misconduct appear to be higher than in the past. This may reflect either a real increase in the incidence of fraud or a greater effort on the part of journals to police the literature. However, research bias is rarely cited as a reason for retraction.\n\nRetractions in the scientific literature: is the incidence of research fraud increasing? R Grant Steen\nABSTRACT Background Scientific papers are retracted for many reasons including fraud (data fabrication or falsification) or error (plagiarism, scientific mistake, ethical problems). Growing attention to fraud in the lay press suggests that the incidence of fraud is increasing. Methods The reasons for retracting 742 English language research papers retracted from the PubMed database between 2000 and 2010 were evaluated. Reasons for retraction were initially dichotomised as fraud or error and then analysed to determine specific reasons for retraction. Results Error was more common than fraud (73.5% of papers were retracted for error (or an undisclosed reason) vs 26.6% retracted for fraud). Eight reasons for retraction were identified; the most common reason was scientific mistake in 234 papers (31.5%), but 134 papers (18.1%) were retracted for ambiguous reasons. Fabrication (including data plagiarism) was more common than text plagiarism. Total papers retracted per year have increased sharply over the decade (r=0.96; p&lt;0.001), as have retractions specifically for fraud (r=0.89; p&lt;0.001). Journals now reach farther back in time to retract, both for fraud (r=0.87; p&lt;0.001) and for scientific mistakes (r=0.95; p&lt;0.001). Journals often fail to alert the naïve reader; 31.8% of retracted papers were not noted as retracted in any way. Conclusions Levels of misconduct appear to be higher than in the past. This may reflect either a real increase in the incidence of fraud or a greater effort on the part of journals to police the literature. However, research bias is rarely cited as a reason for retraction.\nINTRODUCTION Accusations that research is tainted by bias have become commonplace in the news media. The ClimateGate scandal arose when climate change critics hacked into a research database at the University of East Anglia, evaluated the data without authorisation and went public with accusations that data had been selectively published and perhaps even falsified.1 More recently, a scientist at Harvard has been accused of biasing or falsifying data that show tamarin monkeys can learn algebraic rules.2 Yet it can be very hard to prove allegations of bias in the scientific literature. What has been called bias can potentially also be explained by unfavourable primary outcome findings that force a focus on secondary outcomes, problems in measurement of an outcome variable, changes mandated by reviewers, ambiguous or non-significant findings that may make authors reluctant to submit or journals reluctant to publish, errors that were corrected during data analysis, or simply lack of time and inclination to publish unexciting findings.3 For these reasons, it seems that probing allegations of bias may be an ineffective means to study the integrity of the scientific enterprise. If the goal is to characterise the veracity of science, it may be more useful to examine papers that have been retracted from the literature to determine the reasons for retraction.4 5 This approach may be more objective than other approaches; bias is often in the eye of the beholder,6 whereas retraction is akin to the death of a paperdan unambiguous end point.7 We postulate that the media focus on the integrity of science is a rational response to an actual increase in the rate of retraction. Our hypothesis is that the incidence of research fraud has indeed increased in recent years. METHODS Every research paper that was noted as retracted in the PubMed database from 2000 to 2010 was evaluated.4 PubMed was searched on 22 January 2010 using the limits of ‘items with abstracts, retracted publication, English.’ A total of 788 retracted papers were identified, all of which were exported from PubMed and saved as a text file (available upon request). Formal retraction notices could not be obtained for 46 papers (5.8% of all retracted papers), so the reason these papers were retracted cannot be assessed.5 The 742 papers for which retraction notices could be obtained were evaluated to determine the reason(s) for retraction. Initially, reasons for retraction were dichotomised as fraud (manipulation of data) or error (all other causes).5 Each retraction notice was then re-evaluated to determine detailed reasons for retraction. An effort was made to formalise a definition for every identified cause of retraction so that the reasons for retraction could be scored systematically. If a retraction notice noted several reasons for retraction, all reasons were tabulated. In cases where the retraction notice was vague, a ‘best guess’ was made as to the cause, although some retraction notices gave no information at all so an ‘unstated’ category was set up. Additional information about each retracted article was tabulated including: first author surname; number of authors; country of address of first author; year of publication; year of retraction; journal of publication; and journal impact factor. Journal impact factor was determined using the ISI Web of Knowledge (Thomson Reuters) ‘Journal Citation Reports, Science Edition’ for 2008 (last available year). To determine if the incidence of retraction for fraud (fabrication + falsification) has increased in recent years, the number of fraudulent papers was Correspondence to R Grant Steen, Medical Communications Consultants, LLC 103 Van Doren Place, Chapel Hill, NC 27517, USA; g_steen_medicc@yahoo.com Received 18 October 2010 Accepted 5 November 2010 Published Online First 24 December 2010 J Med Ethics 2011;37:249e253. doi:10.1136/jme.2010.040923 249 Research ethics"
  },
  {
    "objectID": "docs/steen2011.html#abstract",
    "href": "docs/steen2011.html#abstract",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Scientific papers are retracted for many reasons including fraud (data fabrication or falsification) or error (plagiarism, scientific mistake, ethical problems). Growing attention to fraud in the lay press suggests that the incidence of fraud is increasing.\n\nRetractions in the scientific literature: is the incidence of research fraud increasing? R Grant Steen\nBackground Scientific papers are retracted for many reasons including fraud (data fabrication or falsification) or error (plagiarism, scientific mistake, ethical problems). Growing attention to fraud in the lay press suggests that the incidence of fraud is increasing. Methods The reasons for retracting 742 English language research papers retracted from the PubMed database between 2000 and 2010 were evaluated. Reasons for retraction were initially dichotomised as fraud or error and then analysed to determine specific reasons for retraction. Results Error was more common than fraud (73.5% of papers were retracted for error (or an undisclosed reason) vs 26.6% retracted for fraud). Eight reasons for retraction were identified; the most common reason was scientific mistake in 234 papers (31.5%), but 134 papers (18.1%) were retracted for ambiguous reasons. Fabrication (including data plagiarism) was more common than text plagiarism. Total papers retracted per year have increased sharply over the decade (r=0.96; p&lt;0.001), as have retractions specifically for fraud (r=0.89; p&lt;0.001). Journals now reach farther back in time to retract, both for fraud (r=0.87; p&lt;0.001) and for scientific mistakes (r=0.95; p&lt;0.001). Journals often fail to alert the naïve reader; 31.8% of retracted papers were not noted as retracted in any way. Conclusions Levels of misconduct appear to be higher than in the past. This may reflect either a real increase in the incidence of fraud or a greater effort on the part of journals to police the literature. However, research bias is rarely cited as a reason for retraction.\n\nRetractions in the scientific literature: is the incidence of research fraud increasing? R Grant Steen\nABSTRACT Background Scientific papers are retracted for many reasons including fraud (data fabrication or falsification) or error (plagiarism, scientific mistake, ethical problems). Growing attention to fraud in the lay press suggests that the incidence of fraud is increasing. Methods The reasons for retracting 742 English language research papers retracted from the PubMed database between 2000 and 2010 were evaluated. Reasons for retraction were initially dichotomised as fraud or error and then analysed to determine specific reasons for retraction. Results Error was more common than fraud (73.5% of papers were retracted for error (or an undisclosed reason) vs 26.6% retracted for fraud). Eight reasons for retraction were identified; the most common reason was scientific mistake in 234 papers (31.5%), but 134 papers (18.1%) were retracted for ambiguous reasons. Fabrication (including data plagiarism) was more common than text plagiarism. Total papers retracted per year have increased sharply over the decade (r=0.96; p&lt;0.001), as have retractions specifically for fraud (r=0.89; p&lt;0.001). Journals now reach farther back in time to retract, both for fraud (r=0.87; p&lt;0.001) and for scientific mistakes (r=0.95; p&lt;0.001). Journals often fail to alert the naïve reader; 31.8% of retracted papers were not noted as retracted in any way. Conclusions Levels of misconduct appear to be higher than in the past. This may reflect either a real increase in the incidence of fraud or a greater effort on the part of journals to police the literature. However, research bias is rarely cited as a reason for retraction.\nINTRODUCTION Accusations that research is tainted by bias have become commonplace in the news media. The ClimateGate scandal arose when climate change critics hacked into a research database at the University of East Anglia, evaluated the data without authorisation and went public with accusations that data had been selectively published and perhaps even falsified.1 More recently, a scientist at Harvard has been accused of biasing or falsifying data that show tamarin monkeys can learn algebraic rules.2 Yet it can be very hard to prove allegations of bias in the scientific literature. What has been called bias can potentially also be explained by unfavourable primary outcome findings that force a focus on secondary outcomes, problems in measurement of an outcome variable, changes mandated by reviewers, ambiguous or non-significant findings that may make authors reluctant to submit or journals reluctant to publish, errors that were corrected during data analysis, or simply lack of time and inclination to publish unexciting findings.3 For these reasons, it seems that probing allegations of bias may be an ineffective means to study the integrity of the scientific enterprise. If the goal is to characterise the veracity of science, it may be more useful to examine papers that have been retracted from the literature to determine the reasons for retraction.4 5 This approach may be more objective than other approaches; bias is often in the eye of the beholder,6 whereas retraction is akin to the death of a paperdan unambiguous end point.7 We postulate that the media focus on the integrity of science is a rational response to an actual increase in the rate of retraction. Our hypothesis is that the incidence of research fraud has indeed increased in recent years. METHODS Every research paper that was noted as retracted in the PubMed database from 2000 to 2010 was evaluated.4 PubMed was searched on 22 January 2010 using the limits of ‘items with abstracts, retracted publication, English.’ A total of 788 retracted papers were identified, all of which were exported from PubMed and saved as a text file (available upon request). Formal retraction notices could not be obtained for 46 papers (5.8% of all retracted papers), so the reason these papers were retracted cannot be assessed.5 The 742 papers for which retraction notices could be obtained were evaluated to determine the reason(s) for retraction. Initially, reasons for retraction were dichotomised as fraud (manipulation of data) or error (all other causes).5 Each retraction notice was then re-evaluated to determine detailed reasons for retraction. An effort was made to formalise a definition for every identified cause of retraction so that the reasons for retraction could be scored systematically. If a retraction notice noted several reasons for retraction, all reasons were tabulated. In cases where the retraction notice was vague, a ‘best guess’ was made as to the cause, although some retraction notices gave no information at all so an ‘unstated’ category was set up. Additional information about each retracted article was tabulated including: first author surname; number of authors; country of address of first author; year of publication; year of retraction; journal of publication; and journal impact factor. Journal impact factor was determined using the ISI Web of Knowledge (Thomson Reuters) ‘Journal Citation Reports, Science Edition’ for 2008 (last available year). To determine if the incidence of retraction for fraud (fabrication + falsification) has increased in recent years, the number of fraudulent papers was Correspondence to R Grant Steen, Medical Communications Consultants, LLC 103 Van Doren Place, Chapel Hill, NC 27517, USA; g_steen_medicc@yahoo.com Received 18 October 2010 Accepted 5 November 2010 Published Online First 24 December 2010 J Med Ethics 2011;37:249e253. doi:10.1136/jme.2010.040923 249 Research ethics"
  },
  {
    "objectID": "docs/knuth1984.html",
    "href": "docs/knuth1984.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Literate Programming\nDonald E. Knuth, The Computer Journal, 1984\n\nLiterate Programming Donald E. Knuth Computer Science Department, Stanford University, Stanford, CA 94305, USA\nThe author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. This paper presents WEB by example, and discusses why the new system appears to be an improvement over previous ones.\nA. INTRODUCTION\nThe past ten years have witnessed substantial improvements in programming methodology. This advance, carried out under the banner of “structured programming,” has led to programs that are more reliable and easier to comprehend; yet the results are not entirely satisfactory. My purpose in the present paper is to propose another motto that may be appropriate for the next decade, as we attempt to make further progress in the state of the art. I believe that the time is ripe for significantly better documentation of programs, and that we can best achieve this by considering programs to be works of literature. Hence, my title: “Literate Programming.”\nB. LITERATE PROGRAMMING\nThe basic idea is to combine the program and its documentation into a single source text, written in a natural language (such as English), which explains the logic of the program in a way that is easy for humans to understand. The program is then extracted from this text by a special-purpose program (a “literate programming system”), which also produces documentation in the form of a structured narrative.\nThe system we have developed is called WEB, and it is based on the Pascal programming language. WEB allows the programmer to write Pascal code interspersed with documentation in a simple markup language. The documentation can include not only explanations of the code, but also diagrams, references to other documents, and even bibliographic citations. The resulting document is a comprehensive description of the program, suitable for publication as a technical report or journal article.\nC. EXAMPLE\nTo illustrate the use of WEB, I will present a small example: the implementation of a simple algorithm for computing the greatest common divisor (GCD) of two integers.\nThe algorithm is based on the observation that if d is a common divisor of a and b, then d is also a divisor of a - b and b. Hence, we can compute the GCD of a and b by repeatedly subtracting the smaller number from the larger one, until the two numbers are equal. This common value is then the GCD of the original pair.\nHere is the WEB source code for this algorithm:\n@dox{This module computes the greatest common divisor (GCD) of two integers.}\n\n@&lt;GCD@&gt;=\nfunction GCD(a, b: integer): integer;\nbegin\nif b = 0 then\nGCD := a\nelse\nGCD := GCD(b, a mod b)\nend;\n\n@&lt;Example@&gt;=\nbegin\nwriteln(GCD(48, 18)) { should print 6 }\nend.\nThe corresponding documentation, produced by the WEB system, looks like this:\nThis module computes the greatest common divisor (GCD) of two integers.\n\nfunction GCD(a, b: integer): integer;\nbegin\nif b = 0 then\nGCD := a\nelse\nGCD := GCD(b, a mod b)\nend;\n\nbegin\nwriteln(GCD(48, 18)) { should print 6 }\nend.\nD. DISCUSSION\nThe main advantage of literate programming is that it encourages the programmer to write documentation as an integral part of the programming process, rather than as an afterthought. This results in programs that are not only easier to understand and maintain, but also more likely to be correct, since the act of writing the documentation forces the programmer to think carefully about the logic of the program.\nAnother advantage is that the documentation can be automatically extracted and formatted by the literate programming system, which saves time and effort, and ensures that the documentation is always up to date.\nOn the downside, literate programming requires a different way of thinking about programs and programming, which may be difficult for some people to adopt. It also requires the use of special-purpose tools, which may not be available or suitable for all programming tasks.\nE. CONCLUSION\nLiterate programming is a powerful new programming paradigm that has the potential to significantly improve the quality and reliability of software systems. It is based on the simple idea of combining the program and its documentation into a single source text, written in a natural language, which explains the logic of the program in a way that is easy for humans to understand. The WEB system is a practical realization of this idea, and it has been successfully used to develop a number of nontrivial software systems.\nI believe that literate programming is the wave of the future, and I urge all programmers to embrace this new paradigm and to start writing their programs in a literate style. The benefits in terms of program quality, reliability, and maintainability will be well worth the effort.\nF. REFERENCES\nKnuth, D. E. (1984). Literate programming. The Computer Journal, 27(2), 97-111."
  },
  {
    "objectID": "docs/allison2016.html",
    "href": "docs/allison2016.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Reproducibility: A tragedy of errors\nAuthors: David B. Allison, Andrew W. Brown, Brandon J. George, Kathryn A. Kaiser Journal: Nature, 2016, Vol 530, Issue 7588, pp. 27-29 DOI: 10.1038/530027a\nMistakes in peer-reviewed papers are easy to find but hard to fix, report David B. Allison and colleagues.\n\nCOMMEN GEO TLOGYDeep-drilling pioneers: what were they drinking? p.33 CONSERVATION Legal loophole allows mango farmers to cull fruit bats in Mauritius p.33 INTERDISCIPLINARITY Can architecture catalyse creativity at the Crick? p.32 EXHIBITION Adolf Fleischmann, pathology sculptor and abstract artist p.30\nJust how error-prone and self-correcting is science? We have spent the past 18 months getting a sense of that. We are a group of researchers working on obesity, nutrition and energetics. In the summer of 2014, one of us (D.B.A.) read a research paper in a well-regarded journal estimating how a change in fastfood consumption would affect children’s weight, and he noted that the analysis applied a mathematical model that overestimated effects by more than tenfold. We and others submitted a letter to the editor explaining the problem. Months later, we were gratified to learn that the authors had elected to retract their paper. In the face of popular articles proclaiming that science is stumbling, this episode was an affirmation that science is self-correcting. Sadly, in our experience, the case is not representative. In the course of assembling weekly lists of articles in our field, we began noticing more peer-reviewed articles containing what we call substantial or invalidating errors. These involve factual mistakes or veer substantially from clearly accepted procedures in ways that, if corrected, might alter a paper’s conclusions. After attempting to address more than 25 of these errors with letters to authors or journals, and identifying at least a dozen more, we had to stop — the work took too much of our time. Our efforts revealed invalidating practices that occur repeatedly (see ‘Three common errors’) and showed how journals and authors react when faced with mistakes that need correction. We learned that post-publication peer review is not consistent, smooth or rapid. Many journal editors and staff members seemed unprepared or ill-equipped to investigate, take action or even respond. Too often, the process spiralled through layers of ineffective e-mails among authors, editors and unidentified journal representatives, often without any public statement added to the original article. Some journals that acknowledged mistakes required a substantial fee to publish our letters: we were asked to spend our research dollars on correcting other people’s errors. As academics who publish, review papers or serve as editors, we appreciate that these issues are complicated. And we feel that journal editors are dedicated and sincere in their efforts. Nevertheless, the scientific community must improve. Science relies essentially but complacently on self-correction, yet scientific publishing raises severe disincentives against such correction. One publisher states that it will charge the author who initiates withdrawal of a published paper US$10,000. Here we summarize our experience, the main barriers we encountered, and our thoughts on how to make published science more rigorous. (Details of other resolved issues are available on request.)\nSIX PROBLEMS Editors are often unable or reluctant to take speedy and appropriate action. For one paper, we obtained raw data deposited online, received institutional approval to reanalyse the data, and submitted a letter to the editor (through the manuscriptsubmission system) describing a need for correction within two weeks. After nine months, we asked the journal why, at minimum, an expression of concern had not been posted. An editor admitted that they had not anticipated the process taking as long as it had. The journal communicated its decision to accept our letter and retract the article 11 months after our submission. The letter and retraction have yet to be published. Where to send expressions of concern is unclear. Journals rarely state whom to contact about potentially invalidating errors. We had to guess whether to send letters to a staff member or editor, formally submit the letter as a manuscript, or contact the authors of a paper directly. On a few occasions, we opted to contact authors when an apparent invalidating error may have merely been an ambiguous description. In unequivocal cases, we usually contacted the journal. Often, journals provided no way to contact editors directly, and editorial staff corresponded without identifying themselves; we were unsure whether editors were involved. Journals that acknowledged invalidating errors were reluctant to issue retractions. In one case, we and others found that a paper had mistakenly argued that a statistical adjustment introduced bias, and we submitted a letter to the editor through the journal’s submission system. An external statistical review subsequently commissioned by the journal confirmed the error. The authors were asked to retract the article, but they refused. The journal ultimately posted the authors’ response to our letter and a summary of commissioned reviewers’ criticism. An accompanying editorial published by the journal stated that “it is each author’s responsibility to make sure that statistical procedures are correctly used and valid for the study submitted”. Journals charge authors to correct others’ mistakes. For one article that we believed contained an invalidating error, our options were to post a comment in an online commenting system or pay a ‘discounted’ submission fee of US$1,716. With another journal from the same publisher, the fee was £1,470 (US$2,100) to publish a letter. Letters from the journal advised that “we are unable to take editorial considerations into account when assessing waiver requests, only the author’s documented ability to pay”. The Committee on Publication Ethics, an independent body that provides advice on how to handle research misconduct, asserts that readers should not have to pay to read retractions. To our knowledge, no authority has discussed whether third parties should be charged to correct errors. No standard mechanism exists to request raw data. When we were able to access data online, we could quickly confirm suspected errors. In at least two cases, we requested data from the authors but received summaries of calculations instead. Sometimes we received no data at all, at which point it was not clear whether journal staff should step in. One journal did retract a paper when its authors refused to show their data or explain discrepancies that we had identified and alerted the journal to in a letter. Working directly with authors can delay correction. After we contacted authors about another paper, they offered to reanalyse the data to address our concerns. After a month with no response, we submitted a letter of concern to the journal. The letter was peer-reviewed and accepted within three weeks. The authors, when made aware of the pending publication of our letter, e-mailed us to state that they would prepare a reply, and we asked the journal not to publish our letter so that we could collaborate with the original authors. That process is ongoing, ten months after we identified the error. Informal expressions of concern are overlooked. Although online platforms such as PubMed Commons offer a convenient way to comment on published papers, they do not include a mediating role for journal editors, and the comments are not incorporated into the literature. Posted concerns are rarely prominent on journals’ websites and are not crossreferenced in any useful way. As a result, readers may assume that a flawed paper is correct, potentially leading to misinformed decisions in science, patient care and public policy. In one case, we chose to post a comment on the journal website and on PubMed Commons after months of private correspondence, in which the authors shared some supplementary data and said that they were preparing a full response. The concerns have been acknowledged but remain unaddressed 15 months after we contacted authors and the journal, and 6 months after we posted our comment (see go.nature.com/fv8tr2).\nWHAT CAN BE DONE? Journals have guidelines for paper submissions and peer review. The Committee on Publication Ethics has outlined recommendations for journals to address problems in areas such as authorship and review. But there is little formal guidance for post-publication corrections. (For our recommendations, see ‘Fixing post-publication peer review’.) Journals, publishers and scientific societies should standardize, streamline and publicize these processes. Authors and journals should share data and code quickly when questions arise. Researchers can aid this process by accessing statistical expertise for experimental design and analysis. Ideally, anyone who detects a potential problem with a study will engage, whether by writing to authors and editors or by commenting online, and will do so in a collegial way. Scientists who engage in post-publication review often do so out of a sense of duty to their community, but this important work does not come with the same prestige as other scientific endeavours. Recognizing and incentivizing such activities could go a long way to cleaning up the literature. Our work was not a systematic search; we simply looked more closely at papers that caught our eye and that we were prepared to assess. We do not know the rate of errors or the motivations behind them (that is, whether they are honest mistakes or a ‘sleight of statistics’). But we showed that a small team of investigators with expertise in statistics and experimental design could find dozens of problematic papers while keeping abreast of the literature. Most were detected simply by reading the paper. A more formal survey would help to determine whether our experiences reflect science in general and whether our recommendations are feasible or effective. Others working to correct the scientific record have encountered similar challenges. Ben Goldacre, a physician and campaigner who is leading COMPare, a project that checks that clinical trials report the outcomes they said they would, told Retraction Watch: “This is a phenomenally laborious process. Not a week goes by that we don’t curse the day we set out to do this.” Robust science needs robust corrections. It is time to make the process less onerous.\nDavid B. Allison is a distinguished professor in the Department of Biostatistics, School of Public Health, University of Alabama at Birmingham, Alabama, USA. Andrew W. Brown is a scientist in the Office of Energetics and the Nutrition Obesity Research Center, University of Alabama at Birmingham, Alabama, USA. Brandon J. George is a statistician in the Office of Energetics, University of Alabama at Birmingham, Alabama, USA. Kathryn A. Kaiser is an instructor in the Office of Energetics and the Nutrition Obesity Research Center, University of Alabama at Birmingham, Alabama, USA. e-mail: dallison@uab.edu\n\nBrown, A. W. et al. Child. Obes. 10, 542–545 (2014).\nLi, P. et al. Obes. Facts 8, 127–129 (2015).\nHauner, H. Obes. Facts 8, 125–126 (2015)\nGeorge, B. J., Brown, A. W. & Allison, D. B. J. Paramedical Sci. 6, 153–154 (2015).\nGeorge, B. J., Goldsby, T. U., Brown, A. W., Li, P. & Allison, D. B. Int. J. Yoga 9, 87–88 (2016).\nThomas, D. M. et al. World J. Acupunct. Moxibustion 25, 66–67 (2015).\nGeorge, B. J. et al. Obesity (in the press).\nBrown, A. W. et al. Am. J. Clin. Nutr. 102, 241–248 (2015).\nObesity 23, 2522 (2015). 10.Bland, J. M. & Altman, D. G. Am. J. Clin. Nutr. 102, 991–994 (2015).\n\nD.B.A. declares competing financial interests: see go.nature.com/hshkkk for details.\n4 FEBRUARY 2016 | VOL 530 | NATURE | 29 COMMENT “Authors and journals should share data and code quickly when questions arise.” © 2016 Macmillan Publishers Limited. All rights reserved"
  },
  {
    "objectID": "docs/wikipedia-literate-programming.html",
    "href": "docs/wikipedia-literate-programming.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "From Wikipedia, the free encyclopedia\nLiterate programming is a programming paradigm introduced in 1984 by Donald Knuth in which a computer program is given as an explanation of how it works in a natural language (such as English), interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. The approach is used in scientific computing and data science for reproducible research, and in other fields where code and documentation are closely integrated.\n\n\nLiterate programming was first introduced by Donald Knuth, who intended it to create programs that were suitable literature for human beings. He implemented it at Stanford University as a part of his research on algorithms and digital typography. The implementation was called WEB, since he believed that it was one of the few three-letter words of English that had not yet been applied to computing. The practice of literate programming has seen an important resurgence in the 2010s with the use of computational notebooks, especially in data science.\n\n\n\nLiterate programming is writing out the program logic in a human language with included (separated by a primitive markup) code snippets and macros. Macros in a literate source file are simply title-like or explanatory phrases in a human language that describe human abstractions created while solving the programming problem, and hiding chunks of code or lower-level macros. These macros are similar to the algorithms in pseudocode typically used in teaching computer science. These arbitrary explanatory phrases become precise new operators, created on the fly by the programmer, forming a meta-language on top of the underlying programming language.\nA preprocessor is used to substitute arbitrary hierarchies, or rather “interconnected ‘webs’ of macros”, to produce the compilable source code with one command (“tangle”), and documentation with another (“weave”). The preprocessor also provides an ability to write out the content of the macros and to add to already created macros in any place in the text of the literate program source file, thereby disposing of the need to keep in mind the restrictions imposed by traditional programming languages or to interrupt the flow of thought.\n\n\nAccording to Knuth, literate programming provides higher-quality programs, since it forces programmers to explicitly state the thoughts behind the program, making poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. The resulting documentation allows the author to restart their own thought processes at any later time, and allows other programmers to understand the construction of the program more easily. This differs from traditional documentation, in which a programmer is presented with source code that follows a compiler-imposed order, and must decipher the thought process behind the program from the code and its associated comments. The meta-language capabilities of literate programming are also claimed to facilitate thinking, giving a higher “bird’s eye view” of the code and increasing the number of concepts the mind can successfully retain and process. Applicability of the concept to programming on a large scale, that of commercial-grade programs, is proven by an edition of TeX code as a literate program.\nKnuth also claims that literate programming can lead to easy porting of software to multiple environments, and even cites the implementation of TeX as an example.\n\n\n\nLiterate programming is very often misunderstood to refer only to formatted documentation produced from a common file with both source code and comments – which is properly called documentation generation – or to voluminous commentaries included with code. This is the converse of literate programming: well-documented code or documentation extracted from code follows the structure of the code, with documentation embedded in the code; while in literate programming, code is embedded in documentation, with the code following the structure of the documentation.\nThis misconception has led to claims that comment-extraction tools, such as the Perl Plain Old Documentation or Java Javadoc systems, are “literate programming tools”. However, because these tools do not implement the “web of abstract concepts” hiding behind the system of natural-language macros, or provide an ability to change the order of the source code from a machine-imposed sequence to one convenient to the human mind, they cannot properly be called literate programming tools in the sense intended by Knuth.\n\n\n\n\nImplementing literate programming consists of two steps:\n\nWeaving: Generating a comprehensive document about the program and its maintenance.\nTangling: Generating machine executable code\n\nWeaving and tangling are done on the same source so that they are consistent with each other.\n\n\n\nA classic example of literate programming is the literate implementation of the standard Unix wc word counting program. Knuth presented a CWEB version of this example in Chapter 12 of his Literate Programming book. The same example was later rewritten for the noweb literate programming tool. This example provides a good illustration of the basic elements of literate programming.\n\n\nThe following snippet of the wc literate program shows how arbitrary descriptive phrases in a natural language are used in a literate program to create macros, which act as new “operators” in the literate programming language, and hide chunks of code or other macros. The mark-up notation consists of double angle brackets (&lt;&lt;...&gt;&gt;) that indicate macros. The @ symbol, in a noweb file, indicates the beginning of a documentation chunk. The &lt;&lt;*&gt;&gt; symbol stands for the “root”, topmost node the literate programming tool will start expanding the web of macros from. Actually, writing out the expanded source code can be done from any section or subsection (i.e. a piece of code designated as &lt;&lt;name of the chunk&gt;&gt;=, with the equal sign), so one literate program file can contain several files with machine source code.\n\n\n\nMacros are not the same as “section names” in standard documentation. Literate programming macros hide the real code behind themselves, and can be used inside any low-level machine language operators, often inside logical operators such as if, while or case. This can be seen in the following wc literate program.\n\n\n\nIn a noweb literate program, besides the free order of their exposition, the chunks behind macros, once introduced with &lt;&lt;...&gt;&gt;=, can be grown later in any place in the file by simply writing &lt;&lt;name of the chunk&gt;&gt;= and adding more content to it. This allows the programmer to present the logic in the order that is most natural for human understanding, rather than the order required by the compiler.\n\n\n\nThe documentation for a literate program is produced as part of writing the program. Instead of comments provided as side notes to source code, a literate program contains the explanation of concepts on each level, with lower level concepts deferred to their appropriate place, which allows for better communication of thought. The exposition of ideas creates the flow of thought that is like a literary work.\n\n\n\n\nAxiom, a computer algebra system, is totally written as a literate program.\nKnuth’s TeX typesetting system and its source code are classic examples.\n\n\n\n\n\nThe first published literate programming environment was WEB, introduced by Knuth in 1981 for his TeX typesetting system; it uses Pascal as its underlying programming language and TeX for typesetting of the documentation. The complete commented TeX source code was published in Knuth’s TeX: The program, volume B of his 5-volume Computers and Typesetting. Knuth had privately used a literate programming system called DOC as early as 1979. He was inspired by the ideas of Pierre-Arnoul de Marneffe. The free CWEB, written by Knuth and Silvio Levy, is WEB adapted for C and C++, runs on most operating systems, and can produce TeX and PDF documentation.\nThere are various other implementations of the literate programming concept, including noweb, Emacs org-mode, Jupyter Notebooks, Sweave, Knitr, and others. Many of the newer among these do not have macros and hence do not comply with the order of human logic principle, which makes them perhaps “semi-literate” tools. These, however, allow cellular execution of code which makes them more along the lines of exploratory programming tools.\n\n\n\n\nDocumentation generator\nNotebook interface\nSweave\nKnitr\nSelf-documenting code\n\n\n\n\nFor the full list of references, see the Wikipedia article.\n\n\n\n\nSewell, Wayne (1989). Weaving a Program: Literate Programming in WEB. Van Nostrand Reinhold.\nKnuth, Donald E. (1992). Literate Programming. Stanford University Center for the Study of Language and Information.\nGurari, Eitan M. (1994). TeX & LaTeX: Drawing and Literate Programming. McGraw Hill.\nNørmark, Kurt (1998). Literate Programming – Issues and Problems. University of Aalborg.\nSchulte, Eric (2012). A Multi-Language Computing Environment for Literate Programming and Reproducible Research. Journal of Statistical Software.\nMall, Daniel. Literate Programming.\nWalsh, Norman (2002). Literate Programming in XML. XML 2002.\n\n\n\n\n\nLiterateProgramming at WikiWikiWeb\nLiterate Programming FAQ at CTAN\n\n\nThis page is based on content from the Wikipedia article on Literate programming, available under the Creative Commons Attribution-ShareAlike 4.0 License."
  },
  {
    "objectID": "docs/wikipedia-literate-programming.html#history-and-philosophy",
    "href": "docs/wikipedia-literate-programming.html#history-and-philosophy",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Literate programming was first introduced by Donald Knuth, who intended it to create programs that were suitable literature for human beings. He implemented it at Stanford University as a part of his research on algorithms and digital typography. The implementation was called WEB, since he believed that it was one of the few three-letter words of English that had not yet been applied to computing. The practice of literate programming has seen an important resurgence in the 2010s with the use of computational notebooks, especially in data science."
  },
  {
    "objectID": "docs/wikipedia-literate-programming.html#concept",
    "href": "docs/wikipedia-literate-programming.html#concept",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Literate programming is writing out the program logic in a human language with included (separated by a primitive markup) code snippets and macros. Macros in a literate source file are simply title-like or explanatory phrases in a human language that describe human abstractions created while solving the programming problem, and hiding chunks of code or lower-level macros. These macros are similar to the algorithms in pseudocode typically used in teaching computer science. These arbitrary explanatory phrases become precise new operators, created on the fly by the programmer, forming a meta-language on top of the underlying programming language.\nA preprocessor is used to substitute arbitrary hierarchies, or rather “interconnected ‘webs’ of macros”, to produce the compilable source code with one command (“tangle”), and documentation with another (“weave”). The preprocessor also provides an ability to write out the content of the macros and to add to already created macros in any place in the text of the literate program source file, thereby disposing of the need to keep in mind the restrictions imposed by traditional programming languages or to interrupt the flow of thought.\n\n\nAccording to Knuth, literate programming provides higher-quality programs, since it forces programmers to explicitly state the thoughts behind the program, making poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. The resulting documentation allows the author to restart their own thought processes at any later time, and allows other programmers to understand the construction of the program more easily. This differs from traditional documentation, in which a programmer is presented with source code that follows a compiler-imposed order, and must decipher the thought process behind the program from the code and its associated comments. The meta-language capabilities of literate programming are also claimed to facilitate thinking, giving a higher “bird’s eye view” of the code and increasing the number of concepts the mind can successfully retain and process. Applicability of the concept to programming on a large scale, that of commercial-grade programs, is proven by an edition of TeX code as a literate program.\nKnuth also claims that literate programming can lead to easy porting of software to multiple environments, and even cites the implementation of TeX as an example.\n\n\n\nLiterate programming is very often misunderstood to refer only to formatted documentation produced from a common file with both source code and comments – which is properly called documentation generation – or to voluminous commentaries included with code. This is the converse of literate programming: well-documented code or documentation extracted from code follows the structure of the code, with documentation embedded in the code; while in literate programming, code is embedded in documentation, with the code following the structure of the documentation.\nThis misconception has led to claims that comment-extraction tools, such as the Perl Plain Old Documentation or Java Javadoc systems, are “literate programming tools”. However, because these tools do not implement the “web of abstract concepts” hiding behind the system of natural-language macros, or provide an ability to change the order of the source code from a machine-imposed sequence to one convenient to the human mind, they cannot properly be called literate programming tools in the sense intended by Knuth."
  },
  {
    "objectID": "docs/wikipedia-literate-programming.html#workflow",
    "href": "docs/wikipedia-literate-programming.html#workflow",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Implementing literate programming consists of two steps:\n\nWeaving: Generating a comprehensive document about the program and its maintenance.\nTangling: Generating machine executable code\n\nWeaving and tangling are done on the same source so that they are consistent with each other."
  },
  {
    "objectID": "docs/wikipedia-literate-programming.html#example",
    "href": "docs/wikipedia-literate-programming.html#example",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "A classic example of literate programming is the literate implementation of the standard Unix wc word counting program. Knuth presented a CWEB version of this example in Chapter 12 of his Literate Programming book. The same example was later rewritten for the noweb literate programming tool. This example provides a good illustration of the basic elements of literate programming.\n\n\nThe following snippet of the wc literate program shows how arbitrary descriptive phrases in a natural language are used in a literate program to create macros, which act as new “operators” in the literate programming language, and hide chunks of code or other macros. The mark-up notation consists of double angle brackets (&lt;&lt;...&gt;&gt;) that indicate macros. The @ symbol, in a noweb file, indicates the beginning of a documentation chunk. The &lt;&lt;*&gt;&gt; symbol stands for the “root”, topmost node the literate programming tool will start expanding the web of macros from. Actually, writing out the expanded source code can be done from any section or subsection (i.e. a piece of code designated as &lt;&lt;name of the chunk&gt;&gt;=, with the equal sign), so one literate program file can contain several files with machine source code.\n\n\n\nMacros are not the same as “section names” in standard documentation. Literate programming macros hide the real code behind themselves, and can be used inside any low-level machine language operators, often inside logical operators such as if, while or case. This can be seen in the following wc literate program.\n\n\n\nIn a noweb literate program, besides the free order of their exposition, the chunks behind macros, once introduced with &lt;&lt;...&gt;&gt;=, can be grown later in any place in the file by simply writing &lt;&lt;name of the chunk&gt;&gt;= and adding more content to it. This allows the programmer to present the logic in the order that is most natural for human understanding, rather than the order required by the compiler.\n\n\n\nThe documentation for a literate program is produced as part of writing the program. Instead of comments provided as side notes to source code, a literate program contains the explanation of concepts on each level, with lower level concepts deferred to their appropriate place, which allows for better communication of thought. The exposition of ideas creates the flow of thought that is like a literary work.\n\n\n\n\nAxiom, a computer algebra system, is totally written as a literate program.\nKnuth’s TeX typesetting system and its source code are classic examples."
  },
  {
    "objectID": "docs/wikipedia-literate-programming.html#literate-programming-practices",
    "href": "docs/wikipedia-literate-programming.html#literate-programming-practices",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "The first published literate programming environment was WEB, introduced by Knuth in 1981 for his TeX typesetting system; it uses Pascal as its underlying programming language and TeX for typesetting of the documentation. The complete commented TeX source code was published in Knuth’s TeX: The program, volume B of his 5-volume Computers and Typesetting. Knuth had privately used a literate programming system called DOC as early as 1979. He was inspired by the ideas of Pierre-Arnoul de Marneffe. The free CWEB, written by Knuth and Silvio Levy, is WEB adapted for C and C++, runs on most operating systems, and can produce TeX and PDF documentation.\nThere are various other implementations of the literate programming concept, including noweb, Emacs org-mode, Jupyter Notebooks, Sweave, Knitr, and others. Many of the newer among these do not have macros and hence do not comply with the order of human logic principle, which makes them perhaps “semi-literate” tools. These, however, allow cellular execution of code which makes them more along the lines of exploratory programming tools."
  },
  {
    "objectID": "docs/wikipedia-literate-programming.html#see-also",
    "href": "docs/wikipedia-literate-programming.html#see-also",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Documentation generator\nNotebook interface\nSweave\nKnitr\nSelf-documenting code"
  },
  {
    "objectID": "docs/wikipedia-literate-programming.html#references",
    "href": "docs/wikipedia-literate-programming.html#references",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "For the full list of references, see the Wikipedia article."
  },
  {
    "objectID": "docs/wikipedia-literate-programming.html#further-reading",
    "href": "docs/wikipedia-literate-programming.html#further-reading",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Sewell, Wayne (1989). Weaving a Program: Literate Programming in WEB. Van Nostrand Reinhold.\nKnuth, Donald E. (1992). Literate Programming. Stanford University Center for the Study of Language and Information.\nGurari, Eitan M. (1994). TeX & LaTeX: Drawing and Literate Programming. McGraw Hill.\nNørmark, Kurt (1998). Literate Programming – Issues and Problems. University of Aalborg.\nSchulte, Eric (2012). A Multi-Language Computing Environment for Literate Programming and Reproducible Research. Journal of Statistical Software.\nMall, Daniel. Literate Programming.\nWalsh, Norman (2002). Literate Programming in XML. XML 2002."
  },
  {
    "objectID": "docs/wikipedia-literate-programming.html#external-links",
    "href": "docs/wikipedia-literate-programming.html#external-links",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "LiterateProgramming at WikiWikiWeb\nLiterate Programming FAQ at CTAN\n\n\nThis page is based on content from the Wikipedia article on Literate programming, available under the Creative Commons Attribution-ShareAlike 4.0 License."
  },
  {
    "objectID": "docs/hernandez2023.html",
    "href": "docs/hernandez2023.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Authors: José Armando Hernández, Miguel Colom\nSource: HAL (hal-04322522), 2023\nOriginal Record\n\n\n\nWith the recognized crisis of credibility in scientific research, there is a growth of reproducibility studies in computer science. This work surveys the gap between reproducibility-oriented practices, journal policies, recommendations, publisher artifact guidelines, and their effective adoption. The authors analyze the relationship between authors and journals, propose recommendations for journal policies, and a unified reproducibility guide for authors. The study includes a survey of 200 articles and 16 journals, classifying them by reproducibility strategies, technologies, and policies.\nKeywords: Repeatability, Reproducibility, Replicability, Reusability, Data Science, AI/ML, Scientific journal, Trustworthy, Data Citation, Rewarding Research, Reproducible Research.\n\n\n\n\nReproducibility is a broad and complex topic strongly related to the history of science and knowledge. The evolution of science through reproducibility can be compared to DNA replication, transmitting knowledge through generations. Scientific journals play a significant role in communicating, validating, and accepting reliable knowledge.\nThe reproducibility crisis has become relevant due to concerns for ethics and transparency, especially with the rise of AI/ML. Publications have evolved towards data-centric and model-centric developments, forcing journals to adapt their business models. This paper analyzes journal policies concerning reproducibility, focusing on computer science journals and the management of software/data-based articles.\n\n\n\n\nReproducibility is considered a fundamental part of the scientific method. The National Academies of Sciences, Engineering, and Medicine (NASEM) defines reproducibility as obtaining consistent computational results using the same input data, code, and methods, while replicability involves new data and similar methods. The ACM’s Artifact Review and Badging report provides definitions for reproducibility, repeatability, replicability, and reusability (4R), with distinctions based on team, setup, code, and environment.\n\n\n\n\n\nExperimental reproducibility: Similar input and protocol yield similar results.\nStatistical reproducibility: Same input and analysis yield same conclusions.\nComputational reproducibility: Same input, code, and environment yield exact results.\n\n\n\n\n\n\nComplex software dependencies (“dependency hell”)\nLow writing quality and poor documentation\nCompilation and infrastructure setup challenges\nFloating point operation differences\nNeed for adapted operating systems and isolated environments\nLack of academic reward for reproducible work\n\n\n\n\n\n\n\n\nUse of platforms like GitHub, Zenodo, Software Heritage, and others\nFAIR data principles: Findable, Accessible, Interoperable, Reusable\n\n\n\n\n\nImportance of open datasets, standardized formats, and benchmarks\nFederated learning for sensitive/confidential data\n\n\n\n\n\nMicroservices and serverless functions for modular, reproducible systems\n\n\n\n\n\nNotebooks (Jupyter), containers (Docker, Singularity), cloud computing\nWorkflow management systems (Taverna, Galaxy, Pegasus, Nextflow, etc.)\nMLOps and AIOps for managing reproducibility in ML pipelines\nInfrastructure as Code (IaC) for replicable environments\nProvenance and metadata traceability\nReproducibility as a Service (RaaS)\n\n\n\n\n\nAdoption of best practices, methodologies (CRISP-DM, KDD, etc.)\nGuides like The Turing Way\n\n\n\n\n\nPublishers’ responsibility for scientific integrity and reproducibility\nInitiatives for associating code with publications and evaluating software artifacts\n\n\n\n\n\n\nA survey of 16 computer science journals was conducted to assess reproducibility policies and practices. Key findings: - 75% have a reproducibility policy - 41.7% make reproducibility mandatory for publication - Most journals are open access and use free platforms for code/data sharing (e.g., GitHub, Zenodo) - Compliance with FAIR principles is increasing but still limited - Peer review remains the main validation method - Preferred rewards for authors: reproducibility badges, free access, or discounts - Reviewers are rewarded with free access, editorial board invitations, or voluntary recognition - Implementation of reproducibility policies is still low, especially for automation and persistent identifiers\n\n\n\n\n\nBenefits of reproducibility: credibility, recognition, increased impact, collaboration, and confidence in results\nChallenges: over-reliance on containers, maintenance, and security\nShared responsibility between authors and publishers; costs should not fall solely on researchers\nReproducibility as a Service (RaaS) can help distribute costs and responsibilities\nSoftware should be recognized as a valuable research artifact, with proper citation and preservation\nBadges and new metrics (e.g., Scientific Impact Factor) can incentivize reproducible software\n\n\n\n\n\n\nAuthors: Should strive for 100% reproducibility or clarify limitations; follow standardized guides for artifact submission\nPublishers: Should support reproducibility infrastructure, possibly via third parties (RaaS), and reward authors appropriately\n\n\n\n\n\nA systematic review and journal survey reveal that reproducibility policies and technologies are advancing but still face significant gaps. Standardized guides, artifact evaluation criteria, and coordinated efforts among authors, publishers, and technology providers are needed to close the reproducibility gap and ensure trustworthy scientific publications.\n\nReferences: A comprehensive list of references is included in the original PDF, covering reproducibility definitions, technological tools, survey studies, and best practices.\nLicense: CC BY-ND 4.0\n\nFor full details, figures, and references, see the full record and PDF."
  },
  {
    "objectID": "docs/hernandez2023.html#abstract",
    "href": "docs/hernandez2023.html#abstract",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "With the recognized crisis of credibility in scientific research, there is a growth of reproducibility studies in computer science. This work surveys the gap between reproducibility-oriented practices, journal policies, recommendations, publisher artifact guidelines, and their effective adoption. The authors analyze the relationship between authors and journals, propose recommendations for journal policies, and a unified reproducibility guide for authors. The study includes a survey of 200 articles and 16 journals, classifying them by reproducibility strategies, technologies, and policies.\nKeywords: Repeatability, Reproducibility, Replicability, Reusability, Data Science, AI/ML, Scientific journal, Trustworthy, Data Citation, Rewarding Research, Reproducible Research."
  },
  {
    "objectID": "docs/hernandez2023.html#introduction",
    "href": "docs/hernandez2023.html#introduction",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Reproducibility is a broad and complex topic strongly related to the history of science and knowledge. The evolution of science through reproducibility can be compared to DNA replication, transmitting knowledge through generations. Scientific journals play a significant role in communicating, validating, and accepting reliable knowledge.\nThe reproducibility crisis has become relevant due to concerns for ethics and transparency, especially with the rise of AI/ML. Publications have evolved towards data-centric and model-centric developments, forcing journals to adapt their business models. This paper analyzes journal policies concerning reproducibility, focusing on computer science journals and the management of software/data-based articles."
  },
  {
    "objectID": "docs/hernandez2023.html#definitions",
    "href": "docs/hernandez2023.html#definitions",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Reproducibility is considered a fundamental part of the scientific method. The National Academies of Sciences, Engineering, and Medicine (NASEM) defines reproducibility as obtaining consistent computational results using the same input data, code, and methods, while replicability involves new data and similar methods. The ACM’s Artifact Review and Badging report provides definitions for reproducibility, repeatability, replicability, and reusability (4R), with distinctions based on team, setup, code, and environment."
  },
  {
    "objectID": "docs/hernandez2023.html#types-of-reproducibility",
    "href": "docs/hernandez2023.html#types-of-reproducibility",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Experimental reproducibility: Similar input and protocol yield similar results.\nStatistical reproducibility: Same input and analysis yield same conclusions.\nComputational reproducibility: Same input, code, and environment yield exact results."
  },
  {
    "objectID": "docs/hernandez2023.html#difficulties-in-achieving-reproducibility",
    "href": "docs/hernandez2023.html#difficulties-in-achieving-reproducibility",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Complex software dependencies (“dependency hell”)\nLow writing quality and poor documentation\nCompilation and infrastructure setup challenges\nFloating point operation differences\nNeed for adapted operating systems and isolated environments\nLack of academic reward for reproducible work"
  },
  {
    "objectID": "docs/hernandez2023.html#reproducibility-strategies-and-technological-evolution",
    "href": "docs/hernandez2023.html#reproducibility-strategies-and-technological-evolution",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Use of platforms like GitHub, Zenodo, Software Heritage, and others\nFAIR data principles: Findable, Accessible, Interoperable, Reusable\n\n\n\n\n\nImportance of open datasets, standardized formats, and benchmarks\nFederated learning for sensitive/confidential data\n\n\n\n\n\nMicroservices and serverless functions for modular, reproducible systems\n\n\n\n\n\nNotebooks (Jupyter), containers (Docker, Singularity), cloud computing\nWorkflow management systems (Taverna, Galaxy, Pegasus, Nextflow, etc.)\nMLOps and AIOps for managing reproducibility in ML pipelines\nInfrastructure as Code (IaC) for replicable environments\nProvenance and metadata traceability\nReproducibility as a Service (RaaS)\n\n\n\n\n\nAdoption of best practices, methodologies (CRISP-DM, KDD, etc.)\nGuides like The Turing Way\n\n\n\n\n\nPublishers’ responsibility for scientific integrity and reproducibility\nInitiatives for associating code with publications and evaluating software artifacts"
  },
  {
    "objectID": "docs/hernandez2023.html#survey-in-computer-science-journals",
    "href": "docs/hernandez2023.html#survey-in-computer-science-journals",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "A survey of 16 computer science journals was conducted to assess reproducibility policies and practices. Key findings: - 75% have a reproducibility policy - 41.7% make reproducibility mandatory for publication - Most journals are open access and use free platforms for code/data sharing (e.g., GitHub, Zenodo) - Compliance with FAIR principles is increasing but still limited - Peer review remains the main validation method - Preferred rewards for authors: reproducibility badges, free access, or discounts - Reviewers are rewarded with free access, editorial board invitations, or voluntary recognition - Implementation of reproducibility policies is still low, especially for automation and persistent identifiers"
  },
  {
    "objectID": "docs/hernandez2023.html#technological-discussion",
    "href": "docs/hernandez2023.html#technological-discussion",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Benefits of reproducibility: credibility, recognition, increased impact, collaboration, and confidence in results\nChallenges: over-reliance on containers, maintenance, and security\nShared responsibility between authors and publishers; costs should not fall solely on researchers\nReproducibility as a Service (RaaS) can help distribute costs and responsibilities\nSoftware should be recognized as a valuable research artifact, with proper citation and preservation\nBadges and new metrics (e.g., Scientific Impact Factor) can incentivize reproducible software"
  },
  {
    "objectID": "docs/hernandez2023.html#efforts-of-authors-and-publishers",
    "href": "docs/hernandez2023.html#efforts-of-authors-and-publishers",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Authors: Should strive for 100% reproducibility or clarify limitations; follow standardized guides for artifact submission\nPublishers: Should support reproducibility infrastructure, possibly via third parties (RaaS), and reward authors appropriately"
  },
  {
    "objectID": "docs/hernandez2023.html#conclusion",
    "href": "docs/hernandez2023.html#conclusion",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "A systematic review and journal survey reveal that reproducibility policies and technologies are advancing but still face significant gaps. Standardized guides, artifact evaluation criteria, and coordinated efforts among authors, publishers, and technology providers are needed to close the reproducibility gap and ensure trustworthy scientific publications.\n\nReferences: A comprehensive list of references is included in the original PDF, covering reproducibility definitions, technological tools, survey studies, and best practices.\nLicense: CC BY-ND 4.0\n\nFor full details, figures, and references, see the full record and PDF."
  },
  {
    "objectID": "docs/team2021distill.html",
    "href": "docs/team2021distill.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Distill\nAbout Prize Submit"
  },
  {
    "objectID": "docs/team2021distill.html#changes-in-how-we-think-about-distill",
    "href": "docs/team2021distill.html#changes-in-how-we-think-about-distill",
    "title": "Workshop on Reproducible Research",
    "section": "Changes in How We Think About Distill",
    "text": "Changes in How We Think About Distill\n\nMentorship is in Tension with Being a Journal\nBehind the scenes, the largest function of Distill is providing feedback and mentorship. For some of our early articles, we provided more than 50 hours of help with designing diagrams, improving writing style, and shaping scientific communication. Although we’ve generally dialed this down over time, each article still requires significant work. All of this is done by our editors in a volunteer capacity, on top of their regular work responsibilities.\nThe first problem with providing mentorship through an editorial role is that it’s not a very good mechanism for distributing mentorship. Ideally, one wants to provide mentorship early on in projects, to mentees with similar interests, and to a number of mentees that one is capable of providing good mentorship to. Providing mentorship to everyone who submits an article to Distill is overwhelming. Another problem is that our advice is often too late because the article’s foundation is already set. Finally, many authors don’t realize the amount of effort it takes to publish a Distill article.\nProviding mentorship also creates a challenging dual relationship for an editor. They have both the role of closely supporting and championing the author while also having to accept or reject them in the end. We’ve found this to be difficult for both the mentor and mentee.\nFinally, the kind of deeply-engaged editing and mentorship that we sometimes provide can often amount to an authorship level contribution, with authors offering co-authorship to editors. This is especially true when an editor was a mentor from early on. In many ways, co-authorship would create healthy incentives, rewarding the editor for spending tens of hours improving the article. But it creates a conflict of interest if the editor is to be an independent decision maker, as the journal format suggests they should be. And even if another editor takes over, it’s a political risk: Distill is sometimes criticized for publishing too many articles with editors as authors.\n\n\nEditor Articles are in Tension with Being a Journal\nAnother important impact of Distill has been articles written by the editors themselves. Distill’s editorial team consists of volunteer researchers who are deeply excited about explanations and interactive articles and have a long history of doing so. Since the set of people with these interests is small, a non-trivial fraction of Distill’s publications have come from editors. In other cases, authors of existing Distill articles were later invited to become an editor.\nEditor articles are sometimes cited as a sign of a kind of corruption for Distill, that Distill is a vehicle for promoting editors. We can see how it might seem dubious for a journal to publish articles by people running it, even if editorial decisions are made by an editor who is at arms-length. This has led Distill to avoid publishing several editor articles despite believing that they are of value to readers.\nWe believe that editor articles are actually a good thing about Distill. Each one represents an immense amount of effort in trying new things scientific publishing. Given the large volume of readers and the positive informal comments we receive, we suspect that for every critic there are many silent but happy readers.\nWhen a structure turns a public good into an appearance of corruption, it suggests it might not be such a good structure. As editors, we want to share our work with the world in a way that is not seen as corrupt.\n\n\nNeutral venues can be achieved in other ways\nThe vast majority of Distill articles are written by multiple authors, often from multiple institutions. As a result, an important function of Distill is providing somewhere to publish that isn’t someone’s home turf. If a Distill article were published on one person or organization’s blog, it could lead to a perception that it is primarily theirs and make other authors feel less comfortable with collaboration. Arxiv normally fills this role, but it only supports PDFs.\nBut it turns out there’s a simpler solution: self publication on one-off websites. David Ha and his collaborators have done a great job demonstrating this, using the Distill template and GitHub pages to self-publish articles (eg. the world models article). In these cases, the articles are standalone rather than being with a particular author or institution.\n\n\nSelf-Publication Seems Like the Future (in most cases)\nIn many areas of physics, self publishing on Arxiv has become the dominant mode of publication. A great deal of machine learning research is also published on Arxiv. We think this type of self-publication is likely the future for a large fraction of publication, possibly along with alternative models of review that are separated from a publisher.\nJournal-led peer review provides many benefits. It can protect against scientific misinformation and non-reproducible results. It can save the research community time by filtering out papers that aren’t worth engaging with. It can provide feedback to junior researchers who may not have other sources of feedback. It can push research groups studying similar topics across institutions to engage with each other’s criticism. And double-blind review may support equity and fairness.\nBut is traditional journal-led peer review the most effective way to achieve these benefits? And is it worth the enormous costs it imposes on editors, reviewers, authors, and readers?\nFor example, avoiding scientific errors, non-reproducible results, and misinformation is certainly important. But for every paper where there’s a compelling public interest in avoiding misinformation (eg. papers about COVID), there are thousands of papers whose audience is a handful of the same researchers we ask to perform review. Additionally, it’s not clear how effective peer review actually is at catching errors. We suspect that a structure which focuses on reviewing controversial and important papers would be more effective at this goal. Our experience from discussion articles is that reviewers are willing to spend orders of magnitude more energy when they feel like reviewing a paper genuinely matters to the community, rather than being pro-forma, and their work will be seen as a scientific contribution.\nSimilarly, we suspect that journal-led review isn’t a very effective way of providing feedback to junior researchers or of promoting equity. These are all very worthy aims, and we’d like to free energy to pursue them in effective ways.\nWe also think there’s a lot of upside to self-publication. Self-publication can move very fast. It doesn’t require a paper to fit into the scope of an existing journal. It allows for more innovation in the format of the paper, such as using interactive diagrams as Distill does. And it aligns incentives better.Self-publication may align certain incentives better than traditional publishing. Many papers go through an informal review process before they’re submitted to a journal or self-published, with authors soliciting feedback from colleagues. This informal review process is often smoother, faster, and provides more constructive and more relevant feedback than a traditional review process. Why is that? In a normal review process, the authors have the highest stakes, but little agency in the process. Meanwhile, neither the reviewers nor the editors share the authors’ incentive to move quickly. And the reviewers are often horribly over-subscribed. In contrast, in an informal review process, the authors have a strong incentive to quickly organize the process and reviewers are focused on providing helpful feedback to someone they know, rather than arbitrating a gatekeeping decision.\n\n\nA Half-hearted Distill May Cause Harm\nDistill isn’t living up to our standards of author experience. Originally, we had a vision of a much more engaged, responsive, and rapid review process with editors deeply involved in helping authors improve their article. But the truth is that, with us being quite burnt out, our review process has become much slower and more similar to a typical journal. It’s unclear to us whether the value added by our present review process is worth the time costs we impose on authors.\nDistill also occupies institutional space, potentially discouraging others from starting similar projects. It’s possible that there are others who could execute something like Distill better than us, but aren’t starting their project because Distill exists.\nOn the flip side, Distill often comes up in conversations about the future of publishers and journals in machine learning, as a positive example of the role a journal can play. But if we no longer believe in our model, Distill may be unintentionally supporting something we don’t really stand behind. We may also be setting unrealistic aspirations: if Distill’s level of editorial engagement and editing was unsustainable, even with a deeply passionate set of volunteers and a relatively small number of articles, we should at least be clearly communicating how difficult it is."
  },
  {
    "objectID": "docs/team2021distill.html#why-a-hiatus",
    "href": "docs/team2021distill.html#why-a-hiatus",
    "title": "Workshop on Reproducible Research",
    "section": "Why a Hiatus?",
    "text": "Why a Hiatus?\nWe think that Distill is a really beautiful artifact which illustrates a vision of scientific publishing. But it is not sustainable for us to continue running the journal in its current form. We think preserving it in its present state is more valuable than diluting it with lower quality editing. We also think that it’s a lot healthier for us and frees up our energy to do new projects that provide value to the community.\nWe’ve considered trying to find others to hand Distill off to. But a lot of the value of Distill is illustrating a weird and idiosyncratic vision. We think there’s value in preserving Distill’s original flavor. We are open to changes to better structure Distill, but we feel protective of Distill’s vision and quirkiness.\nAlthough Distill is going on hiatus, the Distill template is open source, and we’d love to see others run with it!\n\nBurnout\nOver the last few years, Distill has experienced a significant amount of volunteer burnout. The fact that multiple volunteers experienced burnout makes us think it’s partly caused by the issues described in previous sections.\nOne of the biggest risk factors in burnout is having conflicting goals, and as the previous sections describe, we’ve had many conflicting goals. We wanted to mentor people, but we also needed to reject them. We wanted to write beautiful articles ourselves, but we also wanted to be an independent venue.\nAnother significant risk factor is having unachievable goals. We set extremely high standards for ourselves: with early articles, volunteer editors would often spend 50 or more hours improving articles that were submitted to Distill and bringing them up to the level of quality we aspired to. This invisible effort was comparable to the work of writing a short article of one’s own. It wasn’t sustainable, and this left us with a constant sense that we were falling short. A related issue is that we had trouble setting well-defined boundaries of what we felt we owed to authors who submitted to us.\nBy discussing these challenges, we hope that future projects like Distill will be able to learn from our experiences and find ways to balance these competing values.\n\n\nAuthor Contributions\nThis article was drafted by Chris Olah, Nick Cammarata, Sam Greydanus, and Janelle Tam.\n\n\nAcknowledgements\nDistill has been supported by too many people over the years to have any hope of thanking everyone. We’re especially grateful to Distill’s authors for investing so much in their articles, and to our reviewers for generously giving so much time to help Distill. We’re also grateful to the many people who helped us as we struggled with this decision over the last few years. Many people took time to talk with us about burn out, about whether Distill was a good structure, about how to wind Distill down graciously, and about this essay. We’re also grateful to past and present Distill authors for being so understanding of our decision.\n\n\nUpdates and Corrections\nIf you see mistakes or want to suggest changes, please create an issue on GitHub.\n\n\nReuse\nDiagrams and text are licensed under Creative Commons Attribution CC-BY 4.0 with the source available on GitHub, unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by a note in their caption: “Figure from …”.\n\n\nCitation\nFor attribution in academic contexts, please cite this work as\nTeam, \"Distill Hiatus\", Distill, 2021.\nBibTeX citation\n@article{team2021distill,\n  author = {Team, Editorial},\n  title = {Distill Hiatus},\n  journal = {Distill},\n  year = {2021},\n  note = {https://distill.pub/2021/distill-hiatus},\n  doi = {10.23915/distill.00031}\n}\nDistill is dedicated to clear explanations of machine learning\nAbout Submit Prize Archive RSS GitHub Twitter      ISSN 2476-0757"
  },
  {
    "objectID": "docs/bastian2016.html",
    "href": "docs/bastian2016.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Author: Hilda Bastian\nSource: Absolutely Maybe (PLOS Blog), Dec 5, 2016\nOriginal Article\n\nCartoon: Science not being self-correcting (see original for image)\nIt’s not a new story, although “the reproducibility crisis” may seem to be. For life sciences, I think it started in the late 1950s. Large-scale problems in clinical research burst into the open in a very public way then.\nBut before we get to that, what is “research reproducibility”? It’s a euphemism for unreliable research or research reporting. Steve Goodman and colleagues (2016) say 3 dimensions of science that affect reliability are at play:\n\nMethods reproducibility – enough detail available to enable a study to be repeated;\nResults reproducibility – the findings are replicated by others;\nInferential reproducibility – similar conclusions are drawn about results, which brings statistics and interpretation squarely into the mix.\n\nThat doesn’t cover everything, for example, the reproducibility of the materials used in laboratory research.\nThere is a lot of history behind each of those. Here are some of the milestones in awareness and proposed solutions that stick out for me.\n\n\n\nCartoon: 1950s (see original for image)\nEstes Kefauver was a U.S. Senator and Adlai Stevenson’s presidential running mate. He had become famous at the start of the decade with hearings into organized crime. More than 30 million people watched some of those hearings, on television or free in movie theaters.\nHe turned his attention to the pharmaceutical industry at the end of the decade, holding hearings into drug prices in 1958 and 1959. One of the issues that emerged was the “sorry state of science supporting drug effectiveness”. This outcry about research reproducibility led to a major change in research requirements for FDA approval in 1962.\nPart of the problem’s solution had a major milestone in 1959, too. Austin Bradford Hill led a meeting in Vienna that codified the methodology for controlled clinical trials.\nEarlier in the 1950s, though, there was a development that stands as a milestone in fueling science’s reproducibility crisis by inadvertently introducing a perverse incentive: Eugene Garfield proposed the journal impact factor in 1955.\n\n\n\n\nCartoon: 1960s (see original for image)\nIn 1962, the Kefauver-Harris amendments required “adequate and well-controlled clinical investigations” for FDA approval: adequate was at least 2 studies – a major step in expecting replication of results.\nA new problem in biological research was revealed in 1966 at a conference in Bethesda. Stanley Gartler was the first to report contamination in cell lines in cancer research (reported in Nature, 1968). Still not adequately dealt with, that’s grown into a juggernaut of un-reproducibility, making thousands of studies unreliable.\nThat year also saw what may be the first example of the science of systematically studying science’s methods – what John Ioannidis and colleagues have dubbed meta-research. It was a study of statistics and methods of evaluation in medical journal publications.\n\n\n\n\nCartoon: 1970s (see original for image)\nThe 70s brought us 2 key methods: systematically reviewing evidence (1971) and meta-analysis: a way to analyze the results of several studies at once (1976). Those methods didn’t just help make sense of bodies of evidence. They also propelled meta-research.\nCartoon: Meta-analysis (see original for image)\nAnd 1977 surfaced a problem in scientists’ behavior that allows unreliable scientific findings to take root and thrive. Michael Mahoney showed confirmation bias thrived in peer review:\n\nConfirmatory bias is the tendency to emphasize and believe experiences which support one’s views and to ignore or discredit those which do not…[R]eviewers were strongly biased against manuscripts which reported results contrary to their theoretical perspective.\n\n\n\n\n\nCartoon: 1980s (see original for image)\nAnalyzing the reliability of studies took a formal step forward in 1981, with the publication of a method for assessing the quality of clinical trials.\nDoug Altman and colleagues took on the issue of problems with the use and interpretation of statistics, with 1983 statistical guidelines for medical journals.\nAnd a way forward was proposed for the problem of unpublished research results. The unpublished research results are often the disappointing ones. That leaves us with a deceptive public record of science.\nJohn Simes called for an international registry of clinical trials in 1986. Registering the details of all research before it is actually done would mean we could at least identify gaps in the published research record down the line.\n\n\n\n\nCartoon: 1990s (see original for image)\nAccording to Goodman & co (2016), the term “reproducible research” was coined in 1992. Computer scientist Jon Claerbout used it in the sense of methods reproducibility – enough published details for someone else to be able to reproduce the steps of a study.\nFormal standardized guidelines for reporting research – one of the key strategies for trying to improve research reproducibility – arrived in clinical research in the 1990s. The CONSORT statement for clinical trials in 1996 is the milestone here.\n\n\n\n\nCartoon: 2000s (see original for image)\nThe registration of clinical trials at inception took a giant leap forward in 2004 when the International Committee of Medical Journal Editors (ICMJE) announced they would not publish a trial that had not been registered.\n“Why most research findings are false” landed like a bomb on science’s landscape in 2005. John Ioannidis’ paper has now been viewed more than 1.8 million times, and definitely counts as a milestone in awareness of science’s reproducibility problems.\nIn 2007, FDAA, the FDA amendments act, added public results reporting to an earlier requirement that drug clinical trials be registered at inception. Final Rules and NIH policies that give those requirements teeth go into effect in January 2017.\n\n\n\n\nCartoon: 2010s (see original for image)\nPre-clinical research got a major jolt in 2012, when Begley and Ellis at the biotech company Amgen set out to confirm cancer research findings:\n\nFifty-three papers were deemed ‘landmark’ studies…[S]cientific findings were confirmed in only 6 (11%) cases. Even knowing the limitations of preclinical research, this was a shocking result.\n\nIn 2013, registering studies at inception took a leap at the journal Cortex with registered reports. Registered reports are peer reviewed detailed protocols, with the journal’s commitment to publishing the results after the study is completed.\nIn 2014, the NIH began reporting on its strategies for addressing reproducibility, including reporting guidelines for preclinical research, and PubMed Commons, which had arrived in 2013. That’s the commenting system in PubMed, the largest biomedical literature database.\nPsychology’s big jolt came in 2015, when the Open Science Collaboration reported that they replicated between a third and a half of 100 experiments and correlation studies.\nAnd in 2016, the American Statistical Association (ASA) issued its first-ever guidance, trying to stem the tide of misuse and misinterpretation of p values.\n2016 is ending, though, with a potential roll-back in the clinical research rigor and transparency required for FDA approval. Lesser levels of evidence than adequately controlled clinical trials might be back, and full raw data might not be necessary, either.\n\nWe have a “reproducibility crisis” because we need to do more to try to prevent bias in research design, conduct, interpretation, and reporting. And after that we need others to interrogate what’s found and replicate it in different contexts. As Christie Aschwanden points out, that’s just plain hard to do. But “science” isn’t really scientific without all of that, is it?\n\nDisclosure: PubMed Commons is part of my day job.\nIf you’re interested in the history of improving the reliability of clinical research, the James Lind Library is a goldmine. I drew on it heavily to develop this post.\nUpdates on 6 December 2016: Added materials reproducibility – thanks to Joanne Kamens for pointing out this gap in the definition I used on Twitter.\nAndrew Gelman has a reproducibility timeline for psychology (and it has very little overlap with mine).\nThe cartoons and illustrations are my own (CC-NC-ND-SA 4.0 license). (More cartoons at Statistically Funny and on Tumblr.)\nThe thoughts Hilda Bastian expresses here at Absolutely Maybe are personal, and do not necessarily reflect the views of the National Institutes of Health or the U.S. Department of Health and Human Services.\n\nFor full details, references, and images, see the original article."
  },
  {
    "objectID": "docs/bastian2016.html#s",
    "href": "docs/bastian2016.html#s",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Cartoon: 1950s (see original for image)\nEstes Kefauver was a U.S. Senator and Adlai Stevenson’s presidential running mate. He had become famous at the start of the decade with hearings into organized crime. More than 30 million people watched some of those hearings, on television or free in movie theaters.\nHe turned his attention to the pharmaceutical industry at the end of the decade, holding hearings into drug prices in 1958 and 1959. One of the issues that emerged was the “sorry state of science supporting drug effectiveness”. This outcry about research reproducibility led to a major change in research requirements for FDA approval in 1962.\nPart of the problem’s solution had a major milestone in 1959, too. Austin Bradford Hill led a meeting in Vienna that codified the methodology for controlled clinical trials.\nEarlier in the 1950s, though, there was a development that stands as a milestone in fueling science’s reproducibility crisis by inadvertently introducing a perverse incentive: Eugene Garfield proposed the journal impact factor in 1955."
  },
  {
    "objectID": "docs/bastian2016.html#s-1",
    "href": "docs/bastian2016.html#s-1",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Cartoon: 1960s (see original for image)\nIn 1962, the Kefauver-Harris amendments required “adequate and well-controlled clinical investigations” for FDA approval: adequate was at least 2 studies – a major step in expecting replication of results.\nA new problem in biological research was revealed in 1966 at a conference in Bethesda. Stanley Gartler was the first to report contamination in cell lines in cancer research (reported in Nature, 1968). Still not adequately dealt with, that’s grown into a juggernaut of un-reproducibility, making thousands of studies unreliable.\nThat year also saw what may be the first example of the science of systematically studying science’s methods – what John Ioannidis and colleagues have dubbed meta-research. It was a study of statistics and methods of evaluation in medical journal publications."
  },
  {
    "objectID": "docs/bastian2016.html#s-2",
    "href": "docs/bastian2016.html#s-2",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Cartoon: 1970s (see original for image)\nThe 70s brought us 2 key methods: systematically reviewing evidence (1971) and meta-analysis: a way to analyze the results of several studies at once (1976). Those methods didn’t just help make sense of bodies of evidence. They also propelled meta-research.\nCartoon: Meta-analysis (see original for image)\nAnd 1977 surfaced a problem in scientists’ behavior that allows unreliable scientific findings to take root and thrive. Michael Mahoney showed confirmation bias thrived in peer review:\n\nConfirmatory bias is the tendency to emphasize and believe experiences which support one’s views and to ignore or discredit those which do not…[R]eviewers were strongly biased against manuscripts which reported results contrary to their theoretical perspective."
  },
  {
    "objectID": "docs/bastian2016.html#s-3",
    "href": "docs/bastian2016.html#s-3",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Cartoon: 1980s (see original for image)\nAnalyzing the reliability of studies took a formal step forward in 1981, with the publication of a method for assessing the quality of clinical trials.\nDoug Altman and colleagues took on the issue of problems with the use and interpretation of statistics, with 1983 statistical guidelines for medical journals.\nAnd a way forward was proposed for the problem of unpublished research results. The unpublished research results are often the disappointing ones. That leaves us with a deceptive public record of science.\nJohn Simes called for an international registry of clinical trials in 1986. Registering the details of all research before it is actually done would mean we could at least identify gaps in the published research record down the line."
  },
  {
    "objectID": "docs/bastian2016.html#s-4",
    "href": "docs/bastian2016.html#s-4",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Cartoon: 1990s (see original for image)\nAccording to Goodman & co (2016), the term “reproducible research” was coined in 1992. Computer scientist Jon Claerbout used it in the sense of methods reproducibility – enough published details for someone else to be able to reproduce the steps of a study.\nFormal standardized guidelines for reporting research – one of the key strategies for trying to improve research reproducibility – arrived in clinical research in the 1990s. The CONSORT statement for clinical trials in 1996 is the milestone here."
  },
  {
    "objectID": "docs/bastian2016.html#s-5",
    "href": "docs/bastian2016.html#s-5",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Cartoon: 2000s (see original for image)\nThe registration of clinical trials at inception took a giant leap forward in 2004 when the International Committee of Medical Journal Editors (ICMJE) announced they would not publish a trial that had not been registered.\n“Why most research findings are false” landed like a bomb on science’s landscape in 2005. John Ioannidis’ paper has now been viewed more than 1.8 million times, and definitely counts as a milestone in awareness of science’s reproducibility problems.\nIn 2007, FDAA, the FDA amendments act, added public results reporting to an earlier requirement that drug clinical trials be registered at inception. Final Rules and NIH policies that give those requirements teeth go into effect in January 2017."
  },
  {
    "objectID": "docs/bastian2016.html#s-6",
    "href": "docs/bastian2016.html#s-6",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Cartoon: 2010s (see original for image)\nPre-clinical research got a major jolt in 2012, when Begley and Ellis at the biotech company Amgen set out to confirm cancer research findings:\n\nFifty-three papers were deemed ‘landmark’ studies…[S]cientific findings were confirmed in only 6 (11%) cases. Even knowing the limitations of preclinical research, this was a shocking result.\n\nIn 2013, registering studies at inception took a leap at the journal Cortex with registered reports. Registered reports are peer reviewed detailed protocols, with the journal’s commitment to publishing the results after the study is completed.\nIn 2014, the NIH began reporting on its strategies for addressing reproducibility, including reporting guidelines for preclinical research, and PubMed Commons, which had arrived in 2013. That’s the commenting system in PubMed, the largest biomedical literature database.\nPsychology’s big jolt came in 2015, when the Open Science Collaboration reported that they replicated between a third and a half of 100 experiments and correlation studies.\nAnd in 2016, the American Statistical Association (ASA) issued its first-ever guidance, trying to stem the tide of misuse and misinterpretation of p values.\n2016 is ending, though, with a potential roll-back in the clinical research rigor and transparency required for FDA approval. Lesser levels of evidence than adequately controlled clinical trials might be back, and full raw data might not be necessary, either.\n\nWe have a “reproducibility crisis” because we need to do more to try to prevent bias in research design, conduct, interpretation, and reporting. And after that we need others to interrogate what’s found and replicate it in different contexts. As Christie Aschwanden points out, that’s just plain hard to do. But “science” isn’t really scientific without all of that, is it?\n\nDisclosure: PubMed Commons is part of my day job.\nIf you’re interested in the history of improving the reliability of clinical research, the James Lind Library is a goldmine. I drew on it heavily to develop this post.\nUpdates on 6 December 2016: Added materials reproducibility – thanks to Joanne Kamens for pointing out this gap in the definition I used on Twitter.\nAndrew Gelman has a reproducibility timeline for psychology (and it has very little overlap with mine).\nThe cartoons and illustrations are my own (CC-NC-ND-SA 4.0 license). (More cartoons at Statistically Funny and on Tumblr.)\nThe thoughts Hilda Bastian expresses here at Absolutely Maybe are personal, and do not necessarily reflect the views of the National Institutes of Health or the U.S. Department of Health and Human Services.\n\nFor full details, references, and images, see the original article."
  },
  {
    "objectID": "docs/olah2017research.html",
    "href": "docs/olah2017research.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Distill\nAbout Prize Submit"
  },
  {
    "objectID": "docs/olah2017research.html#the-debt",
    "href": "docs/olah2017research.html#the-debt",
    "title": "Workshop on Reproducible Research",
    "section": "The Debt",
    "text": "The Debt\nProgrammers talk about technical debt: there are ways to write software that are faster in the short run but problematic in the long run. Managers talk about institutional debt: institutions can grow quickly at the cost of bad practices creeping in. Both are easy to accumulate but hard to get rid of.\nResearch can also have debt. It comes in several forms:\n\nPoor Exposition – Often, there is no good explanation of important ideas and one has to struggle to understand them. This problem is so pervasive that we take it for granted and don’t appreciate how much better things could be.\nUndigested Ideas – Most ideas start off rough and hard to understand. They become radically easier as we polish them, developing the right analogies, language, and ways of thinking.\nBad abstractions and notation – Abstractions and notation are the user interface of research, shaping how we think and communicate. Unfortunately, we often get stuck with the first formalisms to develop even when they’re bad. For example, an object with extra electrons is negative, and pi is wrong.\nNoise – Being a researcher is like standing in the middle of a construction site. Countless papers scream for your attention and there’s no easy way to filter or summarize them.Because most work is explained poorly, it takes a lot of energy to understand each piece of work. For many papers, one wants a simple one sentence explanation of it, but needs to fight with it to get that sentence. Because the simplest way to get the attention of interested parties is to get everyone’s attention, we get flooded with work. Because we incentivize people being “prolific,” we get flooded with a lot of work… We think noise is the main way experts experience research debt.\n\nThe insidious thing about research debt is that it’s normal. Everyone takes it for granted, and doesn’t realize that things could be different. For example, it’s normal to give very mediocre explanations of research, and people perceive that to be the ceiling of explanation quality. On the rare occasions that truly excellent explanations come along, people see them as one-off miracles rather than a sign that we could systematically be doing better."
  },
  {
    "objectID": "docs/olah2017research.html#interpretive-labor",
    "href": "docs/olah2017research.html#interpretive-labor",
    "title": "Workshop on Reproducible Research",
    "section": "Interpretive Labor",
    "text": "Interpretive Labor\nThere’s a tradeoff between the energy put into explaining an idea, and the energy needed to understand it. On one extreme, the explainer can painstakingly craft a beautiful explanation, leading their audience to understanding without even realizing it could have been difficult. On the other extreme, the explainer can do the absolute minimum and abandon their audience to struggle. This energy is called interpretive labor .\nMany explanations are not one-to-one. People give lectures, write books, or communicate online. In these one-to-many cases, each member of the audience pays the cost of understanding, even though the cost of explaining stays the same.More formally, if N people are trying to understand each other, it takes each one O(1) effort to write an explanation of their ideas but O(N) effort to understand each of the other N-1 people’s ideas. The result is that energy cost looks like O(a + bN) where a and b are coefficients for the trade off between energy on the explanation side and energy on the understanding side. That is a is the energy spent on explaining and b is the corresponding effort needed to understand. This is similar to ideas in The Mythical Man-Month As a result, the cost of understanding has a multiplier in the interpretive labor tradeoff — sometimes a huge multiplier.For example, Christopher’s average blog post is read by over 100,000 people; if he can save each reader just one second, he’s saved humanity 30 hours.\n\nIn research, we often have a group of researchers all trying to understand each other. Just like before, the cost of explaining stays constant as the group grows, but the cost of understanding increases with each new member. At some size, the effort to understand everyone else becomes too much. As a defense mechanism, people specialize, focusing on a narrower area of interest. The maintainable size of the field is controlled by how its members trade off the energy between communicating and understanding.\nResearch debt is the accumulation of missing interpretive labor. It’s extremely natural for young ideas to go through a stage of debt, like early prototypes in engineering. The problem is that we often stop at that point. Young ideas aren’t ending points for us to put in a paper and abandon. When we let things stop there the debt piles up. It becomes harder to understand and build on each other’s work and the field fragments."
  },
  {
    "objectID": "docs/olah2017research.html#clear-thinking",
    "href": "docs/olah2017research.html#clear-thinking",
    "title": "Workshop on Reproducible Research",
    "section": "Clear Thinking",
    "text": "Clear Thinking\nIt’s worth being clear that research debt isn’t just about ideas not being explained well. It’s a lack of digesting ideas – or, at least, a lack of the public version of ideas being digested.Often, some individuals have a much more developed version of an idea than is publicly shared. There are a lot of reasons for not sharing it (in particular, they’re often not traditionally publishable). It’s a communal messiness of thought.\nDeveloping good abstractions, notations, visualizations, and so forth, is improving the user interfaces for ideas. This helps both with understanding ideas for the first time and with thinking clearly about them. Conversely, if we can’t explain an idea well, that’s often a sign that we don’t understand it as well as we could.\nIt shouldn’t be that surprising that these two largely go hand in hand. Part of thinking is having a conversation with ourselves."
  },
  {
    "objectID": "docs/olah2017research.html#research-distillation",
    "href": "docs/olah2017research.html#research-distillation",
    "title": "Workshop on Reproducible Research",
    "section": "Research Distillation",
    "text": "Research Distillation\nResearch distillation is the opposite of research debt. It can be incredibly satisfying, combining deep scientific understanding, empathy, and design to do justice to our research and lay bare beautiful insights.\nDistillation is also hard. It’s tempting to think of explaining an idea as just putting a layer of polish on it, but good explanations often involve transforming the idea. This kind of refinement of an idea can take just as much effort and deep understanding as the initial discovery.\nThis leaves us with no easy way out. We can’t solve research debt by having one person write a textbook: their energy is spread too thin to polish every idea from scratch. We can’t outsource distillation to less skilled non-experts: refining and explaining ideas requires creativity and deep understanding, just as much as novel research.\nResearch distillation doesn’t have to be you, but it does have to be us."
  },
  {
    "objectID": "docs/olah2017research.html#where-are-the-distillers",
    "href": "docs/olah2017research.html#where-are-the-distillers",
    "title": "Workshop on Reproducible Research",
    "section": "Where are the Distillers?",
    "text": "Where are the Distillers?\nLike the theoretician, the experimentalist or the research engineer, the research distiller is an integral role for a healthy research community. Right now, almost no one is filling it.\nWhy do researchers not work on distillation? One possibility is perverse incentives, like wanting your work to look difficult. Those certainly exist, but we don’t think they’re the main factor. There are a lot of perverse incentives that push against explaining things well, sharing data, and so forth. This is especially true when the work you are doing isn’t that interesting or isn’t reproducible and you want to obscure that. Or if you have a lot of competitors and don’t want them to catch up. However, our experience is that most good researchers don’t seem that motivated by these kind of factors. Instead, the main issue is that it isn’t worthwhile for them to divert energy from pursuing results to distill things. Perhaps things are different in other fields, or I’m not cynical enough. Another possibility is that they don’t enjoy research distillation. Again, we don’t think that’s what’s going on.\nLots of people want to work on research distillation. Unfortunately, it’s very difficult to do so, because we don’t support them.There is a strange kind of informal support for people working on research distillation. Christopher has personally benefitted a great deal from this. But it’s unreliable and not widely advertised, which makes it hard to build a career on.\nAn aspiring research distiller lacks many things that are easy to take for granted: a career path, places to learn, examples and role models. Underlying this is a deeper issue: their work isn’t seen as a real research contribution. We need to fix this."
  },
  {
    "objectID": "docs/olah2017research.html#an-ecosystem-for-distillation",
    "href": "docs/olah2017research.html#an-ecosystem-for-distillation",
    "title": "Workshop on Reproducible Research",
    "section": "An Ecosystem for Distillation",
    "text": "An Ecosystem for Distillation\nIf you are excited to distill ideas, seek clarity, and build beautiful explanations, we are letting you down. You have something precious to contribute and we aren’t supporting you the way we should.\nThe Distill Ecosystem is an attempt to better support this kind of work. Right now, it has three parts:\n\nThe Distill Journal – A venue to give traditional validation to non-traditional contributions.\nThe Distill Prize – A $10,000 prize to acknowledge outstanding explanations of machine learning.\nDistill Infrastructure – Tools for making beautiful interactive essays.\n\nThis is just a start: there’s a lot more that needs to be done. A complete ecosystem for this kind of work needs several other components, including places where one can learn these skills and reliable sources of employment doing this kind of work. We’re optimistic that will come with time."
  },
  {
    "objectID": "docs/olah2017research.html#further-reading",
    "href": "docs/olah2017research.html#further-reading",
    "title": "Workshop on Reproducible Research",
    "section": "Further Reading",
    "text": "Further Reading\n\n\nVisual Mathematics: Several mathematicians have made remarkable efforts to visually explain certain topics. Needham’s tour-de-force Visual Complex Analysis is particularly striking, but there are several lovely examples of new clarity being brought to a topic by visually reformulating it and, on a smaller scale, a plethora of visual proofs.\nExplorable Explanations: There’s a loose community exploring how the interactive medium enabled by computers can be used to communicate and think in ways previously impossible. These ideas start, as many ideas in computing do, with work done by Douglas Engelbart and Alan Kay.\nMore recently, Explorable Explanations have started to reimagine what an essay can be in this new medium. This started with Bret Victor’s foundational essay and has been further developed by amazing examples (eg. ).\nThere are also explorations of how we can augment our ability to think in this new medium, bringing previously inaccessible ideas within reach. Once again, Bret Victor has wonderful ideas , as does Michael Nielsen .\nResearch Distribution: Over last few decades, there’s been a big push for research to be freely available online. This includes the formation of arXiv.org and PLOS, and journal editorial boards resigning to start open-access journals.\nIncreasingly, the challenge is filtering accessible content. Karpathy’s ArXiv Sanity is a lovely tool for this. Crowd curation by online communities also helps a great deal.\nOpen-Notebook Science: Open notebook science, like Dror Bar-Natan’s Academic Pensieve, and massively-collaborative research projects like the Polymath Project, separate the sharing of results from formal publishing.\nThis seems really important. Traditionally, if one doesn’t turn research into a paper, it’s basically as though you didn’t do it. This creates a strong incentive for all research to be dressed up as an important paper, increasing noise.\nDiscussion of Debt and Distillation: A number of mathematicians have discussed what we call research distillation. Some great comments and references are collected in this MathOverflow thread — I’d draw particular attention to Thurston’s account of accidentally killing a field by drowning it in research debt in section 6 of .\nOther nice thoughts in this space include the idea of an “open exposition problem” , Grothendieck’s “rising sea” approach to mathematics , and recent calls to value conceptual progress in CS more .\n\n\nAcknowledgments\nWe’re extremely grateful for the advice and assistance of Jennifer Daniel in illustrating this article.\nThis essay has greatly benefitted from the comments of many people, including: Dandelion Mané, Emma Pierson, Michael Nielsen, Cassandra Xia, Geoffrey Irving, Elizabeth Van Nostrand, Maithra Raghu, Greg Brockman, Hannah Davis, Devon Zuegel, Wojciech Zaremba, Vikas Sindhwani, Pierre Sermanet, Mike Schuster, George Dahl, Jascha Sohl-dickstein, Adam Roberts, Greg Corrado, Samy Bengio, Yomna Nasser, Katherine Ye, Dave Rushton-Smith, Martin Wattenberg, Fernanda Viegas, Eric Breck, Aaron Courville.\n\n\nAuthor Contributions\nThis essay was primarily written by Chris Olah and illustrated by Shan Carter.\n\n\nReferences\n\nThe Tau Manifesto  [link] Hartl, M., 2013.\nInterpretive Labor  [link] Van Nostrand, E., 2015.\nThe utopia of rules: On technology, stupidity, and the secret joys of bureaucracy Graeber, D., 2015. Melville House.\nThe Mythical Man-Month Brooks, F., 1986. Tutorial, Vol 11, pp. 35–42.\nVisual Complex Analysis Needham, T., 1998. Oxford University Press.\nVisual Group Theory Carter, N., 2009. MAA.\nA visual explanation of Jensen’s inequality Needham, T., 1993. The American mathematical monthly, Vol 100(8), pp. 768–771. JSTOR. DOI: 10.2307/2324783\nVisual Differential Geometry Needham, T., Work In Progress.\nA Research Center for Augmenting Human Intellect  [link] Engelbart, D.C. and English, W.K., 1968. Proceedings of the December 9-11, 1968, Fall Joint Computer Conference, Part I, pp. 395–410. ACM. DOI: 10.1145/1476589.1476645\nPersonal Dynamic Media  [link] Kay, A. and Goldberg, A., 1977. Computer, Vol 10(3), pp. 31–41. IEEE Computer Society Press. DOI: 10.1109/C-M.1977.217672\nExplorable explanations  [link] Victor, B., 2011.\nVisualizing Algorithms  [link] Bostock, M., 2014.\nHow to Fold a Julia Fractal  [link] Wittens, S., 2013.\nBack to the Future of Handwriting Recognition  [link] Schaedler, J., 2016.\nMedia for thinking the unthinkable  [link] Victor, B., 2013.\nLearnable programming  [link] Victor, B., 2012.\nThought as a Technology  [HTML] Nielsen, M., 2016.\nToward an Exploratory Medium for Mathematics  [HTML] Nielsen, M., 2016.\nOn Proof and Progress in Mathematics  [PDF] Thurston, W.P., 1998. New directions in the philosophy of mathematics, pp. 337–55.\nA beginner’s guide to forcing  [PDF] Chow, T.Y., 2009. Communicating mathematics, Vol 479, pp. 25–40. American Mathematical Soc.\nRecoltes et Semailles Grothendieck, A., 1985. Reflexions et temoignage sur un passe de mathematicien, pp. 621.\nStatement on conceptual contributions in theory  [link] Aaronson, S., Borodin, A., Chazelle, B., Goldreich, O., Goldwasser, S., Karp, R., Kearns, M., Papadimitriou, C., Sudan, M. and Vadhan, S., 2008. Shtetl-Optimized.\n\n\n\nUpdates and Corrections\nView all changes to this article since it was first published. If you see mistakes or want to suggest changes, please create an issue on GitHub.\n\n\nReuse\nDiagrams and text are licensed under Creative Commons Attribution CC-BY 4.0 with the source available on GitHub, unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by a note in their caption: “Figure from …”.\n\n\nCitation\nFor attribution in academic contexts, please cite this work as\nOlah & Carter, \"Research Debt\", Distill, 2017.\nBibTeX citation\n@article{olah2017research,\n  author = {Olah, Chris and Carter, Shan},\n  title = {Research Debt},\n  journal = {Distill},\n  year = {2017},\n  note = {https://distill.pub/2017/research-debt},\n  doi = {10.23915/distill.00005}\n}\nDistill is dedicated to clear explanations of machine learning\nAbout Submit Prize Archive RSS GitHub Twitter      ISSN 2476-0757"
  },
  {
    "objectID": "docs/whitfield2021.html",
    "href": "docs/whitfield2021.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Replication Crisis\nAuthor: John Whitfield\nSource: London Review of Books, Vol. 43 No. 19, 7 October 2021\nOriginal Article\nIn November 2017 the China Daily ran a story about a Beijing barbecue restaurant called Liuyedao (‘The Lancet’), which was offering a discount to any customer who could show they had recently published a paper in a scientific journal. The restaurant took the journal’s ‘impact factor’ – a statistic based on the average number of citations received by papers carried in the journal in the two years after publication – and converted it into a cash equivalent, to be deducted from the bill.\nThe impact factor was invented in the 1960s as a way of helping academic librarians decide which journals to hold in their collections. It is a reasonable measure of the strength of a journal. But the distribution of citations is skewed: a small fraction of papers account for most of them, while most papers get few if any. So the impact factor isn’t much use as a guide to the quality of an individual paper: no one should judge a scientific article by the journal in which it appears. Most people working in research know this. Yet because scientific papers are difficult for non-specialists to understand, and because it is so hard to keep on top of the literature, the temptation to assess scientific work by means of a single, simple measurement, even a bogus one, is hard to resist. As a result, the impact factor has become a marker of prestige. Even before anyone has read it, a paper published in the Lancet, which currently has an impact factor of 60, is worth far more than a paper in a specialist outlet such as Clinical Genetics, with its impact factor of 4.4. Unsurprisingly, this has an influence on where researchers submit their work.\nWhether and where a researcher gets hired depends to a large extent on their publication record: how many papers they put their name to, which journals they are published in, how many times their papers are cited in other papers. This has created a system that favours speed of publication, volume of output and – because journals prefer new, eye-catching findings over negative results or replications of previous work – sensationalism. There can be financial incentives too. At the time the barbecue-discount story appeared, many Chinese universities were giving cash bonuses for publications, with higher-impact journals securing bigger rewards for researchers. In a survey of Chinese university policy in 2016, the average bonus for the lead author of a paper in Nature or Science was calculated at $44,000, five times the average professorial salary.\nThe chief form of pre-publication quality control in science is peer review. Journal editors send submissions to experts, usually two of them. Their job is to judge whether a study’s methods, data and analyses are sound, and whether the evidence backs up the authors’ claims. Their (most often anonymous) reports assess the work’s validity and importance, suggest how it might be improved, and recommend rejection or acceptance, usually with required revisions. Reviewers are increasingly difficult to find, since they are generally not paid or otherwise credited, and must fit the work around their own teaching, research and administrative responsibilities. It is next to impossible, even in the several hours it takes to put together a typical review, to check all of a paper’s methods and analyses. The same goes for detecting simple errors, such as a wrong number typed into a spreadsheet or a mix-up in cell cultures, let alone fabricated, fraudulent data. As a result, reviewers and journals end up taking a lot on trust. Even diligent reviewing is inconsistent, since reviewers may disagree about a paper’s merits, and will have their own intellectual and social biases.\nThe most comprehensive attempt to estimate the extent of the replication crisis was made in a 2015 study called ‘Estimating the Reproducibility of Psychological Science’. The researchers behind it tried to replicate 100 studies published in three psychology journals. They found that only 39 per cent of the replications were successful. The study caused a stir, not least because the journals involved had a high impact factor. The researchers concluded that ‘the scientific community should be concerned about the current state of reproducibility in psychology’. Since then, similar studies have been carried out in other fields, with varying results. A 2016 analysis of 21 studies in cancer biology found that only six could be successfully replicated. A 2018 survey of 1,500 researchers across various disciplines found that 70 per cent had tried and failed to reproduce another scientist’s work.\nIn response to the crisis, some journals have introduced stricter editorial policies, increased transparency requirements, and more rigorous peer review processes. Funding bodies and governments are also taking action. In 2018, the US National Institutes of Health announced a new policy requiring researchers to submit a plan for how they will make their data and findings available to the public. The European Commission has launched a pilot project to test the feasibility of a European research data space, which would make it easier for researchers to share data across borders and disciplines.\nThere is widespread agreement that something has to be done about these problems. Funding bodies and governments are showing an increasing willingness to act as regulators. In the UK, the House of Commons Science and Technology Committee has launched an inquiry into research integrity, and in the US, the National Academies of Sciences, Engineering, and Medicine have convened a committee to develop recommendations for improving the reproducibility of research. The stakes are high. If the problems are not addressed, public trust in science will erode, and the ability of researchers to secure funding, publish their work, and have it taken seriously will be undermined.\nAll this is welcome, but one issue that those in authority have shown little interest in tackling is how to reduce the reliance of the system on short-term contracts, which far outnumber the permanent jobs available in publicly funded research. Is it even possible to increase job security for researchers at a time when teaching in universities is becoming ever more precarious? A similar question might be asked about universities’ ability to resist the forces that drive competition in research. Can any particular institution or nation afford to opt out unilaterally? What happens if the kinder, gentler universities start to slide in the international rankings, which are based partly on measures of publications and citations?\nSuch reform as there has been is most evident in academic publishing. Historically, as scientists see it, the largest for-profit publishing companies have taken their free labour as authors and reviewers and made them pay through the nose to read the results. In 2019 Elsevier, the largest of these companies, had a profit margin of 37 per cent. The open access movement, begun by activist scientists around the turn of the century, makes the moral case that readers shouldn’t have to pay to see the results of science that has already been paid for with public money, and the practical case that freely accessible papers would be easier to validate and build on. The movement has made a lot of progress, at least in Europe, where the EU and many other funders, including UKRI and the Wellcome, have now stopped the paywalling of publications resulting from their grants.\nPrint journals have always been selective because they have a limited number of pages. Traditionally, they have sought to judge not just whether a given paper is sound, but whether its findings are important. Many still do; it’s part of their cachet. But as most journals move entirely online, space is no longer an issue. In the twenty years since the open access movement began, a new business model has emerged in which subscriptions are replaced by publication fees charged to authors. We have seen the rise of open access megajournals, which ask reviewers to judge the validity of results but not their significance, and accept all submissions, including replications and negative findings. This approach is not watertight; in 2016, the Public Library of Science’s megajournal PLOS One, which currently charges a publication fee of $1695 per paper, carried a study reporting that the human hand showed ‘the proper design by the Creator’. But the model has proved popular both with researchers, who get a relatively quick, painless and cheap route to publication, and with publishers, who get a cash cow.\nPLOS One has an impact factor of just 2.7. ‘Ideally,’ Ritchie writes, ‘what we want to see is an accurate proportion of null results, and more attempted replications, in the glamorous, high-impact journals.’ But that’s to take for granted that the ‘glamorous journals’ should retain their status. The argument can be made that the scholarly publishing industry is beyond fixing, and we would be better off without journals – perhaps even without pre-publication peer review. Publishers are no longer needed for typesetting or distribution, and social media – including specialist sites such as ResearchGate, which has more than 17 million users – can perform some of their curatorial and marketing functions. What value the journals retain lies in their brands, and in their capacity to organise peer review. But if peer review isn’t working, what is the brand worth? Why not just let researchers make their work public, and let it thrive, or wither?\nThis approach is feasible thanks to preprint servers. These are online repositories on which papers can be posted without peer review, though most of them do carry out vetting for plagiarism, health and security risks, and general appropriateness. Publishing studies as preprints has long been the norm in many areas of physics and maths, where pretty much everything in journals will already have appeared – for free, in an unreviewed but often very similar form – on a site called arXiv (‘archive’). Biologists were initially slower to publish preprints, partly as a result of the worry that showing their hand would make it easier for others to beat them to a reviewed journal paper. This reluctance had already begun to fade before 2020, but the Covid-19 pandemic has transformed attitudes. The imperative to share findings as quickly and widely as possible, so that others can test and make use of them, is inarguable.\nThis is science working in the way many would want it to: rapid, open, collaborative, and focused on the benefit to the public. There is a downside: preprint servers and journals have been swamped by shoddy papers, sent out in a hurry to catch the Covid-19 bandwagon. If scientists are the only ones reading the work, this isn’t much of a problem, but over the course of the last year, journalists, patients, cranks and anyone else with an interest have also been monitoring the servers, and they don’t always distinguish between a preprint and a reviewed paper. As a result, some papers that would have been unlikely to make it into a journal have received bursts of publicity. In January last year, for example, a paper appeared on the bioRxiv server, run out of the august Cold Spring Harbor Laboratory on Long Island, claiming that there was an ‘uncanny similarity’ between Sars-Cov-2 and HIV, which was ‘unlikely to be fortuitous’. This fuelled wild talk that the coronavirus was an engineered bioweapon. The article now bears a red ‘withdrawn’ label, but it is still easy to find and free to read. (To preserve the integrity of the literature, retracted papers usually aren’t deleted.) The following month, the Fox News presenter Tucker Carlson cited a preprint posted on ResearchGate claiming that the Wuhan wet market suspected of being the origin of the outbreak was close to a coronavirus research lab. The author later took it down, telling the Wall Street Journal that it ‘was not supported by direct proofs’; Carlson said the paper had been ‘covered up’.\nThis problem isn’t confined to preprints: plenty of journals have retracted Covid-19 studies that had passed peer review and been published. And science has always been open to misappropriation; making it less accessible, if that were possible, wouldn’t end that. But papers, whether journal articles or preprints, are still the most important interface between scientific research and wider society. The increased potential for the rapid dissemination of bad-faith interpretations of scientific publications gives researchers one more good reason to take pains over their work."
  },
  {
    "objectID": "docs/hohman2020communicating.html",
    "href": "docs/hohman2020communicating.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Distill\nAbout Prize Submit"
  },
  {
    "objectID": "docs/hohman2020communicating.html#interactive-articles-theory-practice",
    "href": "docs/hohman2020communicating.html#interactive-articles-theory-practice",
    "title": "Workshop on Reproducible Research",
    "section": "Interactive Articles: Theory & Practice",
    "text": "Interactive Articles: Theory & Practice\nInteractive articles draw from and connect many types of media, from static text and images to movies and animations. But in contrast to these existing forms, they also leverage interaction techniques such as details-on demand, belief elicitation, play, and models and simulations to enhance communication.\nWhile the space of possible designs is far too broad to be solved with one-size-fits-all guidelines, by connecting the techniques used in these articles back to underlying theories presented across disparate fields of research we provide a missing foundation for designers to use when considering the broad space of interactions that could be added to a born-digital article.\nWe draw from a corpus of over sixty interactive articles to highlight the breadth of techniques available and analyze how their authors took advantage of a digital medium to improve the reading experience along one or more dimensions, for example, by reducing the overall cognitive load, instilling positive affect, or improving information recall.\nBecause diverse communities create interactive content, this medium goes by many different names and has not yet settled on a standardized format nor definition.However, one is taking shape. Researchers have proposed artifacts such as explorable multiverse analyses , explainables , and exploranations to more effectively disseminate their work, communicate their results to the public, and remove research debt . In newsrooms, data journalists, developers, and designers work together to make complex news and investigative reporting clear and engaging using interactive stories . Educators use interactive textbooks as an alternative learning format to give students hands-on experience with learning material .\nBesides these groups, others such as academics, game developers, web developers, and designers blend editorial, design, and programming skills to create and publish explorable explanations , interactive fiction , interactive non-fiction , active essays , and interactive games . While these all slightly differ in their technical approach and target audience, they all largely leverage the interactivity of the modern web.\nWe focus on five unique affordances of interactive articles, listed below. In-line videos and example interactive graphics are presented alongside this discussion to demonstrate specific techniques.\n\nConnecting People and Data\nAs visual designers are well aware, and as journalism researchers have confirmed empirically , an audience which finds content to be aesthetically pleasing is more likely to have a positive attitude towards it. This in turn means people will spend more time engaging with content and ultimately lead to improved learning outcomes. While engagement itself may not be an end goal of most research communications, the ability to influence both audience attitude and the amount of time that is spent is a useful lever to improve learning: we know from education research that both time spent and emotion are predictive of learning outcomes.\nAnimations can also be used to improve engagement . While there is debate amongst researchers if animations in general are able to more effectively convey the same information compared to a well designed static graphic , animation has been shown to be effective specifically for communicating state transitions , uncertainty , causality , and constructing narratives . A classic example of this is Muybridge’s motion study that can be seen in 3: while the series of still images may be more effective for answering specific questions like, “Does a horse lift all four of its feet off the ground when it runs?” watching the animation in slow motion gives the viewer a much more visceral sense of how it runs. A more modern example can be found in OpenAI’s reporting on their hide-and-seek agents . The animations here instantly give the viewer a sense of how the agents are operating in their environment.\nPassively, animation can be used to add drama to a graphic displaying important information, but which readers may otherwise find dry. Scientific data which is inherently time varying may be shown using an animation to connect viewers more closely with the original data, as compared to seeing an abstracted static view. For example, Ed Hawkins designed “Climate Spirals,” which shows the average global temperature change over time . This presentation of the data resonated with a large public audience, so much so that it was displayed at the opening ceremony at the 2016 Rio Olympics. In fact, many other climate change visualizations of this same dataset use animation to build suspense and highlight the recent spike in global temperatures .\nBy adding variation over time, authors have access to a new dimension to encode information and an even wider design space to work in. Consider the animated graphic in The New York Times story “Extensive Data Shows Punishing Reach of Racism for Black Boys,” which shows economic outcomes for 10,000 men who grew up in rich families . While there are many ways in which the same data could have been communicated more succinctly using a static visualization , by utilizing animation, it became possible for the authors to design a unit visualization in which each data point shown represented an individual, reminding readers that the data in this story was about real peoples’ lives.\nUnit visualizations have also been used to evoke empathy in readers in other works covering grim topics such as gun deaths and soldier deaths in war . Using person-shaped glyphs (as opposed to abstract symbols like circles or squares) has been shown not to produce additional empathic responses , but including actual photographs of people helps readers gain interest in, remember , and communicate complex phenomena using visualizations. Correll argues that much of the power of visualization comes from abstraction, but quantization stymies empathy . He instead suggests anthropomorphizing data, borrowing journalistic and rhetoric techniques to create novel designs or interventions to foster empathy in readers when viewing visualizations .\nRegarding the format of interactive articles, an ongoing debate within the data journalism community has been whether articles which utilize scroll-based graphics (scrollytelling) are more effective than those which use step-based graphics (slideshows). McKenna et al. found that their study participants largely preferred content to be displayed with a step- or scroll-based navigation as opposed to traditional static articles, but did not find a significant difference in engagement between the two layouts. In related work, Zhi et al. found that performance on comprehension tasks was better in slideshow layouts than in vertical scroll-based layouts . Both studies focused on people using desktop (rather than mobile) devices. More work is needed to evaluate the effectiveness of various layouts on mobile devices, however the interviews conducted by MckEnna et al. suggest that additional features, such as supporting navigation through swipe gestures, may be necessary to facilitate the mobile reading experience.\nThe use of games to convey information has been explored in the domains of journalism and education . Designers of newsgames use them to help readers build empathy with their subject, for example in The Financial Times’s “Uber Game” , and explain complex systems consisting of multiple parts, for example in Wired’s “Cutthroat Capitalism: The Game” . In educational settings the use of games has been shown to motivate students while maintaining or improving learning outcomes .\nAs text moves away from author-guided narratives towards more reader-driven ones , the reading experience becomes closer to that of playing a game. For example, the critically acclaimed explorable explanation “Parable of the Polygons” puts play at the center of the story, letting a reader manually run an algorithm that is later simulated in the article to demonstrate how a population of people with slight personal biases against diversity leads to social segregation .\n\n\nMaking Systems Playful\nInteractive articles utilize an underlying computational infrastructure, allowing authors editorial control over the computational processes happening on a page. This access to computation allows interactive articles to engage readers in an experience they could not have with traditional media. For example, in “Drawing Dynamic Visualizations”, Victor demonstrates how an interactive visualization can allow readers to build an intuition about the behavior of a system, leading to a fundamentally different understanding of an underlying system compared to looking at a set of static equations . These articles leverage active learning and reading, combined with critical thinking to help diverse sets of people learn and explore using sandboxed models and simulations .\nComplex systems often require extensive setup to allow for proper study: conducting scientific experiments, training machine learning models, modeling social phenomenon, digesting advanced mathematics, and researching recent political events, all require the configuration of sophisticated software packages before a user can interact with a system at all, even just to tweak a single parameter. This barrier to entry can deter people from engaging with complex topics, or explicitly prevent people who do not have the necessary resources, for example, computer hardware for intense machine learning tasks. Interactive articles drastically lower these barriers.\nScience that utilizes physical and computational experiments requires systematically controlling and changing parameters to observe their effect on the modeled system. In research, dissemination is typically done through static documents, where various figures show and compare the effect of varying particular parameters. However, efforts have been made to leverage interactivity in academic publishing, summarized in . Reimagining the research paper with interactive graphics , as exploranations , or as explorable multiverse analyses , gives readers control over the reporting of the research findings and shows great promise in helping readers both digest new ideas and learn about existing fields that are built upon piles of research debt .\nBeyond reporting statistics, interactive articles are extremely powerful when the studied systems can be modeled or simulated in real-time with interactive parameters without setup, e.g., in-browser sandboxes. Consider the example in 4 of a Boids simulation that models how birds flock together. Complex systems such as these have many different parameters that change the resulting simulation. These sandbox simulations allow readers to play with parameters to see their effect without worrying about technical overhead or other experimental consequences.\nThis is a standout design pattern within interactive articles, and many examples exist ranging in complexity. “How You Will Die” visually simulates the average life expectancy of different groups of people, where a reader can choose the gender, race, and age of a person . “On Particle Physics” allows readers to experiment with accelerating different particles through electric and magnetic fields to build intuition behind electromagnetism foundations such as the Lorentz force and Maxwell’s equations — the experiments backing these simulations cannot be done without multi-million dollar machinery . “Should Prison Sentences Be Based On Crimes That Haven’t Been Committed Yet?” shows the outcome of calculating risk assessments for recidivism where readers adjust the thresholds for determining who gets parole .\nThe dissemination of modern machine learning techniques has been bolstered by interactive models and simulations. Three articles, “How to Use t-SNE Effectively” , “The Beginner’s Guide to Dimensionality Reduction” , and “Understanding UMAP” show the effect that hyperparameters and different dimensionality reduction techniques have on creating low dimensional embeddings of high-dimensional data. A popular approach is to demonstrate how machine learning models work with in-browser models , for example, letting readers use their own video camera as input to an image classification model or handwriting as input to a stroke prediction model . Other examples are aimed at technical readers who wish to learn about specific concepts within deep learning. Here, interfaces allow readers to choose model hyperparameters, datasets, and training procedures that, once selected, visualize the training process and model internals to inspect the effect of varying the model configuration .\nInteractive articles commonly communicate a single idea or concept using multiple representations. The same information represented in different forms can have different impact. For example, in mathematics often a single object has both an algebraic and a geometric representation. A clear example of this is the definition of a circle . Both are useful, inform one another, and lead to different ways of thinking. Examples of interactive articles that demonstrate this include various media publications’ political election coverage that break down the same outcome in multiple ways, for example, by voter demographics, geographical location, and historical perspective .\nThe Multimedia Principle states that people learn better from words and pictures rather than words or pictures alone , as people can process information through both a visual channel and auditory channel simultaneously. Popular video creators such as 3Blue1Brown and Primer exemplify these principles by using rich animation and simultaneous narration to break down complex topics. These videos additionally take advantage of the Redundancy Principle by including complementary information in the narration and in the graphics rather than repeating the same information in both channels .\nWhile these videos are praised for their approachability and rich exposition, they are not interactive. One radical extension from traditional video content is also incorporating user input into the video while narration plays. A series of these interactive videos on “Visualizing Quaternions” lets a reader listen to narration of a live animation on screen, but at any time the viewer can take control of the video and manipulate the animation and graphics while simultaneously listening to the narration .\nUtilizing multiple representations allows a reader to see different abstractions of a single idea. Once these are familiar and known, an author can build interfaces from multiple representations and let readers interact with them simultaneously, ultimately leading to interactive experiences that demonstrate the power of computational communication mediums. Next, we discuss such experiences where interactive articles have transformed communication and learning by making live models and simulations of complex systems and phenomena accessible.\n\n\nPrompting Self-Reflection\nAsking a student to reflect on material that they are studying and explain it back to themselves — a learning technique called self-explanation — is known to have a positive impact on learning outcomes . By generating explanations and refining them as new information is obtained, it is hypothesized that a student will be more engaged with the processes which they are studying . When writing for an interactive environment, components can be included which prompt readers to make a prediction or reflection about the material and cause them to engage in self-explanation .\nWhile these prompts may take the form of text entry or other standard input widgets, one of the most prominent examples of this technique used in practice comes from The New York Times “You Draw It” visualizations . In these visualizations, readers are prompted to complete a trendline on a chart, causing them to generate an explanation based on their current beliefs for why they think the trend may move in a certain direction. Only after readers make their prediction are they shown the actual data. Kim et al. showed that using visualizations as a prompt is an effective way to encourage readers to engage in self explanation and improve their recall of the information . 5 shows one these visualizations for CO₂ emissions from burning fossil fuels. After clicking and dragging to guess the trend, your guess will be compared against the actual data.\nIn the case of “You Draw It,” readers were also shown the predictions that others made, adding a social comparison element to the experience. This additional social information was not shown to necessarily be effective for improving recall . However, one might hypothesize that this social aspect may have other benefits such as improving reader engagement, due to the popularity of recent visual stories using this technique, for example in The Pudding’s “Gyllenhaal Experiment” and Quartz’s “How do you draw a circle?” .\nPrompting readers to remember previously presented material, for example through the use of quizzes, can be an effective way to improve their ability to recall it in the future . This result from cognitive psychology, known as the testing effect , can be utilized by authors writing for an interactive medium . While testing may call to mind stressful educational experiences for many, quizzes included in web articles can be low stakes: there is no need to record the results or grade readers. The effect is enhanced if feedback is given to the quiz-takers, for example by providing the correct answer after the user has recorded their response .\nThe benefits of the testing effect can be further enhanced if the testing is repeated over a period of time , assuming readers are willing to participate in the process. The idea of spaced repetition has been a popular foundation for memory building applications, for example in the Anki flash card system. More recently, authors have experimented with building spaced repetition directly into their web-based writing , giving motivated readers the opportunity to easily opt-in to a repeated testing program over the relevant material.\n\n\nPersonalizing Reading\nContent personalization — automatically modifying text and multimedia based on a reader’s individual features or input (e.g., demographics or location) — is a technique that has been shown to increase engagement and learning within readers and support behavioral change . The PersaLog system gives developers tools to build personalized content and presents guidelines for personalization based on user research from practicing journalists . Other work has shown that “personalized spatial analogies,” presenting distance measurements in regions where readers are geographically familiar with, help people more concretely understand new distance measurements within news stories .\nPersonalization alone has also been used as the standout feature of multiple interactive articles. Both “How Much Hotter Is Your Hometown Than When You Were Born?” and “Human Terrain” use a reader’s location to drive stories relating to climate change and population densities respectively. Other examples ask for explicit reader input, such as a story that visualizes a reader’s net worth to challenge a reader’s assumptions if they are wealthy or not (relative to the greater population) , or predicting a reader’s political party affiliation . Another example is the interactive scatterplot featured in “Find Out If Your Job Will Be Automated” . In this visualization, professions are plotted to determine their likelihood of being automated against their average annual wage. The article encourages readers to use the search bar to type in their own profession to highlight it against the others.\nAn interactive medium has the potential to offer readers an experience other than static, linear text. Non-linear stories, where a reader can choose their own path through the content, have the potential to provide a more personalized experience and focus on areas of user interest . For example, the BBC has used this technique in both online articles and in a recent episode of “Click” , a technology focused news television program. Non-linear stories present challenges for authors, as they must consider the myriad possible paths through the content, and consider the different possible experiences that the audience would have when pursuing different branches.\nAnother technique interactive articles often use is segmenting content into small pieces to be read in-between or alongside other graphics. While we have already discussed cognitive load theory, the Segmenting Theory, the idea that complex lessons are broken into smaller, bit-sized parts , also supports personalization within interactive articles. Providing a reader the ability to play, pause, and scrub content allows the reader to move at their own speed, comprehending the information at a speed that works best for them. Segmenting also engages a reader’s essential processing without overloading their cognitive system .\nMultiple studies have been conducted showing that learners perform better when information is segmented, whether it be only within an animation or within an interface with textual descriptions . One excellent example of using segmentation and animation to personalize content delivery is “A Visual Introduction to Machine Learning,” which introduces fundamental concepts within machine learning in bite-sized pieces, while transforming a single dataset into a trained machine learning model . Extending this idea, in “Quantum Country,” an interactive textbook covering quantum computing, the authors implemented a user account system, allowing readers to save their position in the text and consume the content at their own pace . This book further utilizes the interactive medium by utilizing spaced repetition that helps improve recall.\n\n\nReducing Cognitive Load\nAuthors must calibrate the detail at which to discuss ideas and content to their readers expertise and interest to not overload them. When topics become multifaceted and complex, a balance must be struck between a high-level overview of a topic and its lower-level details. One interaction technique used to prevent a cognitive overload within a reader is “details-on-demand.”\nDetails-on-demand has become an ubiquitous design pattern. For example, modern operating systems offer to fetch dictionary definitions when a word is highlighted. When applied to visualization, this technique allows users to select parts of a dataset to be shown in more detail while maintaining a broad overview. This is particularly useful when a change of view is not required, so that users can inspect elements of interest on a point-by-point basis in the context of the whole . Below we highlight areas where details-on-demand has been successfully applied to reduce the amount of information present within an interface at once.\n\nData Visualization\nDetails-on-demand is core to information visualization, and concludes the seminal Visual Information-Seeking Mantra: “Overview first, zoom and filter, then details-on-demand” . Successful visualizations not only provide the base representations and techniques for these three steps, but also bridge the gaps between them . In practice, the solidified standard for details-on-demand in data visualization manifests as a tooltip, typically summoned on a cursor mouseover, that presents extra information in an overlay. Given that datasets often contain multiple attributes, tooltips can show the other attributes that are not currently encoded visually , for example, the map in 6 that shows where different types of birdsongs where recorded and what they sound like.\n\n\nIllustration\nDetails-on-demand is also used in illustrations, interactive textbooks, and museum exhibits, where highlighted segments of a figure can be selected to display additional information about the particular segment. For example, in “How does the eye work?” readers can select segments of an anatomical diagram of the human eye to learn more about specific regions, e.g., rods and cones . Another example is “Earth Primer,” an interactive textbook on tablets that allows readers to inspect the Earth’s interior, surface, and biomes . Each illustration contains segments the reader can tap to learn and explore in depth. 7 demonstrates this by pointing out specific regions in machine-generated imagery to help people spot fake images.\n\n\nMathematical Notation\nFormal mathematics, a historically static medium, can benefit from details-on-demand, for example, to elucidate a reader with intuition about a particular algebraic term, present a geometric interpretation of an equation, or to help a reader retain high-level context while digesting technical details.See this list of examples that experiment with applying new design techniques to various mathematical notation. For example, in “Why Momentum Really Works,” equation layout is done using Gestalt principles plus annotation to help a reader easily identify specific terms. In “Colorized Math Equations,” the Fourier transform equation is presented in both mathematical notation and plain text, but the two are linked through a mouseover that highlights which term in the equation corresponds to which word in the text . Another example that visualizes mathematics and computation is the “Image Kernels” tutorial where a reader can mouseover a real image and observe the effect and exact computation for applying a filter over the image . Instead of writing down long arithmetic sums, the interface allows readers to quickly see the summation operation’s terms and output. In 8, one of Maxwell’s equations is shown. Click the two buttons to reveal, or remind yourself, what each notation mark and variable represent.\n\n\nText\nWhile not as pervasive, text documents and other long-form textual mediums have also experimented with letting readers choose a variable level of detail to read. This idea was explored as early as the 1960s in StretchText, a hypertext feature that allows a reader to reveal a more descriptive or exhaustive explanation of something by expanding or contracting the content in place . The idea has resurfaced in more recent examples, including “On Variable Level-of-detail Documents” , a PhD thesis turned interactive article , and the call for proposals of The Parametric Press . One challenge that has limited this technique’s adoption is the burden it places on authors to write multiple versions of their content. For example, drag the slider in 9 to read descriptions of the Universal Approximation Theorem in increasing levels of detail. For other examples of details-on-demand for text, such as application in code documentation, see this small collection of examples .\n\n\nPreviewing Content\nDetails-on-demand can also be used as a method for previewing content without committing to another interaction or change of view. For example, when hovering over a hyperlink on Wikipedia, a preview card is shown that can contain an image and brief description; this gives readers a quick preview of the topic without clicking through and loading a new page . This idea is also not new: work from human-computer interaction explored fluid links within hypertext that present information about a particular topic in a location that does not obscure the source material. Both older and modern preview techniques use perceptually-based animation and simple tooltips to ensure their interactions are natural and lightweight feeling to readers ."
  },
  {
    "objectID": "docs/hohman2020communicating.html#challenges-for-authoring-interactives",
    "href": "docs/hohman2020communicating.html#challenges-for-authoring-interactives",
    "title": "Workshop on Reproducible Research",
    "section": "Challenges for Authoring Interactives",
    "text": "Challenges for Authoring Interactives\nIf interactive articles provide clear benefits over other mediums for communicating complex ideas, then why aren’t they more prevalent?\nUnfortunately, creating interactive articles today is difficult. Domain-specific diagrams, the main attraction of many interactive articles, must be individually designed and implemented, often from scratch. Interactions need to be intuitive and performant to achieve a nice reading experience. Needless to say, the text must also be well-written, and, ideally, seamlessly integrated with the graphics.\nThe act of creating a successful interactive article is closer to building a website than writing a blog post, often taking significantly more time and effort than a static article, or even an academic publication.As a proxy, see the number of commits on an example Distill article. Most interactive articles are created using general purpose web-development frameworks which, while expressive, can be difficult to work with for authors who are not also web developers. Even for expert web developers, current tools offer lower levels of abstraction than may be desired to prototype and iterate on designs.\nWhile there are some tools that help with alleviating this problem , they are relatively immature and mainly help with reducing the necessary programming tedium. Tools like Idyll can help authors start writing quickly and even enable rapid iteration through various designs (for example, letting an author quickly compare between sequencing content using a “scroller” or “stepper” based layout). However, Idyll does not offer any design guidance, help authors think through where interactivity would be most effectively applied, nor highlight how their content could be improved to increase its readability and memorability. For example, Idyll encodes no knowledge of the positive impact of self-explanation, instead it requires authors to be familiar with this research and how to operationalize it.\nTo design an interactive article successfully requires a diverse set of editorial, design, and programming skills. While some individuals are able to author these articles on their own, many interactive articles are created by a collective team consisting of multiple members with specialized skills, for example, data analysts, scripters, editors, journalists, graphic designers, and typesetters, as outlined in . The current generation of authoring tools do not acknowledge this collaboration. For example, to edit only the text of this article requires one to clone its source code using git, install project-specific dependencies using a terminal, and be comfortable editing HTML files. All of this complexity is incidental to task of editing text.\nPublishing to the web brings its own challenges: while interactive articles are available to anyone with a browser, they are burdened by rapidly changing web technologies that could break interactive content after just a few years. For this reason, easy and accessible interactive article archival is important for authors to know their work can be confidently preserved indefinitely to support continued readership.This challenge has been pointed out by the community. Authoring interactive articles also requires designing for a diverse set of devices, for example, ensuring bespoke content can be adapted for desktop and mobile screen sizes with varying connection speeds, since accessing interactive content demands more bandwidth.\nThere are other non-technical limitations for publishing interactive articles. For example, in non-journalism domains, there is a mis-aligned incentive structure for authoring and publishing interactive content: why should a researcher spend time on an “extra” interactive exposition of their work when they could instead publish more papers, a metric by which their career depends on? While different groups of people seek to maximize their work’s impact, legitimizing interactive artifacts requires buy-in from a collective of communities.\nMaking interactive articles accessible to people with disabilities is an open challenge. The dynamic medium exacerbates this problem compared to traditional static writing, especially when articles combine multiple formats like audio, video, and text. Therefore, ensuring interactive articles are accessible to everyone will require alternative modes of presenting content (e.g. text-to-speech, video captioning, data physicalization, data sonification) and careful interaction design.\nIt is also important to remember that not everything needs to be interactive. Authors should consider the audience and context of their work when deciding if use of interactivity would be valuable. In the worst case, interactivity may be distracting to readers or the functionality may go unused, the author having wasted their time implementing it. However, even in a domain where the potential communication improvement is incremental,In reality, multimedia studies show large effect sizes for improvement of transfer learning in many cases, see Chapter 12 of. at scale (e.g., delivering via the web), interactive articles can still have impact ."
  },
  {
    "objectID": "docs/hohman2020communicating.html#critical-reflections",
    "href": "docs/hohman2020communicating.html#critical-reflections",
    "title": "Workshop on Reproducible Research",
    "section": "Critical Reflections",
    "text": "Critical Reflections\nWe write this article not as media theorists, but as practitioners, researchers, and tool builders. While it has never been easier for writers to share their ideas online, current publishing tools largely support only static authoring and do not take full advantage of the fact that the web is a dynamic medium. We want that to change, and we are not alone. Others from the explorable explanations community have identified design patterns that help share complex ideas through play .\nTo explore these ideas further, two of this work’s authors created The Parametric Press: an annually published digital magazine that showcases the expository power that interactive dynamic media can have when effectively combined . In late 2018, we invited writers to respond to a call for proposals for our first issue focusing on exploring scientific and technological phenomena that stand to shape society at large. We sought to cover topics that would benefit from using the interactive or otherwise dynamic capabilities of the web. Given the challenges of authoring interactive articles, we did not ask authors to submit fully developed pieces. Instead, we accepted idea submissions, and collaborated with the authors over the course of four months to develop the issue, offering technical, design, and editorial assistance collectively to the authors that lacked experience in one of these areas. For example, we helped a writer implement visualizations, a student frame a cohesive narrative, and a scientist recap history and disseminate to the public. Multiple views from one article are shown in 10.\nWe see The Parametric Press as a crucial connection between the often distinct worlds of research and practice. The project serves as a platform through which to operationalize the theories put forth by education, journalism, and HCI researchers. Tools like Idyll which are designed in a research setting need to be validated and tested to ensure that they are of practical use; The Parametric Press facilitates this by allowing us to study its use in a real-world setting, by authors who are personally motivated to complete their task of constructing a high-quality interactive article, and only have secondary concerns and care about the tooling being used, if at all.\nThrough The Parametric Press, we saw the many challenges of authoring, designing, and publishing first hand, dually as researchers and practitioners. 2 summarizes interactive communication opportunities from both research and practice.\nAs researchers we can treat the project as a series of case studies, where we were observers of the motivation and workflows which were used to craft the stories, from their initial conception to their publication. Motivation to contribute to the project varied by author. Where some authors had personal investment in an issue or dataset they wanted to highlight and raise awareness to broadly, others were drawn towards the medium, recognizing its potential but not having the expertise or support to communicate interactively. We also observed how research software packages like Apparatus , Idyll , and D3 fit into the production of interactive articles, and how authors must combine these disparate tools to create an engaging experience for readers. In one article, “On Particle Physics,” an author combined two tools in a way that allowed him to create and embed dynamic graphics directly into his article without writing any code beyond basic markup. One of the creators of Apparatus had not considered this type of integration before, and upon seeing the finished article commented, “That’s fantastic! Reading that article, I had no idea that Apparatus was used. This is a very exciting proof-of-concept for unconventional explorable-explanation workflows.”\nWe were able to provide editorial guidance to the authors drawing on our knowledge of empirical studies done in the multimedia learning and information visualization communities to recommend graphical structures and page layouts, helping each article’s message be communicated most effectively. One of the most exciting outcomes of the project is that we saw authors develop interactive communication skills like any other skill: through continued practice, feedback, and iteration. We also observed the challenges that are inherent in publishing dynamic content on the web and identified the need for improved tooling in this area, specifically around the archiving of interactive articles. Will an article’s code still run a year from now? Ten years from now? To address interactive content archival, we set up a system to publish a digital archive of all of our articles at the time that they are first published to the site. At the top of each article on The Parametric Press is an archive link that allows readers to download a WARC (Web ARChive) file that can “played back” without requiring any web infrastructure. While our first iteration of the project relied on ad-hoc solutions to these problems, we hope to show how digital works such as ours can be published confidently knowing that they will be preserved indefinitely.\nAs practitioners we pushed the boundaries of the current generation of tools designed to support the creation of interactive articles on the web. We found bugs and limitations in Idyll, a tool which was originally designed to support the creation of one-off articles that we used as a content management system to power an entire magazine issue. We were forced to write patches and plugins to work around the limitations and achieve our desired publication.Many of these patches have since been merged to Idyll itself. This is the power of modular open-source tooling in action. We were also forced to craft designs under a more realistic set of constraints than academics usually deal with: when creating a visualization it is not enough to choose the most effective visual encodings, the graphics also had to be aesthetically appealing, adhere to a house style, have minimal impact on page load time and runtime performance, be legible on both mobile and desktop devices, and not be overly burdensome to implement. Any extra hour spent implementing one graphic was an hour that was not spent improving some other part of the issue, such as the clarity of the text, or the overall site design.\nThere are relatively few outlets that have the skills, technology, and desire to publish interactive articles. From its inception, one of the objectives of The Parametric Press is to showcase the new forms of media and publishing that are possible with tools available today, and inspire others to create their own dynamic writings. For example, Omar Shehata, the authors of The Parametric Press article “Unraveling the JPEG,” told us he had wanted to write this interactive article for years yet never had the opportunity, support, or incentive to create it. His article drew wide interest and critical acclaim.\nWe also wanted to take the opportunity as an independent publication to serve as a concrete example for others to follow, to represent a set of best practices for publishing interactive content. To that end, we made available all of the software that runs the site, including reusable components, custom data visualizations, and the publishing engine itself."
  },
  {
    "objectID": "docs/hohman2020communicating.html#looking-forward",
    "href": "docs/hohman2020communicating.html#looking-forward",
    "title": "Workshop on Reproducible Research",
    "section": "Looking Forward",
    "text": "Looking Forward\nA diverse community has emerged to meet these challenges, exploring and experimenting with what interactive articles could be. The Explorable Explanations community is a “disorganized ‘movement’ of artists, coders & educators who want to reunite play and learning.” Their online hub contains 170+ interactive articles on topics ranging from art, natural sciences, social sciences, journalism, and civics. The curious can also find tools, tutorials, and meta-discussion around learning, play, and representations. Explorables also hosted a mixed in-person and online Jam: a community-based sprint focused on creating new explorable explanations. 11 highlights a subset of the interactive articles created during the Jam.\nMany interactive articles are self-published due to a lack of platforms that support interactive publishing. Creating more outlets that allow authors to publish interactive content will help promote their development and legitimization. The few existing examples, including newer journals such as Distill, academic workshops like VISxAI , open-source publications like The Parametric Press , and live programming notebooks like Observable help, but currently target a narrow group of authors, namely those who have programming skills. Such platforms should also provide clear paths to submission, quality and editorial standards, and authoring guidelines. For example, news outlets have clear instructions for pitching written pieces, yet this is under-developed for interactive articles. Lastly, there is little funding available to support the development of interactive articles and the tools that support them. Researchers do not receive grants to communicate their work, and practitioners outside of the largest news outlets are not able to afford the time and implementation investment. Providing more funding for enabling interactive articles incentivizes their creation and can contribute to a culture where readers expect digital communications to better utilize the dynamic medium.\nWe have already discussed the breadth of skills required to author an interactive article. Can we help lower the barrier to entry? While there have been great, practical strides in this direction , there is still opportunity for creating tools to design, develop, and evaluate interactive articles in the wild. Specific features should include supporting mobile-friendly adaptations of interactive graphics (for example ), creating content for different platforms besides just the web, and tools that allow people to create interactive content without code.\nThe usefulness of interactive articles is predicated on the assumption that these interactive articles actually facilitate communication and learning. There is limited empirical evaluation of the effectiveness of interactive articles. The problem is exacerbated by the fact that large publishers are unwilling to share internal metrics, and laboratory studies may not generalize to real world reading trends. The New York Times provided one of the few available data points, stating that only a fraction of readers interact with non-static content, and suggested that designers should move away from interactivity . However, other research found that many readers, even those on mobile devices, are interested in utilizing interactivity when it is a core part of the article’s message . This statement from The New York Times has solidified as a rule-of-thumb for designers and many choose not to utilize interactivity because of it, despite follow-up discussion that contextualizes the original point and highlights scenarios where interactivity can be beneficial . This means designers are potentially choosing a suboptimal presentation of their story due to this anecdote. More research is needed in order to identify the cases in which interactivity is worth the cost of creation.\nWe believe in the power and untapped potential of interactive articles for sparking reader’s desire to learn and making complex ideas accessible and understandable to all.\n\nAcknowledgments\nWe are grateful to Arvind Satyanarayan for his early encouragement for pursuing this work, and Ludwig Schubert for his prompt help in fixing templating issues. We also thank The Parametric Press editors and authors for their continued support of the project, and to the Explorables community for their creativity and inspiration.\nThis work was supported in part by a NASA Space Technology Fellowship.\nThe birdsongs were provided by the users of https://www.xeno-canto.org/.\n\n\nAuthor Contributions\nResearch: After initial conversations over two years ago, Fred and Matthew jointly conducted the research bootstrapping early ideas from Matthew, including distilling literature, pulling published examples, and formulating the structure of the work.\nWriting & Graphics: Fred and Matthew also jointly wrote the text and collaborated on the design and implementation of the interactive graphics.\n\n\nReferences\n\nReport on the role of digital access providers  [link] OHCHR,, 2017. United Nations Human Rights Office of the High Commissioner.\nA personal computer for children of all ages  [link] Kay, A.C., 1972. Proceedings of the ACM Annual Conference.\nAugmenting human intellect: A conceptual framework  [link] Engelbart, D.C., 1962. Menlo Park, CA.\nThe diamond age Stephenson, N., 1998. Penguin UK.\nThe knowledge navigator  [link] Dubberly, H. and Mitch, D., 1987. Apple Computer, Inc.\nGetting it out of our system Nelson, T.H., 1967. Information Retrieval: A Critical Review, pp. 191–210. Thompson Books.\nPLATO  [link] UI,, 1960. University of Illinois.\nPhET interactive simulations  [link] CU,, 2002. University of Colorado Boulder.\nExplorable explanations  [link] Victor, B., 2011.\nHow y’all, youse and you guys talk  [HTML] Katz, J. and Andrews, W., 2013. The New York Times.\nSnow fall: The avalanche at tunnel creek  [HTML] Branch, J., 2014. The New York Times.\nWhy outbreaks like coronavirus spread exponentially, and how to ‘flatten the curve’  [link] Stevens, H., 2020. The Washington Post.\nAttacking discrimination with smarter machine learning  [link] Wattenberg, M., Viégas, F. and Hardt, M., 2016. Google Research.\nCoeffects: Context-aware programming languages  [link] Petricek, T., 2017.\nComplexity explained  [link] De Domenico, M. and Sayama, H., 2019.\nWhat’s really warming the world  [link] Roston, E. and Migliozzi, B., 2015. Bloomberg.\nYou draw it: How family income predicts children’s college chances  [HTML] Aisch, G., Cox, A. and Quealy, K., 2015. The New York Times.\nThe Uber Game  [link] Blood, D., Kao, J.S., Knoll, N., Kwong, R., Locke, C. and Rininsland, Æ., 2017. Financial Times.\nLet’s learn about waveforms  [link] Comeau, J., 2018. The Pudding.\nThe book of shaders  [link] Vivo, P.G. and Lowe, J., 2015.\nEconGraphs  [link] Makler, C., 2017.\nTo build a better ballot  [link] Case, N., 2016.\nThe atlas of redistricting  [link] Bycoffe, A., Koeze, E., Wasserman, D. and Wolfe, J., 2018. FiveThirtyEight.\nIs it better to rent or buy  [HTML] Bostock, M., Carter, S. and Tse, A., 2014. The New York Times.\nExplorable explanation  [link]\n\nWikipedia.\n\nIncreasing the transparency of research papers with explorable multiverse analyses  [link] Dragicevic, P., Jansen, Y., Sarma, A., Kay, M. and Chevalier, F., 2019. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 65.\nWorkshop on Visualization for AI Explainability  [link] VISxAI,, 2018. IEEE Visualization.\nExploranation: A new science communication paradigm  [link] Ynnerman, A., Löwgren, J. and Tibell, L., 2018. IEEE computer graphics and applications, Vol 38(3), pp. 13–20. IEEE.\nResearch debt  [link] Olah, C. and Carter, S., 2017. Distill.\nMore than telling a story: Transforming data into visually shared stories  [link] Lee, B., Riche, N.H., Isenberg, P. and Carpendale, S., 2015. IEEE Computer Graphics and Applications, Vol 35(5), pp. 84–90. IEEE.\n“Concrete” computer manipulatives in mathematics education  [link] Sarama, J. and Clements, D.H., 2009. Child Development Perspectives, Vol 3(3), pp. 145–150. Wiley Online Library.\nCybertext: Perspectives on ergodic literature Aarseth, E.J., 1997. JHU Press.\nInteractive non-fiction: Towards a new approach for storytelling in digital journalism  [link] Sizemore, J.H. and Zhu, J., 2011. International Conference on Interactive Digital Storytelling, pp. 313–316.\nActive essays on the web  [link] Yamamiya, T., Warth, A. and Kaehler, T., 2009. 2009 Seventh International Conference on Creating, Connecting and Collaborating through Computing, pp. 3–10.\nNewsgames: Journalism at play Bogost, I., Ferrari, S. and Schweizer, B., 2012. Mit Press.\nSimply bells and whistles? Cognitive effects of visual aesthetics in digital longforms  [link] Greussing, E. and Boomgaarden, H.G., 2019. Digital Journalism, Vol 7(2), pp. 273–293. Taylor & Francis.\nLearning as a function of time  [link] Fredrick, W.C. and Walberg, H.J., 1980. The Journal of Educational Research, Vol 73(4), pp. 183–194. Taylor & Francis.\nEmotional design in multimedia learning  [link] Um, E., Plass, J.L., Hayward, E.O. and Homer, B.D., 2012. Journal of Educational Psychology, Vol 104(2), pp. 485. American Psychological Association.\nHooked on data videos: assessing the effect of animation and pictographs on viewer engagement  [link] Amini, F., Riche, N.H., Lee, B., Leboe-McGowan, J. and Irani, P., 2018. Proceedings of the 2018 International Conference on Advanced Visual Interfaces, pp. 1–9.\nAnimation: Can it facilitate?  [link] Tversky, B., Morrison, J.B. and Betrancourt, M., 2002. International Journal of Human-computer Studies, Vol 57(4), pp. 247–262. Elsevier.\nAnimated transitions in statistical data graphics  [link] Heer, J. and Robertson, G., 2007. IEEE Transactions on Visualization and Computer Graphics, Vol 13(6), pp. 1240–1247. IEEE.\nHypothetical outcome plots outperform error bars and violin plots for inferences about reliability of variable ordering  [link] Hullman, J., Resnick, P. and Adar, E., 2015. PlOS One, Vol 10(11), pp. e0142444. Public Library of Science.\nLa perception de la causalité.(Etudes Psychol.), Vol. VI Michotte, A., 1946. Inst. Sup. De Philosophie.\nThe illusion of life: Disney animation Thomas, F., Johnston, O. and Thomas, F., 1995. Hyperion New York.\nThe horse in motion  [link] Muybridge, J., 1882. Nature, Vol 25(652), pp. 605. Nature Publishing Group.\nEmergent tool use from multi-agent autocurricula  [PDF] Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B. and Mordatch, I., 2019. arXiv preprint arXiv:1909.07528.\nClimate spirals  [link] Hawkins, E., 2016. Climate Lab Book.\nEarth’s relentless warming sets a brutal new record in 2017  [link] Randall, T. and Migliozzi, B., 2018. Bloomberg.\nGlobal temperature  [link] NASA,, 2020. NASA Global Climate Change.\nIt’s not your imagination. Summers are getting hotter.  [HTML] Popovich, N. and Pearce, A., 2017. The New York Times.\nExtensive data shows punishing reach of racism for black boys  [HTML] Badger, E., Miller, C.C., Pearce, A. and Quealy, K., 2018. The New York Times.\nDisagreements  [link] Cox, A. and Quealy, K., 2018. OpenVisConf.\nGun deaths in america  [link] Casselman, B., Conlen, M. and Fischer-Baum, R., 2016. FiveThirtyEight.\nThe fallen of World War II  [link] Halloran, N., 2015.\nShowing people behind data: Does anthropomorphizing visualizations elicit more empathy for human rights data?  [link] Boy, J., Pandey, A.V., Emerson, J., Satterthwaite, M., Nov, O. and Bertini, E., 2017. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, pp. 5462–5474.\nBeyond memorability: Visualization recognition and recall  [link] Borkin, M.A., Bylinskii, Z., Kim, N.W., Bainbridge, C.M., Yeh, C.S., Borkin, D., Pfister, H. and Oliva, A., 2015. IEEE Transactions on Visualization and Computer Graphics, Vol 22(1), pp. 519–528. IEEE.\nWhat makes a visualization memorable?  [link] Borkin, M.A., Vo, A.A., Bylinskii, Z., Isola, P., Sunkavalli, S., Oliva, A. and Pfister, H., 2013. IEEE Transactions on Visualization and Computer Graphics, Vol 19(12), pp. 2306–2315. IEEE.\nWhat if the data visualization is actually people  [link] Slobin, S., 2014. Source.\nEthical dimensions of visualization research  [PDF] Correll, M., 2019. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1–13.\nA walk among the data  [link] Ivanov, A., Danyluk, K., Jacob, C. and Willett, W., 2019. IEEE Computer Graphics and Applications, Vol 39(3), pp. 19–28. IEEE.\nVisual narrative flow: Exploring factors shaping data visualization story reading experiences  [link] McKenna, S., Henry Riche, N., Lee, B., Boy, J. and Meyer, M., 2017. Computer Graphics Forum, Vol 36(3), pp. 377–387.\nLinking and layout: Exploring the integration of text and visualization in storytelling  [link] Zhi, Q., Ottley, A. and Metoyer, R., 2019. Computer Graphics Forum, Vol 38(3), pp. 675–685.\nVideo games and learning Squire, K., 2011. Teaching and Participatory Culture in the Digital Age. Teachers College Print.\nCutthroat Capitalism: The Game  [link] Webworks, S. and Crothers, D., 2009. Wired.\nCombining software games with education: Evaluation of its educational effectiveness  [PDF] Virvou, M., Katsionis, G. and Manos, K., 2005. Journal of Educational Technology & Society, Vol 8(2), pp. 54–65. JSTOR.\nNarrative visualization: Telling stories with data  [link] Segel, E. and Heer, J., 2010. IEEE Transactions on Visualization and Computer Graphics, Vol 16(6), pp. 1139–1148. IEEE.\nParable of the polygons  [link] Hart, V. and Case, N., 2016.\nDrawing dynamic visualizations  [link] Victor, B., 2013.\nHow to read a book: The classic guide to intelligent reading Adler, M.J. and Van Doren, C., 2014. Simon and Schuster.\nScientific communication as sequential art  [link] Victor, B., 2011.\nHow you will die  [link] Yau, N., 2016. Flowing Data.\nOn Particle Physics  [link] Bianchi, R.M., Hohman, F. and Conlen, M., 2019. The Parametric Press.\nShould prison sentences be based on crimes that haven’t been committed yet  [link] Barry-Jester, A.M., Casselman, B. and Goldstein, D., 2015. FiveThirtyEight.\nHow to use t-SNE effectively  [link] Wattenberg, M., Viégas, F. and Johnson, I., 2016. Distill, Vol 1(10), pp. e2.\nThe beginner’s guide to dimensionality reduction  [link] Conlen, M. and Hohman, F., 2018. VISxAI at IEEE VIS.\nUnderstanding UMAP  [link] Coenen, A. and Pearce, A., 2019. Google PAIR.\nTensorflow.js: Machine learning for the web and beyond  [PDF] Smilkov, D., Thorat, N., Assogba, Y., Yuan, A., Kreeger, N., Yu, P., Zhang, K., Cai, S., Nielsen, E., Soergel, D. and others,, 2019. arXiv preprint arXiv:1901.05350.\nDesigning (and learning from) a teachable machine  [link] Webster, B., 2018. Google Design.\nExperiments in handwriting with a neural network  [link] Carter, S., Ha, D., Johnson, I. and Olah, C., 2016. Distill.\nDirect-manipulation visualization of deep networks Smilkov, D., Carter, S., Sculley, D., Viégas, F.B. and Wattenberg, M., 2017. arXiv preprint arXiv:1708.03788.\nGan lab: Understanding complex deep generative models using interactive visual experimentation  [link] Kahng, M., Thorat, N., Chau, D.H.P., Viégas, F.B. and Wattenberg, M., 2018. IEEE Transactions on Visualization and Computer Graphics, Vol 25(1), pp. 1–11. IEEE.\nUsing artificial intelligence to augment human intelligence  [link] Carter, S. and Nielsen, M., 2017. Distill, Vol 2(12), pp. e9.\nWho will win the presidency  [link]\n\nFiveThirtyEight.\n\nWho will be president  [HTML] Katz, J., 2016. The New York Times.\nLive results: Presidential election  [link]\n\nThe Washington Post.\n\nMultimedia learning  [link] Mayer, R.E., 2002. Psychology of Learning and Motivation, Vol 41, pp. 85–139. Elsevier.\n3Blue1Brown  [link] Sanderson, G.. Youtube.\nPrimer  [link] Helps, J.. Youtube.\nRevising the redundancy principle in multimedia learning  [link] Mayer, R.E. and Johnson, C.I., 2008. Journal of Educational Psychology, Vol 100(2), pp. 380. American Psychological Association.\nVisualizing quaternions: An explorable video series  [link] Sanderson, G. and Eater, B., 2018.\nSelf-explanations: How students study and use examples in learning to solve problems Chi, M.T., Bassok, M., Lewis, M.W., Reimann, P. and Glaser, R., 1989. Cognitive Science, Vol 13(2), pp. 145–182. Elsevier.\nSelf-explaining expository texts: The dual processes of generating inferences and repairing mental models Chi, M.T., 2000. Advances in Instructional Psychology, Vol 5, pp. 161–238.\nExplaining the gap: Visualizing one’s predictions improves recall and comprehension of data Kim, Y., Reinecke, K. and Hullman, J., 2017. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, pp. 1375–1386.\nYou draw it: Just how bad is the drug overdose epidemic  [HTML] Katz, J., 2017. The New York Times.\nYou draw it: What got better or worse during Obama’s presidency  [HTML] Buchanan, L., Park, H. and Pearce, A., 2017. The New York Times.\nThey draw it!  [link] Nguyen, F., Kim, Y., Germuska, J. and Hullman, J., 2019. Midwest Uncertainty Collective and The Knight Lab..\nData through others’ eyes: The impact of visualizing others’ expectations on visualization interpretation  [link] Kim, Y., Reinecke, K. and Hullman, J., 2017. IEEE Transactions on Visualization and Computer Graphics, Vol 24(1), pp. 760–769. IEEE.\nThe Gyllenhaal experiment  [link] Goldenberg, R. and Daniels, M., 2019. The Pudding.\nHow do you draw a circle? We analyzed 100,000 drawings to show how culture shapes our instincts  [link] Ha, T. and Sonnad, N., 2017. Quartz.\nRecitation as a factor in memorizing Gates, A.I., 1922. Science Press.\nThe power of testing memory: Basic research and implications for educational practice  [link] Roediger III, H.L. and Karpicke, J.D., 2006. Perspectives on Psychological Science, Vol 1(3), pp. 181–210. SAGE Publications Sage CA: Los Angeles, CA.\nKhan Academy  [link]\n\n\n\nThe instructional effect of feedback in test-like events Bangert-Drowns, R.L., Kulik, C.C., Kulik, J.A. and Morgan, M., 1991. Review of Educational Research, Vol 61(2), pp. 213–238. Sage Publications Sage CA: Thousand Oaks, CA.\nThe critical importance of retrieval for learning  [link] Karpicke, J.D. and Roediger, H.L., 2008. Science, Vol 319(5865), pp. 966–968. American Association for the Advancement of Science.\nHow to remember anything for forever-ish  [link] Case, N., 2018.\nQuantum country  [link] Matuschak, A. and Nielsen, M.A., 2019.\nIntrinsic motivation and the process of learning: Beneficial effects of contextualization, personalization, and choice.  [link] Cordova, D.I. and Lepper, M.R., 1996. Journal of Educational Psychology, Vol 88(4), pp. 715. American Psychological Association.\nAuthoring and generation of individualized patient education materials  [link] Di Marco, C., Bray, P., Covvey, H.D., Cowan, D.D., Di Ciccio, V., Hovy, E., Lipa, J. and Yang, C., 2006. AMIA Annual Symposium Proceedings, Vol 2006, pp. 195.\nPersaLog: Personalization of news article content  [HTML] Adar, E., Gearig, C., Balasubramanian, A. and Hullman, J., 2017. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, pp. 3188–3200.\nGenerating personalized spatial analogies for distances and areas  [link] Kim, Y., Hullman, J. and Agrawala, M., 2016. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pp. 38–48.\nHow much hotter is your hometown than when you were born  [HTML] Popvich, N., Migliozzi, B., Taylor, R., Williams, J. and Watkins, D., 2018. The New York Times.\nHuman terrain  [link] Daniels, M., 2018. The Pudding.\nAre you rich? This income-rank quiz might change how you see yourself  [HTML] Quealy, K., Gebeloff, R. and Taylor, R., 2019. The New York Times.\nQuiz: Let us predict whether you’re a democrat or a republican  [HTML] Chinoy, S., 2019. The New York Times.\nFind Out If Your Job Will Be Automated  [link] Whitehouse, M. and Rojanasakul, M., 2017. Bloomberg.\nBooze calculator: What’s your drinking nationality  [link] Lowther, E., Huynh, L., Bryson, M. and Connor, S., 2017. BBC.\nClick 1,000: How the pick-your-own-path episode was made  [link] Beckett, S., 2019. BBC.\nE-learning and the science of instruction: Proven guidelines for consumers and designers of multimedia learning Clark, R.C. and Mayer, R.E., 2016. John Wiley & Sons.\nMultimedia learning in an interactive self-explaining environment: What works in the design of agent-based microworlds?  [link] Mayer, R.E., Dow, G.T. and Mayer, S., 2003. Journal of Educational Psychology, Vol 95(4), pp. 806. American Psychological Association.\nPictorial aids for learning by doing in a multimedia geology simulation game.  [link] Mayer, R.E., Mautone, P. and Prothero, W., 2002. Journal of Educational Psychology, Vol 94(1), pp. 171. American Psychological Association.\nA visual introduction to machine learning  [link] Yee, S. and Chu, T., 2015. R2D3.\nBeyond guidelines: What can we learn from the visual information seeking mantra?  [link] Craft, B. and Cairns, P., 2005. Ninth International Conference on Information Visualisation (IV’05), pp. 110–118.\nThe eyes have it: A task by data type taxonomy for information visualizations  [link] Shneiderman, B., 1996. Proceedings 1996 IEEE Symposium on Visual Languages, pp. 336–343.\nInformation visualization and visual data mining  [link] Keim, D.A., 2002. IEEE Transactions on Visualization and Computer Graphics, Vol 8(1), pp. 1–8. IEEE.\nHow the recession shaped the economy, in 255 charts  [HTML] Ashkenas, J. and Parlapiano, A., 2014. The New York Times.\nHow does the eye work  [link]\n\nExplorable Explanations Game Jam.\n\nEarth primer  [link] Gingold, C., 2015.\nProgressive growing of gans for improved quality, stability, and variation  [PDF] Karras, T., Aila, T., Laine, S. and Lehtinen, J., 2018. International Conference on Learning Representations.\nA style-based generator architecture for generative adversarial networks  [HTML] Karras, T., Laine, S. and Aila, T., 2019. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4401–4410.\nWhy momentum really works  [link] Goh, G., 2017. Distill.\nColorized math equations  [link] Azad, K., 2017. Better Explained.\nImage kernels  [link] Powell, V., 2015. Setosa.\nStretchtext – hypertext note #8 Nelson, T., 1967. Project Xanadu.\nOn variable level-of-detail documents  [link] Beecroft, W., 2016.\nCall for proposals winter/spring 2019  [link]\n\nThe Parametric Press.\n\nA UI that lets readers control how much information they see  [link] Basques, K., 2018.\nWikipedia Preview Card  [link] Vasileva, O., 2018. Wikipedia.\nFluid links for informed and incremental link transitions  [link] Zellweger, P., Chang, B. and Mackinlay, J.D., 1998. Proceedings of Hypertext.\nReading and writing fluid hypertext narratives  [link] Zellweger, P.T., Mangen, A. and Newman, P., 2002. Proceedings of the Thirteenth ACM Conference on Hypertext and Hypermedia, pp. 45–54.\nIdyll: A markup language for authoring and publishing interactive articles on the web  [link] Conlen, M. and Heer, J., 2018. Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology, pp. 977–989.\nApparatus: A hybrid graphics editor and programming environment for creating interactive diagrams  [link] Schachman, T., 2015. Apparatus.\nObservable  [link]\n\n\n\nLOOPY: a tool for thinking in systems  [link] Case, N., 2017.\nWebstrates: shareable dynamic media  [link] Klokmose, C.N., Eagan, J.R., Baader, S., Mackay, W. and Beaudouin-Lafon, M., 2015. Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology, pp. 280–290.\nNeural networks and deep learning  [link] Nielsen, M.A., 2015. Determination Press.\nHow I make explorable explanations  [link] Case, N., 2017.\nExplorable explanations: 4 more design patterns  [link] Case, N., 2018.\nEmerging and recurring data-driven storytelling techniques: Analysis of a curated collection of recent stories  [link] Stolper, C.D., Lee, B., Riche, N.H. and Stasko, J., 2016. Microsoft Research.\nIssue 01: Science & Society  [link] Conlen, M., Hohman, F., Uren, V., Stalla, S. and Sass, A., 2018. The Parametric Press.\nThe myth of the impartial machine  [link] Feng, A., Wu, S., Hohman, F., Conlen, M. and Stalla, S., 2019. The Parametric Press.\nD3 data-driven documents  [link] Bostock, M., Ogievetsky, V. and Heer, J., 2011. IEEE Transactions on Visualization and ccomputer Graphics, Vol 17(12), pp. 2301–2309. IEEE.\nLaunching the Parametric Press  [link] Conlen, M. and Hohman, F., 2019. Visualization for Communication (VisComm) at IEEE VIS.\nTechniques for flexible responsive visualization design  [link] Hoffswell, J., Li, W. and Liu, Z., 2020. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems.\nA Comparative Evaluation of Animation and Small Multiples for Trend Visualization on Mobile Phones  [link] Brehmer, M., Lee, B., Isenberg, P. and Choe, E.K., 2019. IEEE Transactions on Visualization and Computer Graphics, Vol 26(1), pp. 364–374. IEEE.\nVisualizing ranges over time on mobile phones: a task-based crowdsourced evaluation  [link] Brehmer, M., Lee, B., Isenberg, P. and Choe, E.K., 2018. IEEE Transactions on Visualization and Computer Graphics, Vol 25(1), pp. 619–629. IEEE.\nWhy we are doing fewer interactives  [PDF] Tse, A., 2016. Malofiej Infographics World Summit.\nCapture & analysis of active reading behaviors for interactive articles on the web  [link] Conlen, M., Kale, A. and Heer, J., 2019. Computer Graphics Forum, Vol 38(3), pp. 687–698.\nIn defense of interactive graphics  [link] Aisch, G., 2017.\n\n\n\nUpdates and Corrections\nIf you see mistakes or want to suggest changes, please create an issue on GitHub.\n\n\nReuse\nDiagrams and text are licensed under Creative Commons Attribution CC-BY 4.0 with the source available on GitHub, unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by a note in their caption: “Figure from …”.\n\n\nCitation\nFor attribution in academic contexts, please cite this work as\nHohman, et al., \"Communicating with Interactive Articles\", Distill, 2020.\nBibTeX citation\n@article{hohman2020communicating,\n  author = {Hohman, Fred and Conlen, Matthew and Heer, Jeffrey and Chau, Duen Horng (Polo)},\n  title = {Communicating with Interactive Articles},\n  journal = {Distill},\n  year = {2020},\n  note = {https://distill.pub/2020/communicating-with-interactive-articles},\n  doi = {10.23915/distill.00028}\n}\nDistill is dedicated to clear explanations of machine learning\nAbout Submit Prize Archive RSS GitHub Twitter      ISSN 2476-0757"
  },
  {
    "objectID": "docs/theturingway2022.html",
    "href": "docs/theturingway2022.html",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "Authors: Zoé Ancion, Lidia Borrell-Damián, Pierre Mounier, Johan Rooryck, Bregt Saenen\nSource: Zenodo, March 2022\nOriginal PDF\n\n\nThis action plan outlines priority actions to develop and expand a sustainable, community-driven Diamond Open Access scholarly communication ecosystem. Key points include:\n\nDefinition and principles of Diamond Open Access (no fees for authors or readers)\nThe scale and importance of Diamond OA journals\nChallenges: technical capacity, management, visibility, sustainability\nFour central elements: efficiency, quality standards, capacity building, sustainability\nRecommendations for shared infrastructure, quality alignment, capacity building, and coordinated funding\n\nFor the full action plan and details, see the original PDF."
  },
  {
    "objectID": "docs/theturingway2022.html#summary",
    "href": "docs/theturingway2022.html#summary",
    "title": "Workshop on Reproducible Research",
    "section": "",
    "text": "This action plan outlines priority actions to develop and expand a sustainable, community-driven Diamond Open Access scholarly communication ecosystem. Key points include:\n\nDefinition and principles of Diamond Open Access (no fees for authors or readers)\nThe scale and importance of Diamond OA journals\nChallenges: technical capacity, management, visibility, sustainability\nFour central elements: efficiency, quality standards, capacity building, sustainability\nRecommendations for shared infrastructure, quality alignment, capacity building, and coordinated funding\n\nFor the full action plan and details, see the original PDF."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computo workshop",
    "section": "",
    "text": "⏰ 9h-10h: Introduction to Computo and Quarto\n☕ 10h-10h30: Coffee break/Discussion\n🧑‍💻 10h30-12h00: Hands-on with a toy example\n\n\n\n\n\n📝 13h30-17h00: Follow-up with personal article submission process"
  },
  {
    "objectID": "index.html#planning",
    "href": "index.html#planning",
    "title": "Computo workshop",
    "section": "",
    "text": "⏰ 9h-10h: Introduction to Computo and Quarto\n☕ 10h-10h30: Coffee break/Discussion\n🧑‍💻 10h30-12h00: Hands-on with a toy example\n\n\n\n\n\n📝 13h30-17h00: Follow-up with personal article submission process"
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Computo workshop",
    "section": "Learning objectives",
    "text": "Learning objectives\n\n🎯 Understand the benefits of reproducible research\n📄 Learn how to create a quarto document\n📝 Learn how to include code, data, and narrative text in a quarto document\n📤 Learn how to submit a quarto document to Computo\n🧭 How to navigate the Computo submission process (optional)"
  },
  {
    "objectID": "index.html#team",
    "href": "index.html#team",
    "title": "Computo workshop",
    "section": "Team",
    "text": "Team\n\n\nEditorial board\n\n\n\n\nIT support\n\n\n\n\n\n\n\nJulien Chiquet (chief editor)\nStat. learning, DR INRAE Paris-Saclay University\n\n\n\n\n\n\nPierre Neuvial\nStatistics, DR CNRS IMT Toulouse\n\n\n\n\n\n\nMathurin Massias\nOptim./Machine-Learning CR INRIA Lyon\n\n\n\n\n\n\n\n\nFra.-Dav. Collin\nCS/Stats/ML, IR CNRS IMAG, Montpellier University\n\n\n\n\n\n\n\n\n\nNelle Varoquaux\nMachine learning, CR CNRS Grenoble Alpes University\n\n\n\n\n\n\nMarie-Pierre Étienne\nStatistics, MCF Institut Agro Rennes-Angers\n\n\n\n\n\n\nChloé Azencott\nMachine Learning CR MinesParisTech\n\n\n\n\n\n\n\n\nGhislain Durif\nStats/ML/dev, IR CNRS LBMC, ENS LYON"
  },
  {
    "objectID": "index.html#what-is-reproducible-research",
    "href": "index.html#what-is-reproducible-research",
    "title": "Computo workshop",
    "section": "What is reproducible research?",
    "text": "What is reproducible research?\nFundamentally, it provides three things:\n\n\n\n\n\n\nTools to reproduce the results (that’s like cooking)\n\n\n\n\n\n\n\nA “recipe” to reproduce the results (still like cooking)\n\n\n\n\n\n\n\nA path to understanding the results and the process that led to them (unlike cooking…1)"
  },
  {
    "objectID": "index.html#pre-computo-era",
    "href": "index.html#pre-computo-era",
    "title": "Computo workshop",
    "section": "Pre-Computo era",
    "text": "Pre-Computo era\n\n\n\n\n\n\n\nThe pdf era and paper submission.\nThe reproducibility was not a priority:\n\n🛠️ Tools had to be bought, installed, and maintained\n🔒 Data and code were not shared (social engineering)\n❓ Even methodology details are often missing\n\n\n\n\n\n\n\nSocial engineering is required to get reproducible results: at best you just have to ask the authors, at worst you have to reverse-engineer everything… and have no guarantee whatsoever to get to the same results, when authors are in an “Après moi, le déluge” mood and ditch, forget or let it crumble once the paper is published.\n\n\n\n\n\n\n\nAnd then in the Machine Learning domain, there was distill.pub [1]\n\n\n📊 State-of-the-art visualizations\n🔄 Paradigm shift in scientific publication: “distillation” of complex ideas\n💯% reproducible (just a git clone and a few standard commands)\n\n\n\n\nbut…\n\n\n\n\n\n\n\n \n\n\n\n… engineering was too complex for the average scientist (a lot of javascript, etc.)\n\n\nIn fact, the distill.pub project was discontinued in 2021 [2]\n\n\n\n\n\nDue to a series of burnouts from the staff"
  },
  {
    "objectID": "index.html#the-rise-of-the-pragmatic-2",
    "href": "index.html#the-rise-of-the-pragmatic-2",
    "title": "Computo workshop",
    "section": "The Rise of the Pragmatic",
    "text": "The Rise of the Pragmatic\ndistill.pub’s goals were right, but they outpaced themselves in terms of development complexity.\n\n🚀 Computo is a fresh start with a pragmatic approach\n🧰 Leverage what the scientific community is already using (Rmarkdown, Jupyter notebooks, etc.)"
  },
  {
    "objectID": "index.html#origin-of-computo-2020s",
    "href": "index.html#origin-of-computo-2020s",
    "title": "Computo workshop",
    "section": "Origin of Computo (~ 2020s)",
    "text": "Origin of Computo (~ 2020s)\nFrench Statistical Society appoints a “publication” committee (led by Julien then Pierre) to develop a new journal\n\n\n\n\n\n\n\n\n\nAssessment\n\n\n\n\n\n😔 Multiplication of “traditional” journals…\n😔 No valorization of “negative” results\n😥 No or not enough valorization of source codes and case studies\n😱 ↘ of publication quality and time dedicated to each article (on author or reviewer sides) [3]\n😱 Issue with scientific reproducibility (analyses, experiments) [4–9]\n\n\n\n\n\n\n\n\n\n\n\n\nPoint of view\n\n\n\n\n🔄 Need for renewal regarding scientific research implementation\n📈 Need for higher standards regarding result publications\n\n\n\n\n\n\n\n⇝ Emergence of “Computo” idea"
  },
  {
    "objectID": "index.html#philosophy",
    "href": "index.html#philosophy",
    "title": "Computo workshop",
    "section": "Philosophy",
    "text": "Philosophy\n\n\n\n\n\n\nScientific perimeter\n\n\n\nPromote contribution in statistics and machine learning that provide insight into which models or methods are more appropriate to address a specific scientific question\n\n\n\n\n\n\n\n\nOpen access\n\n\n\n\n\n\n💎 “Diamond” open access (free to publish and free to read, possible to reuse)\n🅭 🅯 Content published under CC-BY license (attribution, share, adapt)\n📝💬 Reviews and discussions available after acceptance for publication (anonymous reviews)\n\n➡️ In accordance with Budapest Open Access Initiative (BOAI) and Plan S\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible\n\n\n\n\n🔢 Numerical (statistical) reproducibility is a necessary condition\n💾 Source code and data should be available, at least partly executed and fully executable"
  },
  {
    "objectID": "index.html#note-on-reproducible-research-desquilbet2019-hejblum2020-the_turing_way2022",
    "href": "index.html#note-on-reproducible-research-desquilbet2019-hejblum2020-the_turing_way2022",
    "title": "Computo workshop",
    "section": "Note on reproducible research [10–12]",
    "text": "Note on reproducible research [10–12]\n\nWhy reproduce scientific results?\n\n💪 To strengthen their credibility\n🔍 To check for errors (everyone makes errors at some point!!!)\n🧱 To build new research upon them (science is incremental)\n\n\n\nIssues?\n\n🔄 Reproduce numerical scientific results is often difficult (technology/environment evolution, source code/environment configuration/software partially available or not available)\n⏳💸 Waste of time and resources to reproduce existing non-reproducible results\n\n\n\nReproducible research?\n\n👥 For others but also for your future self\n✅ Improve result credibility\n🚀 Facilitate future research works"
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "Computo workshop",
    "section": "Setup",
    "text": "Setup\nOfficial launch at the end of 2021\n\n \n\n\n“Economical” model\n\n💪 A few tenacious people…\n🛠️ Free/Open-source community tools (Pandoc, Quarto, Git forge)\n🏛️ Institutional support (INRAE, INRIA, CNRS, SFdS)"
  },
  {
    "objectID": "index.html#functioning",
    "href": "index.html#functioning",
    "title": "Computo workshop",
    "section": "Functioning",
    "text": "Functioning\n\n\n\nWriting system\nNotebook and literate programmingtext (markdown) + math (\\(\\LaTeX\\)) + code (Python/R/Julia), references (bib\\(\\TeX\\))\n\n\nPublication system\nEnvironment management, Compilation, Multi-format publication (pdf, html)Continuous integration/Continuous deployment (CI/CD)\n\n\nReviewing system\n\n🕵️ Anonymous exchange published after acceptance\n🧑‍⚖️ Reviewer pool (you can join)\n🔄 [Ongoing switch from Scholastica to Open review]\n\n\n\n\nSolutions/Prototype\nReproducible articles and computations\n\nAutomatic editorial reproducibility\n\nScientific validation"
  },
  {
    "objectID": "index.html#note-on-literate-programming",
    "href": "index.html#note-on-literate-programming",
    "title": "Computo workshop",
    "section": "Note on literate programming",
    "text": "Note on literate programming\n\n\nLiterate programming [13]: notebook including text and code\nMarkup formatting language: e.g. markdown\nSeparate content from rendering (≠ “what you see is what you get” editors)\nRendering includes text, code and results (from code computations)\n\n\n---\ntitle: \"My article\"\n---\n\nWe compute 1+1:\n\n```{r}\n1+1\n```"
  },
  {
    "objectID": "index.html#note-on-quarto",
    "href": "index.html#note-on-quarto",
    "title": "Computo workshop",
    "section": "Note on quarto",
    "text": "Note on quarto\n\n\n https://quarto.org\n\n\n\nGeneralization of Rmarkdown\nRelying on top community tools like universal document converter Pandoc\nDeveloped and supported by RStudio/Posit\nNative support of complex documents (website, articles, books) and multiple languages for computations (R, Python, Julia)\nManagement of references, citations, figures, tables, metadata, etc."
  },
  {
    "objectID": "index.html#note-on-continuous-integration",
    "href": "index.html#note-on-continuous-integration",
    "title": "Computo workshop",
    "section": "Note on continuous integration",
    "text": "Note on continuous integration\n\nImplementation in git forges (e.g. github actions or gitlab CI/CD)\nTriggered by commits\nAutomatic tests\nAutomatic deployment: package/software publication, website\n\n\n\n\n\n\n\nCredit: Pratik89Roy CC-BY-SA-4.0 from Wikimedia"
  },
  {
    "objectID": "index.html#tools-for-authors",
    "href": "index.html#tools-for-authors",
    "title": "Computo workshop",
    "section": "Tools for authors",
    "text": "Tools for authors\n\n\n\nDocument model\nquarto Computo extension \n\n\n\nDocument template\nGit template repository\nwith template notebook document + doc + pre-configured compilation and publication setup\n\n\n\n\n\nLocally\n\n\n\nText editor/IDE (VS Code, Rstudio, NeoVim, etc.)\nQuarto (compilation)\n\n\n\nJulia / R / Python code + computations\ngit versioning system"
  },
  {
    "objectID": "index.html#author-point-of-view-13",
    "href": "index.html#author-point-of-view-13",
    "title": "Computo workshop",
    "section": "Author point of view (1/3)",
    "text": "Author point of view (1/3)\n\n\nStep 0: setup a git repository for your article\n\n\nStartup from a template repository (R, Python, Julia)\n\n\n\n\n\n\nTip\n\n\n\nYou can host your git repository on github and soon an any gitlab forge2.\n\n\n\n\n\n\n\n\n\nStep 1: write your article\nLet’s go, locally (same spirit as Jupyter/Rmarkdown notebooks)"
  },
  {
    "objectID": "index.html#author-point-of-view-23",
    "href": "index.html#author-point-of-view-23",
    "title": "Computo workshop",
    "section": "Author point of view (2/3)",
    "text": "Author point of view (2/3)\n\nStep 2: configure the environment (dependencies management)\n\nExample in PythonExample in RExample in Julia\n\n\n\n\nvenv: use a virtual environment and generate the requirements.txt file\n\n# requirements.txt\njupyter\nmatplotlib\nnumpy\n\n\n\n\n\n\nrenv: generate the renv.lock file\n\nrenv::init()\nrenv::install(\"ggplot2\")\n# or equivalently install.packages(\"ggplot2\")\nrenv::snapshot()\n\n\n\n\n\n\nPkg: native Julia package manager (with generated Project.toml et Manifest.toml files)\n\nadd Plots\nadd IJulia\n\n\n\n\n\nConfiguration file versioned and used during CI compilation/publication action\n\n\nStep 3: (re)production\nA git push command will trigger your article compilation (including computations) and publication as a github page3\nSee the pre-configured .github/workflows/build.yml file for the github action configuration4"
  },
  {
    "objectID": "index.html#author-point-of-view-33",
    "href": "index.html#author-point-of-view-33",
    "title": "Computo workshop",
    "section": "Author point of view (3/3)",
    "text": "Author point of view (3/3)\n\n\n\n\nStep 4: submit your article\nIf the CI process succeeds, both HTML and PDF versions are published on the github-page associated to the repository\n\n\nScholastica Open review (soon to be replaced by PCI)\nhttps://openreview.net/group?id=Computo\nSubmit:\n\nyour article PDF (scientific content review)\nyour git repository (source code and reproducibility review)"
  },
  {
    "objectID": "index.html#editor-point-of-view",
    "href": "index.html#editor-point-of-view",
    "title": "Computo workshop",
    "section": "Editor point of view",
    "text": "Editor point of view\n\n\nAfter a “traditional” review process, a 3-step procedure:\n\n✅ Acceptance\n🛠️ Pre-production\n📰 Publication in Computo (with a DOI)\n\nincluding\n\n📥 Copy of the author git repository to https://github.com/computorg/\n🖋️ Final version formatting\n📝 Review report publication\n📚 Registration in the journal bibliographic database\n🗄️ Copy of the repository to Software Heritage for archiving\n🌐 Publication of the article on the journal website\n\n\nTask-list = github issue"
  },
  {
    "objectID": "index.html#year-and-a-half-report",
    "href": "index.html#year-and-a-half-report",
    "title": "Computo workshop",
    "section": "2year and a half report",
    "text": "2year and a half report\n\n🥲 Fully operational + doi, ISSN\n🙂 7 published articles, 3 in pre-production, 6 under review (more details here)\n🙂 x presentations (Montpellier, Toronto, Humastica, Grenoble, RR2023, etc.)\n🙂 French reproducible research network\n🤯 Difficult to find reviewers\n🤔 Institutional support?\n🤔 Changing practices in the scientific community?"
  },
  {
    "objectID": "index.html#discussion",
    "href": "index.html#discussion",
    "title": "Computo workshop",
    "section": "Discussion",
    "text": "Discussion\n\nAbout several choices\n\n Quarto: dynamic, agnostic language, FOSS5, community-based (pandoc), Rstudio/Posit support 🚀\n GitHub: dynamic, large user community but not institutional and limited computing resources 🌐\n\n\n\nComparison/inspiration\n\nPeer Community-In (PCI)6, EpiSciences: peer review and publication of articles, but not reproducible by default, but we have just built an official community, stay tuned!\nhttps://rescience.github.io/: “remake” published articles\nhttps://distill.pub (discontinued): technically more complicated and only ML/AI-oriented"
  },
  {
    "objectID": "index.html#perspectives",
    "href": "index.html#perspectives",
    "title": "Computo workshop",
    "section": "Perspectives",
    "text": "Perspectives\n\n\n🖥️ Provision of computing resources (to be able to run all computations)\n🦊 Full gitlab support (CI/CD, docker, registry, etc.)\n🇫🇷 Switch to a french institutional gitlab forge?\n🛡️ Improve long-term reproducibility stack (docker container, GUIX fully reproducible environment, or Containers only at the end of the publication process)\n\n\n\nHow to help?\n\n\n\n✍️ By submitting7 your work!\n\n\n\n👀 By becoming reviewer8"
  },
  {
    "objectID": "index.html#regarding-the-logo",
    "href": "index.html#regarding-the-logo",
    "title": "Computo workshop",
    "section": "Regarding the logo",
    "text": "Regarding the logo"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Computo workshop",
    "section": "References",
    "text": "References\n\n\n1. Olah, C and Carter, S 2017 Research debt. Distill. DOI: https://doi.org/10.23915/distill.00005\n\n\n2. Team, E 2021 Distill hiatus. Distill. DOI: https://doi.org/10.23915/distill.00031\n\n\n3. Hanson, M A, Barreiro, P G, Crosetto, P, and Brockington, D 2023 The strain on scientific publishing. DOI: https://doi.org/10.48550/arXiv.2309.15884\n\n\n4. Ioannidis, J P A 2005 Why Most Published Research Findings Are False. PLoS Medicine, 2(8): e124. DOI: https://doi.org/10.1371/journal.pmed.0020124\n\n\n5. Steen, R G 2011 Retractions in the scientific literature: Is the incidence of research fraud increasing? Journal of Medical Ethics, 37(4): 249–253. DOI: https://doi.org/10.1136/jme.2010.040923\n\n\n6. Allison, D B, Brown, A W, George, B J, and Kaiser, K A 2016 Reproducibility: A tragedy of errors. Nature, 530(7588): 27–29. DOI: https://doi.org/10.1038/530027a\n\n\n7. Bastian, H 2016 Reproducibility Crisis Timeline: Milestones in Tackling Research Reliability. URL https://absolutelymaybe.plos.org/2016/12/05/reproducibility-crisis-timeline-milestones-in-tackling-research-reliability/. [Online; accessed 22-March-2023]\n\n\n8. Whitfield, J 2021 Replication Crisis. London Review of Books, 43(19). URL https://www.lrb.co.uk/the-paper/v43/n19/john-whitfield/replication-crisis. [Online; accessed 22-March-2023]\n\n\n9. Hernández, J A and Colom, M 2023 Repeatability, Reproducibility, Replicability, Reusability (4R) in Journals’ Policies and Software/Data Management in Scientific Publications: A Survey, Discussion, and Perspectives. URL https://hal.science/hal-04322522. [Online; accessed 4-January-2024]\n\n\n10. Desquilbet, L L, Granger, S, Hejblum, B, Legrand, A, Pernot, P, Rougier, N P, Castro Guerra, E de, Courbin-Coulaud, M, Duvaux, L, Gravier, P, Le Campion, G, Roux, S, and Santos, F 2019 Vers une recherche reproductible. Unité régionale de formation à l’information scientifique et technique de Bordeaux. URL https://hal.science/hal-02144142\n\n\n11. Hejblum, B P, Kunzmann, K, Lavagnini, E, Hutchinson, A, Robertson, D, Jones, S, and Eckes-Shephard, A 2020 Realistic and Robust Reproducible Research for Biostatistics. DOI: https://doi.org/10.20944/preprints202006.0002.v1\n\n\n12. The Turing Way Community 2022 The Turing Way: A handbook for reproducible, ethical and collaborative research. DOI: https://doi.org/10.5281/zenodo.7625728\n\n\n13. Knuth, D E 1984 Literate programming. The Computer Journal, 27(2): 97–111."
  },
  {
    "objectID": "index.html#two-fold-reproducibility",
    "href": "index.html#two-fold-reproducibility",
    "title": "Computo workshop",
    "section": "Two-fold reproducibility",
    "text": "Two-fold reproducibility\nThe global scientific workflow of a reproducible process for a Computo may be split into two types of steps:\nExternal and Editorial"
  },
  {
    "objectID": "index.html#external",
    "href": "index.html#external",
    "title": "Computo workshop",
    "section": "External",
    "text": "External\n\nExternal\n\nProcess to obtain (intermediate) results utside of the notebook environment, for a list of reasons (non-exclusive to each other):\n\n\n\n🕰️ the process is too long to be conducted in a notebook\n💾 the data to be processed is too big to be handled directly in the notebook\n🖥️ it needs a specific environment (e.g. a cluster, with gpus, etc.)\n🛠️ it needs to involve specific languages (e.g. C, C++, Fortran, etc.) or build tools (e.g. make, cmake, etc.)"
  },
  {
    "objectID": "index.html#editorial",
    "href": "index.html#editorial",
    "title": "Computo workshop",
    "section": "Editorial",
    "text": "Editorial\n\nEditorial\n\nnotebook rendering with the results of the external process\n\n\n\n\n\n\n\n\nRequirement\n\n\n\nIf the notebook contains everything to produce the final document\n\\(\\Rightarrow\\) “Direct reproducibility” in the sense that the notebook is the only thing needed to reproduce the results.\nUltimately, the workflow must end with a direct reproducibility step which concludes the whole process.\n\n\n\nData transfer\n\nWhen applicable, the switch from external to editorial reproducibility is done with a “data transfer” step,\n\n\ndata produced by the external process \\(\\Rightarrow\\) transferred to the notebook environment.\n\n\n\n\n\n\nRequirement\n\n\n\nNot only the intermediate results are provided, but also the code to transfer it in the notebook environment.\nThere are a variety of software solutions to do so."
  },
  {
    "objectID": "index.html#examples-of-data-transfer-solutions",
    "href": "index.html#examples-of-data-transfer-solutions",
    "title": "Computo workshop",
    "section": "Examples of data transfer solutions",
    "text": "Examples of data transfer solutions\n\nIntermediate results storage\n\n🐍 Python: joblib.Memory, caching mechanism for python functions, save the results of a function call to disk, and load it back later.\n📦 R : .RData file format, can be loaded back in R with the load() function.\n🗃️ If results are small enough, storing them in a text file (e.g. .csv, .tsv, .json, etc.) is also a solution.\n\n\n\nTransfer of the results to the notebook environment\n\n📁 (.joblib directory or .Rdata file) could be committed to the git repository, and directly loaded in the notebook environment.\n☁️ Alternative, centralize input data (when large enough) and intermediate results on a shared scientific provider (we recommend Zenodo for this purpose), and download them in the notebook environment."
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Computo workshop",
    "section": "Quarto",
    "text": "Quarto\nIn this workshop, we will learn how to use quarto to create a document that includes code, data, and narrative text. We will also learn how to make the CI (continuous integration) work."
  },
  {
    "objectID": "index.html#the-main-pipeline-step-by-step",
    "href": "index.html#the-main-pipeline-step-by-step",
    "title": "Computo workshop",
    "section": "The main pipeline, step by step",
    "text": "The main pipeline, step by step\n\n🧰 Template installation\n🧪 computing environment: renv, conda, etc.\n✍️ Authoring in the qmd\n🖨️ rendering locally\n🚀 pushing to github"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Computo workshop",
    "section": "Getting started",
    "text": "Getting started\nTo get started you will need to clone the mock template for this workshop. The template is available at\nhttps://github.com/computorg/jds2025-workshop-template"
  },
  {
    "objectID": "index.html#creating-a-repo-from-a-template",
    "href": "index.html#creating-a-repo-from-a-template",
    "title": "Computo workshop",
    "section": "Creating a repo from a template",
    "text": "Creating a repo from a template\n\nOn GitHub.com, navigate to the main page of the repository.\nAbove the file list, click Use this template.\nSelect Create a new repository."
  },
  {
    "objectID": "index.html#cloning-the-repository",
    "href": "index.html#cloning-the-repository",
    "title": "Computo workshop",
    "section": "Cloning the repository",
    "text": "Cloning the repository\nMake a git clone of the repository you just templated and open it in your favorite IDE."
  },
  {
    "objectID": "index.html#edit-the-quarto-document",
    "href": "index.html#edit-the-quarto-document",
    "title": "Computo workshop",
    "section": "Edit the quarto document",
    "text": "Edit the quarto document\nOpen published-paper-tsne.qmd in your favorite text editor.\nFind the code cells marked as “TODO” and modify them in order to have the plots.\nLook at the scripts in the scripts directory for the plotting code"
  },
  {
    "objectID": "index.html#rendering-the-document",
    "href": "index.html#rendering-the-document",
    "title": "Computo workshop",
    "section": "Rendering the document",
    "text": "Rendering the document\nTry to render the document with quarto, it should produce a pdf and an html document.\nquarto render"
  },
  {
    "objectID": "index.html#making-the-ci-work",
    "href": "index.html#making-the-ci-work",
    "title": "Computo workshop",
    "section": "Making the CI work",
    "text": "Making the CI work\nONce you’re happy with the result, it’s time to push the changes to GitHub.\ngit add .\ngit commit -m \"My first article\"\ngit push\nThis will trigger the CI workflow, which will run the tests and render the document."
  },
  {
    "objectID": "index.html#checking-the-ci",
    "href": "index.html#checking-the-ci",
    "title": "Computo workshop",
    "section": "Checking the CI",
    "text": "Checking the CI\nYou can check the CI status by going to the 🏃‍♂️ “Actions” tab of your repository on GitHub. You should see a workflow run triggered by your push 🚦."
  },
  {
    "objectID": "index.html#a-brief-overview-of-the-ci",
    "href": "index.html#a-brief-overview-of-the-ci",
    "title": "Computo workshop",
    "section": "A brief overview of the CI",
    "text": "A brief overview of the CI\n\nIf everything is 🟢 green, you can go to the 📄 “Pages” tab of your repository and see the rendered document 🎉."
  },
  {
    "objectID": "index.html#submitting-the-article",
    "href": "index.html#submitting-the-article",
    "title": "Computo workshop",
    "section": "Submitting the article",
    "text": "Submitting the article\nNow you can write a nice mail to the Computo board! 📧🎉"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Computo workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEven so, we may discuss the fact that blindly following recipes will not make you a good cook.↩︎\nwith CI/CD support↩︎\nor as a gitlab page when gitlab will be supported (soon)↩︎\nand soon the .gitlab-ci.yml file for the gitlab CI/CD configuration↩︎\n“free and open-source”↩︎\nComputo is a PCI-friendly journal↩︎\nhttps://computo.sfds.asso.fr/submit/↩︎\ncontact us at computo@sfds.asso.fr↩︎"
  }
]